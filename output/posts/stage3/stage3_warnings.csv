/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
