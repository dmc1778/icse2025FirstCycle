F:\stackexchange\extracted\askubuntu.com,"  <row Id=""1365537"" PostTypeId=""1"" AcceptedAnswerId=""1365544"" CreationDate=""2021-09-24T13:37:27.050"" Score=""0"" ViewCount=""1390"" Body=""&lt;p&gt;I try to get tensorflow GPU up and running in a virtual environment (venv):&lt;/p&gt;&#xA;&lt;p&gt;I use lambdalabs&#xA;OS is Ubuntu 20.04.3 LTS.&lt;/p&gt;&#xA;&lt;p&gt;I have following python script: checkGPY.py:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;&#xA;if tf.test.gpu_device_name():&#xA;    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))&#xA;else:&#xA;    print(&amp;quot;Please install GPU version of TF&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Outside the venv it works fine. I obtain Default GPU Device: /device:GPU:0.&#xA;If a train a small neural network (NN) and &lt;code&gt;watch nvidia-smi&lt;/code&gt; I see that the GPU memory increases during training. So the GPU resources are used for NN training.&lt;/p&gt;&#xA;&lt;p&gt;However if I run it is inside a venv (I installed tensorflow version: 2.6.0 inside the venv.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;(venv) x@y $ python checkGPU.py&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I obtain: Please install GPU version of TF&lt;/p&gt;&#xA;&lt;p&gt;I obtain also following: Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory&lt;/p&gt;&#xA;&lt;p&gt;So I understand that the dynamic library libcudnn.so.8 cannot be accessed from inside the venv.&lt;/p&gt;&#xA;&lt;p&gt;How can I resolve this?&lt;/p&gt;&#xA;"" OwnerUserId=""1413695"" LastEditorUserId=""1413695"" LastEditDate=""2021-09-24T13:52:09.740"" LastActivityDate=""2021-09-29T07:35:03.830"" Title=""Tensorflow does not detect GPU - lambdalabs"" Tags=""&lt;nvidia&gt;&lt;python&gt;&lt;gpu&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\askubuntu.com,"  <row Id=""1365537"" PostTypeId=""1"" AcceptedAnswerId=""1365544"" CreationDate=""2021-09-24T13:37:27.050"" Score=""0"" ViewCount=""1390"" Body=""&lt;p&gt;I try to get tensorflow GPU up and running in a virtual environment (venv):&lt;/p&gt;&#xA;&lt;p&gt;I use lambdalabs&#xA;OS is Ubuntu 20.04.3 LTS.&lt;/p&gt;&#xA;&lt;p&gt;I have following python script: checkGPY.py:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;&#xA;if tf.test.gpu_device_name():&#xA;    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))&#xA;else:&#xA;    print(&amp;quot;Please install GPU version of TF&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Outside the venv it works fine. I obtain Default GPU Device: /device:GPU:0.&#xA;If a train a small neural network (NN) and &lt;code&gt;watch nvidia-smi&lt;/code&gt; I see that the GPU memory increases during training. So the GPU resources are used for NN training.&lt;/p&gt;&#xA;&lt;p&gt;However if I run it is inside a venv (I installed tensorflow version: 2.6.0 inside the venv.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;(venv) x@y $ python checkGPU.py&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I obtain: Please install GPU version of TF&lt;/p&gt;&#xA;&lt;p&gt;I obtain also following: Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory&lt;/p&gt;&#xA;&lt;p&gt;So I understand that the dynamic library libcudnn.so.8 cannot be accessed from inside the venv.&lt;/p&gt;&#xA;&lt;p&gt;How can I resolve this?&lt;/p&gt;&#xA;"" OwnerUserId=""1413695"" LastEditorUserId=""1413695"" LastEditDate=""2021-09-24T13:52:09.740"" LastActivityDate=""2021-09-29T07:35:03.830"" Title=""Tensorflow does not detect GPU - lambdalabs"" Tags=""&lt;nvidia&gt;&lt;python&gt;&lt;gpu&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\askubuntu.com,"  <row Id=""1365537"" PostTypeId=""1"" AcceptedAnswerId=""1365544"" CreationDate=""2021-09-24T13:37:27.050"" Score=""0"" ViewCount=""1390"" Body=""&lt;p&gt;I try to get tensorflow GPU up and running in a virtual environment (venv):&lt;/p&gt;&#xA;&lt;p&gt;I use lambdalabs&#xA;OS is Ubuntu 20.04.3 LTS.&lt;/p&gt;&#xA;&lt;p&gt;I have following python script: checkGPY.py:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;&#xA;if tf.test.gpu_device_name():&#xA;    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))&#xA;else:&#xA;    print(&amp;quot;Please install GPU version of TF&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Outside the venv it works fine. I obtain Default GPU Device: /device:GPU:0.&#xA;If a train a small neural network (NN) and &lt;code&gt;watch nvidia-smi&lt;/code&gt; I see that the GPU memory increases during training. So the GPU resources are used for NN training.&lt;/p&gt;&#xA;&lt;p&gt;However if I run it is inside a venv (I installed tensorflow version: 2.6.0 inside the venv.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;(venv) x@y $ python checkGPU.py&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I obtain: Please install GPU version of TF&lt;/p&gt;&#xA;&lt;p&gt;I obtain also following: Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory&lt;/p&gt;&#xA;&lt;p&gt;So I understand that the dynamic library libcudnn.so.8 cannot be accessed from inside the venv.&lt;/p&gt;&#xA;&lt;p&gt;How can I resolve this?&lt;/p&gt;&#xA;"" OwnerUserId=""1413695"" LastEditorUserId=""1413695"" LastEditDate=""2021-09-24T13:52:09.740"" LastActivityDate=""2021-09-29T07:35:03.830"" Title=""Tensorflow does not detect GPU - lambdalabs"" Tags=""&lt;nvidia&gt;&lt;python&gt;&lt;gpu&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\askubuntu.com,"  <row Id=""1365537"" PostTypeId=""1"" AcceptedAnswerId=""1365544"" CreationDate=""2021-09-24T13:37:27.050"" Score=""0"" ViewCount=""1390"" Body=""&lt;p&gt;I try to get tensorflow GPU up and running in a virtual environment (venv):&lt;/p&gt;&#xA;&lt;p&gt;I use lambdalabs&#xA;OS is Ubuntu 20.04.3 LTS.&lt;/p&gt;&#xA;&lt;p&gt;I have following python script: checkGPY.py:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;&#xA;if tf.test.gpu_device_name():&#xA;    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))&#xA;else:&#xA;    print(&amp;quot;Please install GPU version of TF&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Outside the venv it works fine. I obtain Default GPU Device: /device:GPU:0.&#xA;If a train a small neural network (NN) and &lt;code&gt;watch nvidia-smi&lt;/code&gt; I see that the GPU memory increases during training. So the GPU resources are used for NN training.&lt;/p&gt;&#xA;&lt;p&gt;However if I run it is inside a venv (I installed tensorflow version: 2.6.0 inside the venv.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;(venv) x@y $ python checkGPU.py&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I obtain: Please install GPU version of TF&lt;/p&gt;&#xA;&lt;p&gt;I obtain also following: Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory&lt;/p&gt;&#xA;&lt;p&gt;So I understand that the dynamic library libcudnn.so.8 cannot be accessed from inside the venv.&lt;/p&gt;&#xA;&lt;p&gt;How can I resolve this?&lt;/p&gt;&#xA;"" OwnerUserId=""1413695"" LastEditorUserId=""1413695"" LastEditDate=""2021-09-24T13:52:09.740"" LastActivityDate=""2021-09-29T07:35:03.830"" Title=""Tensorflow does not detect GPU - lambdalabs"" Tags=""&lt;nvidia&gt;&lt;python&gt;&lt;gpu&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""216170"" PostTypeId=""1"" CreationDate=""2019-03-25T14:54:21.213"" Score=""2"" ViewCount=""1460"" Body=""&lt;p&gt;I've implemented a self-organising map in Tensorflow's low-level API. After testing with sklearn's Iris data, the results seem correct. I did implement the algorithm also using NumPy before converting it to &lt;code&gt;tf&lt;/code&gt;, because I'm new to Tensorflow, and had no idea whether it would work or not. So of course I tried out which would perform better. It turns out the Numpy implementation is far better for a smallish-scale problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To me this sounds a bit off. When loading a data set of ~3500x10 to GPU memory and carrying out basically the exact same algorithm with all-tf operations, I was at least expecting some sort of improvement with matrix-matrix broadcasted sums, multiplications and powers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I said, I have little experience with Tensorflow, so I started wondering if something is wrong with the code. The result seems good enough, but I'm not aware of many best practices or performance caveats. I'm especially after performance and TF-related feedback, but of course I'll happily take any criticism on my code!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import tensorflow as tf&#xA;&#xA;from tqdm import tqdm&#xA;&#xA;&#xA;def learning_rate(epoch: tf.placeholder, max_epochs: int):&#xA;    with tf.name_scope('learning_rate'):&#xA;        return tf.exp(-4 * epoch / max_epochs)&#xA;&#xA;&#xA;def neighbourhood(r: tf.placeholder, epoch: tf.placeholder, max_epochs: int, size: int):&#xA;    with tf.name_scope('neighbourhood'):&#xA;        return tf.exp(&#xA;            - (2 * r / size) ** 2&#xA;            * (max_epochs / (max_epochs - epoch)) ** 3&#xA;        )&#xA;&#xA;&#xA;class SelfOrganisingMap:&#xA;    def __init__(self, shape: tuple, features: int, *_,&#xA;                 max_epochs: int = None, init: str = 'uniform', learning_rate: float = 0.1):&#xA;        &quot;&quot;&quot;&#xA;        Self-organising map using TensorFlow.&#xA;&#xA;        :param shape: map dimensions&#xA;        :param features: number of input features&#xA;        :param _: used to force calling with keyword arguments below&#xA;        :param max_epochs: used to scale the neighbourhood and learning rate functions&#xA;        :param init: method of weight initialisation. 'uniform' for drawing from an uniform&#xA;            distribution between 0..1, 'normal' for drawing from X~N(0,1)&#xA;        :param learning_rate: initial learning rate multiplier&#xA;        &quot;&quot;&quot;&#xA;        self._weights = None&#xA;&#xA;        self._shape = shape&#xA;        self._features = features&#xA;        self._neighbour_shape = (len(shape),) + tuple(1 for _ in shape) + (-1,)&#xA;&#xA;        self._epochs = 0&#xA;        self._max_epochs = max_epochs&#xA;        self._initial_lr = learning_rate&#xA;&#xA;        if init == 'uniform':&#xA;            self._initialiser = tf.random_uniform_initializer&#xA;        elif init == 'normal':&#xA;            self._initialiser = tf.random_normal_initializer&#xA;        else:&#xA;            raise AssertionError('Unknown weights initialiser type &quot;%s&quot;!' % init)&#xA;&#xA;    @property&#xA;    def weights(self):&#xA;        if self._weights is None:&#xA;            raise ValueError('Map not fitted!')&#xA;        return self._weights&#xA;&#xA;    @property&#xA;    def initialiser(self):&#xA;        if self._weights is None:&#xA;            return self._initialiser&#xA;        else:&#xA;            return tf.convert_to_tensor(self._weights)&#xA;&#xA;    @property&#xA;    def shape(self): return self._shape&#xA;&#xA;    @property&#xA;    def n_nodes(self): return int(np.prod(self.shape))&#xA;&#xA;    @property&#xA;    def features(self): return self._features&#xA;&#xA;    @property&#xA;    def epochs(self): return self._epochs&#xA;&#xA;    @property&#xA;    def max_epochs(self): return self._max_epochs&#xA;&#xA;    def project(self, data: np.ndarray) -&amp;gt; np.array:&#xA;        &quot;&quot;&quot;&#xA;        Project data onto the map. NumPy implementation for simplicity.&#xA;&#xA;        :param data: samples&#xA;        :return: node indices&#xA;        &quot;&quot;&quot;&#xA;        diff = self.weights - data&#xA;        dist = np.sum(diff ** 2, axis=-1, keepdims=True)&#xA;        return np.array(np.unravel_index(&#xA;            np.argmin(dist.reshape((-1, data.shape[0])), axis=0), self.shape&#xA;        ))&#xA;&#xA;    def train(self, x: np.ndarray, epochs: int, batch_size: int = 1) -&amp;gt; None:&#xA;        &quot;&quot;&quot;&#xA;        Create training graph and train SOM.&#xA;&#xA;        :param x: training data&#xA;        :param epochs: number of epochs to train&#xA;        :param batch_size: number of training examples per step&#xA;        :return: None&#xA;        &quot;&quot;&quot;&#xA;        graph = tf.Graph()&#xA;        sess = tf.Session(graph=graph)&#xA;&#xA;        x = x.astype(np.float64)&#xA;&#xA;        if x.shape[0] % batch_size != 0:&#xA;            raise ValueError('Bad batch_size, last batch would be incomplete!')&#xA;&#xA;        # Construct graph&#xA;        with graph.as_default():&#xA;            indices = tf.convert_to_tensor(np.expand_dims(&#xA;                np.indices(self.shape, dtype=np.float64), axis=-1&#xA;            ))&#xA;            weights = tf.get_variable(&#xA;                'weights', (*self.shape, 1, self.features), initializer=self.initialiser, dtype=tf.float64&#xA;            )&#xA;&#xA;            with tf.name_scope('data'):&#xA;                data = tf.data.Dataset.from_tensor_slices(x)&#xA;                data = data.shuffle(buffer_size=10000).repeat(epochs)&#xA;                data = data.batch(batch_size, drop_remainder=True)&#xA;                data = data.make_one_shot_iterator().get_next()&#xA;&#xA;            with tf.name_scope('winner'):&#xA;                diff = weights - data&#xA;                dist = tf.reduce_sum(diff ** 2, axis=-1, keepdims=True)&#xA;                w_ix = tf.argmin(tf.reshape(dist, (self.n_nodes, data.shape[0])), axis=0)&#xA;                winner_op = tf.convert_to_tensor(tf.unravel_index(w_ix, self.shape))&#xA;&#xA;            with tf.name_scope('update'):&#xA;                curr_epoch = tf.placeholder(dtype=tf.int64, shape=())&#xA;&#xA;                idx_diff = indices - tf.reshape(tf.cast(&#xA;                    winner_op, dtype=tf.float64&#xA;                ), shape=self._neighbour_shape)&#xA;                idx_dist = tf.norm(idx_diff, axis=0)&#xA;&#xA;                l_rate = learning_rate(curr_epoch, self.max_epochs)&#xA;                n_hood = neighbourhood(&#xA;                    idx_dist, curr_epoch, self.max_epochs, max(self.shape)&#xA;                )&#xA;&#xA;                update = diff * l_rate * tf.expand_dims(n_hood, axis=-1)&#xA;                update_op = weights.assign(&#xA;                    weights - self._initial_lr * tf.reduce_sum(update, axis=-2, keepdims=True)&#xA;                )&#xA;&#xA;            init = tf.global_variables_initializer()&#xA;&#xA;        # Initialise all variables&#xA;        sess.run(init)&#xA;&#xA;        batches = int(np.ceil(x.shape[0] // batch_size))&#xA;        for i in tqdm(range(epochs)):&#xA;            for b in range(batches):&#xA;                sess.run(update_op, feed_dict={&#xA;                    curr_epoch: self.epochs + i&#xA;                })&#xA;&#xA;        self._weights = sess.run(weights)&#xA;        self._epochs += epochs&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's a short test snippet as well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.utils import shuffle&#xA;from sklearn.datasets import load_iris&#xA;from sklearn.preprocessing import RobustScaler&#xA;&#xA;a, _ = load_iris(True)&#xA;a, _ = shuffle(a, _)&#xA;a = RobustScaler().fit_transform(a)&#xA;&#xA;epochs = 100&#xA;som = SelfOrganisingMap((100, 100), 4, max_epochs=epochs, init='normal')&#xA;som.train(a, epochs, batch_size=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""165539"" LastEditorUserId=""22222"" LastEditDate=""2019-04-21T04:14:58.550"" LastActivityDate=""2019-04-21T04:14:58.550"" Title=""Simple Self-Organizing Map (SOM) in Tensorflow"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""216170"" PostTypeId=""1"" CreationDate=""2019-03-25T14:54:21.213"" Score=""2"" ViewCount=""1460"" Body=""&lt;p&gt;I've implemented a self-organising map in Tensorflow's low-level API. After testing with sklearn's Iris data, the results seem correct. I did implement the algorithm also using NumPy before converting it to &lt;code&gt;tf&lt;/code&gt;, because I'm new to Tensorflow, and had no idea whether it would work or not. So of course I tried out which would perform better. It turns out the Numpy implementation is far better for a smallish-scale problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To me this sounds a bit off. When loading a data set of ~3500x10 to GPU memory and carrying out basically the exact same algorithm with all-tf operations, I was at least expecting some sort of improvement with matrix-matrix broadcasted sums, multiplications and powers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I said, I have little experience with Tensorflow, so I started wondering if something is wrong with the code. The result seems good enough, but I'm not aware of many best practices or performance caveats. I'm especially after performance and TF-related feedback, but of course I'll happily take any criticism on my code!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import tensorflow as tf&#xA;&#xA;from tqdm import tqdm&#xA;&#xA;&#xA;def learning_rate(epoch: tf.placeholder, max_epochs: int):&#xA;    with tf.name_scope('learning_rate'):&#xA;        return tf.exp(-4 * epoch / max_epochs)&#xA;&#xA;&#xA;def neighbourhood(r: tf.placeholder, epoch: tf.placeholder, max_epochs: int, size: int):&#xA;    with tf.name_scope('neighbourhood'):&#xA;        return tf.exp(&#xA;            - (2 * r / size) ** 2&#xA;            * (max_epochs / (max_epochs - epoch)) ** 3&#xA;        )&#xA;&#xA;&#xA;class SelfOrganisingMap:&#xA;    def __init__(self, shape: tuple, features: int, *_,&#xA;                 max_epochs: int = None, init: str = 'uniform', learning_rate: float = 0.1):&#xA;        &quot;&quot;&quot;&#xA;        Self-organising map using TensorFlow.&#xA;&#xA;        :param shape: map dimensions&#xA;        :param features: number of input features&#xA;        :param _: used to force calling with keyword arguments below&#xA;        :param max_epochs: used to scale the neighbourhood and learning rate functions&#xA;        :param init: method of weight initialisation. 'uniform' for drawing from an uniform&#xA;            distribution between 0..1, 'normal' for drawing from X~N(0,1)&#xA;        :param learning_rate: initial learning rate multiplier&#xA;        &quot;&quot;&quot;&#xA;        self._weights = None&#xA;&#xA;        self._shape = shape&#xA;        self._features = features&#xA;        self._neighbour_shape = (len(shape),) + tuple(1 for _ in shape) + (-1,)&#xA;&#xA;        self._epochs = 0&#xA;        self._max_epochs = max_epochs&#xA;        self._initial_lr = learning_rate&#xA;&#xA;        if init == 'uniform':&#xA;            self._initialiser = tf.random_uniform_initializer&#xA;        elif init == 'normal':&#xA;            self._initialiser = tf.random_normal_initializer&#xA;        else:&#xA;            raise AssertionError('Unknown weights initialiser type &quot;%s&quot;!' % init)&#xA;&#xA;    @property&#xA;    def weights(self):&#xA;        if self._weights is None:&#xA;            raise ValueError('Map not fitted!')&#xA;        return self._weights&#xA;&#xA;    @property&#xA;    def initialiser(self):&#xA;        if self._weights is None:&#xA;            return self._initialiser&#xA;        else:&#xA;            return tf.convert_to_tensor(self._weights)&#xA;&#xA;    @property&#xA;    def shape(self): return self._shape&#xA;&#xA;    @property&#xA;    def n_nodes(self): return int(np.prod(self.shape))&#xA;&#xA;    @property&#xA;    def features(self): return self._features&#xA;&#xA;    @property&#xA;    def epochs(self): return self._epochs&#xA;&#xA;    @property&#xA;    def max_epochs(self): return self._max_epochs&#xA;&#xA;    def project(self, data: np.ndarray) -&amp;gt; np.array:&#xA;        &quot;&quot;&quot;&#xA;        Project data onto the map. NumPy implementation for simplicity.&#xA;&#xA;        :param data: samples&#xA;        :return: node indices&#xA;        &quot;&quot;&quot;&#xA;        diff = self.weights - data&#xA;        dist = np.sum(diff ** 2, axis=-1, keepdims=True)&#xA;        return np.array(np.unravel_index(&#xA;            np.argmin(dist.reshape((-1, data.shape[0])), axis=0), self.shape&#xA;        ))&#xA;&#xA;    def train(self, x: np.ndarray, epochs: int, batch_size: int = 1) -&amp;gt; None:&#xA;        &quot;&quot;&quot;&#xA;        Create training graph and train SOM.&#xA;&#xA;        :param x: training data&#xA;        :param epochs: number of epochs to train&#xA;        :param batch_size: number of training examples per step&#xA;        :return: None&#xA;        &quot;&quot;&quot;&#xA;        graph = tf.Graph()&#xA;        sess = tf.Session(graph=graph)&#xA;&#xA;        x = x.astype(np.float64)&#xA;&#xA;        if x.shape[0] % batch_size != 0:&#xA;            raise ValueError('Bad batch_size, last batch would be incomplete!')&#xA;&#xA;        # Construct graph&#xA;        with graph.as_default():&#xA;            indices = tf.convert_to_tensor(np.expand_dims(&#xA;                np.indices(self.shape, dtype=np.float64), axis=-1&#xA;            ))&#xA;            weights = tf.get_variable(&#xA;                'weights', (*self.shape, 1, self.features), initializer=self.initialiser, dtype=tf.float64&#xA;            )&#xA;&#xA;            with tf.name_scope('data'):&#xA;                data = tf.data.Dataset.from_tensor_slices(x)&#xA;                data = data.shuffle(buffer_size=10000).repeat(epochs)&#xA;                data = data.batch(batch_size, drop_remainder=True)&#xA;                data = data.make_one_shot_iterator().get_next()&#xA;&#xA;            with tf.name_scope('winner'):&#xA;                diff = weights - data&#xA;                dist = tf.reduce_sum(diff ** 2, axis=-1, keepdims=True)&#xA;                w_ix = tf.argmin(tf.reshape(dist, (self.n_nodes, data.shape[0])), axis=0)&#xA;                winner_op = tf.convert_to_tensor(tf.unravel_index(w_ix, self.shape))&#xA;&#xA;            with tf.name_scope('update'):&#xA;                curr_epoch = tf.placeholder(dtype=tf.int64, shape=())&#xA;&#xA;                idx_diff = indices - tf.reshape(tf.cast(&#xA;                    winner_op, dtype=tf.float64&#xA;                ), shape=self._neighbour_shape)&#xA;                idx_dist = tf.norm(idx_diff, axis=0)&#xA;&#xA;                l_rate = learning_rate(curr_epoch, self.max_epochs)&#xA;                n_hood = neighbourhood(&#xA;                    idx_dist, curr_epoch, self.max_epochs, max(self.shape)&#xA;                )&#xA;&#xA;                update = diff * l_rate * tf.expand_dims(n_hood, axis=-1)&#xA;                update_op = weights.assign(&#xA;                    weights - self._initial_lr * tf.reduce_sum(update, axis=-2, keepdims=True)&#xA;                )&#xA;&#xA;            init = tf.global_variables_initializer()&#xA;&#xA;        # Initialise all variables&#xA;        sess.run(init)&#xA;&#xA;        batches = int(np.ceil(x.shape[0] // batch_size))&#xA;        for i in tqdm(range(epochs)):&#xA;            for b in range(batches):&#xA;                sess.run(update_op, feed_dict={&#xA;                    curr_epoch: self.epochs + i&#xA;                })&#xA;&#xA;        self._weights = sess.run(weights)&#xA;        self._epochs += epochs&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's a short test snippet as well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.utils import shuffle&#xA;from sklearn.datasets import load_iris&#xA;from sklearn.preprocessing import RobustScaler&#xA;&#xA;a, _ = load_iris(True)&#xA;a, _ = shuffle(a, _)&#xA;a = RobustScaler().fit_transform(a)&#xA;&#xA;epochs = 100&#xA;som = SelfOrganisingMap((100, 100), 4, max_epochs=epochs, init='normal')&#xA;som.train(a, epochs, batch_size=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""165539"" LastEditorUserId=""22222"" LastEditDate=""2019-04-21T04:14:58.550"" LastActivityDate=""2019-04-21T04:14:58.550"" Title=""Simple Self-Organizing Map (SOM) in Tensorflow"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""216170"" PostTypeId=""1"" CreationDate=""2019-03-25T14:54:21.213"" Score=""2"" ViewCount=""1460"" Body=""&lt;p&gt;I've implemented a self-organising map in Tensorflow's low-level API. After testing with sklearn's Iris data, the results seem correct. I did implement the algorithm also using NumPy before converting it to &lt;code&gt;tf&lt;/code&gt;, because I'm new to Tensorflow, and had no idea whether it would work or not. So of course I tried out which would perform better. It turns out the Numpy implementation is far better for a smallish-scale problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To me this sounds a bit off. When loading a data set of ~3500x10 to GPU memory and carrying out basically the exact same algorithm with all-tf operations, I was at least expecting some sort of improvement with matrix-matrix broadcasted sums, multiplications and powers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I said, I have little experience with Tensorflow, so I started wondering if something is wrong with the code. The result seems good enough, but I'm not aware of many best practices or performance caveats. I'm especially after performance and TF-related feedback, but of course I'll happily take any criticism on my code!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import tensorflow as tf&#xA;&#xA;from tqdm import tqdm&#xA;&#xA;&#xA;def learning_rate(epoch: tf.placeholder, max_epochs: int):&#xA;    with tf.name_scope('learning_rate'):&#xA;        return tf.exp(-4 * epoch / max_epochs)&#xA;&#xA;&#xA;def neighbourhood(r: tf.placeholder, epoch: tf.placeholder, max_epochs: int, size: int):&#xA;    with tf.name_scope('neighbourhood'):&#xA;        return tf.exp(&#xA;            - (2 * r / size) ** 2&#xA;            * (max_epochs / (max_epochs - epoch)) ** 3&#xA;        )&#xA;&#xA;&#xA;class SelfOrganisingMap:&#xA;    def __init__(self, shape: tuple, features: int, *_,&#xA;                 max_epochs: int = None, init: str = 'uniform', learning_rate: float = 0.1):&#xA;        &quot;&quot;&quot;&#xA;        Self-organising map using TensorFlow.&#xA;&#xA;        :param shape: map dimensions&#xA;        :param features: number of input features&#xA;        :param _: used to force calling with keyword arguments below&#xA;        :param max_epochs: used to scale the neighbourhood and learning rate functions&#xA;        :param init: method of weight initialisation. 'uniform' for drawing from an uniform&#xA;            distribution between 0..1, 'normal' for drawing from X~N(0,1)&#xA;        :param learning_rate: initial learning rate multiplier&#xA;        &quot;&quot;&quot;&#xA;        self._weights = None&#xA;&#xA;        self._shape = shape&#xA;        self._features = features&#xA;        self._neighbour_shape = (len(shape),) + tuple(1 for _ in shape) + (-1,)&#xA;&#xA;        self._epochs = 0&#xA;        self._max_epochs = max_epochs&#xA;        self._initial_lr = learning_rate&#xA;&#xA;        if init == 'uniform':&#xA;            self._initialiser = tf.random_uniform_initializer&#xA;        elif init == 'normal':&#xA;            self._initialiser = tf.random_normal_initializer&#xA;        else:&#xA;            raise AssertionError('Unknown weights initialiser type &quot;%s&quot;!' % init)&#xA;&#xA;    @property&#xA;    def weights(self):&#xA;        if self._weights is None:&#xA;            raise ValueError('Map not fitted!')&#xA;        return self._weights&#xA;&#xA;    @property&#xA;    def initialiser(self):&#xA;        if self._weights is None:&#xA;            return self._initialiser&#xA;        else:&#xA;            return tf.convert_to_tensor(self._weights)&#xA;&#xA;    @property&#xA;    def shape(self): return self._shape&#xA;&#xA;    @property&#xA;    def n_nodes(self): return int(np.prod(self.shape))&#xA;&#xA;    @property&#xA;    def features(self): return self._features&#xA;&#xA;    @property&#xA;    def epochs(self): return self._epochs&#xA;&#xA;    @property&#xA;    def max_epochs(self): return self._max_epochs&#xA;&#xA;    def project(self, data: np.ndarray) -&amp;gt; np.array:&#xA;        &quot;&quot;&quot;&#xA;        Project data onto the map. NumPy implementation for simplicity.&#xA;&#xA;        :param data: samples&#xA;        :return: node indices&#xA;        &quot;&quot;&quot;&#xA;        diff = self.weights - data&#xA;        dist = np.sum(diff ** 2, axis=-1, keepdims=True)&#xA;        return np.array(np.unravel_index(&#xA;            np.argmin(dist.reshape((-1, data.shape[0])), axis=0), self.shape&#xA;        ))&#xA;&#xA;    def train(self, x: np.ndarray, epochs: int, batch_size: int = 1) -&amp;gt; None:&#xA;        &quot;&quot;&quot;&#xA;        Create training graph and train SOM.&#xA;&#xA;        :param x: training data&#xA;        :param epochs: number of epochs to train&#xA;        :param batch_size: number of training examples per step&#xA;        :return: None&#xA;        &quot;&quot;&quot;&#xA;        graph = tf.Graph()&#xA;        sess = tf.Session(graph=graph)&#xA;&#xA;        x = x.astype(np.float64)&#xA;&#xA;        if x.shape[0] % batch_size != 0:&#xA;            raise ValueError('Bad batch_size, last batch would be incomplete!')&#xA;&#xA;        # Construct graph&#xA;        with graph.as_default():&#xA;            indices = tf.convert_to_tensor(np.expand_dims(&#xA;                np.indices(self.shape, dtype=np.float64), axis=-1&#xA;            ))&#xA;            weights = tf.get_variable(&#xA;                'weights', (*self.shape, 1, self.features), initializer=self.initialiser, dtype=tf.float64&#xA;            )&#xA;&#xA;            with tf.name_scope('data'):&#xA;                data = tf.data.Dataset.from_tensor_slices(x)&#xA;                data = data.shuffle(buffer_size=10000).repeat(epochs)&#xA;                data = data.batch(batch_size, drop_remainder=True)&#xA;                data = data.make_one_shot_iterator().get_next()&#xA;&#xA;            with tf.name_scope('winner'):&#xA;                diff = weights - data&#xA;                dist = tf.reduce_sum(diff ** 2, axis=-1, keepdims=True)&#xA;                w_ix = tf.argmin(tf.reshape(dist, (self.n_nodes, data.shape[0])), axis=0)&#xA;                winner_op = tf.convert_to_tensor(tf.unravel_index(w_ix, self.shape))&#xA;&#xA;            with tf.name_scope('update'):&#xA;                curr_epoch = tf.placeholder(dtype=tf.int64, shape=())&#xA;&#xA;                idx_diff = indices - tf.reshape(tf.cast(&#xA;                    winner_op, dtype=tf.float64&#xA;                ), shape=self._neighbour_shape)&#xA;                idx_dist = tf.norm(idx_diff, axis=0)&#xA;&#xA;                l_rate = learning_rate(curr_epoch, self.max_epochs)&#xA;                n_hood = neighbourhood(&#xA;                    idx_dist, curr_epoch, self.max_epochs, max(self.shape)&#xA;                )&#xA;&#xA;                update = diff * l_rate * tf.expand_dims(n_hood, axis=-1)&#xA;                update_op = weights.assign(&#xA;                    weights - self._initial_lr * tf.reduce_sum(update, axis=-2, keepdims=True)&#xA;                )&#xA;&#xA;            init = tf.global_variables_initializer()&#xA;&#xA;        # Initialise all variables&#xA;        sess.run(init)&#xA;&#xA;        batches = int(np.ceil(x.shape[0] // batch_size))&#xA;        for i in tqdm(range(epochs)):&#xA;            for b in range(batches):&#xA;                sess.run(update_op, feed_dict={&#xA;                    curr_epoch: self.epochs + i&#xA;                })&#xA;&#xA;        self._weights = sess.run(weights)&#xA;        self._epochs += epochs&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's a short test snippet as well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.utils import shuffle&#xA;from sklearn.datasets import load_iris&#xA;from sklearn.preprocessing import RobustScaler&#xA;&#xA;a, _ = load_iris(True)&#xA;a, _ = shuffle(a, _)&#xA;a = RobustScaler().fit_transform(a)&#xA;&#xA;epochs = 100&#xA;som = SelfOrganisingMap((100, 100), 4, max_epochs=epochs, init='normal')&#xA;som.train(a, epochs, batch_size=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""165539"" LastEditorUserId=""22222"" LastEditDate=""2019-04-21T04:14:58.550"" LastActivityDate=""2019-04-21T04:14:58.550"" Title=""Simple Self-Organizing Map (SOM) in Tensorflow"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""216170"" PostTypeId=""1"" CreationDate=""2019-03-25T14:54:21.213"" Score=""2"" ViewCount=""1460"" Body=""&lt;p&gt;I've implemented a self-organising map in Tensorflow's low-level API. After testing with sklearn's Iris data, the results seem correct. I did implement the algorithm also using NumPy before converting it to &lt;code&gt;tf&lt;/code&gt;, because I'm new to Tensorflow, and had no idea whether it would work or not. So of course I tried out which would perform better. It turns out the Numpy implementation is far better for a smallish-scale problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To me this sounds a bit off. When loading a data set of ~3500x10 to GPU memory and carrying out basically the exact same algorithm with all-tf operations, I was at least expecting some sort of improvement with matrix-matrix broadcasted sums, multiplications and powers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I said, I have little experience with Tensorflow, so I started wondering if something is wrong with the code. The result seems good enough, but I'm not aware of many best practices or performance caveats. I'm especially after performance and TF-related feedback, but of course I'll happily take any criticism on my code!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import tensorflow as tf&#xA;&#xA;from tqdm import tqdm&#xA;&#xA;&#xA;def learning_rate(epoch: tf.placeholder, max_epochs: int):&#xA;    with tf.name_scope('learning_rate'):&#xA;        return tf.exp(-4 * epoch / max_epochs)&#xA;&#xA;&#xA;def neighbourhood(r: tf.placeholder, epoch: tf.placeholder, max_epochs: int, size: int):&#xA;    with tf.name_scope('neighbourhood'):&#xA;        return tf.exp(&#xA;            - (2 * r / size) ** 2&#xA;            * (max_epochs / (max_epochs - epoch)) ** 3&#xA;        )&#xA;&#xA;&#xA;class SelfOrganisingMap:&#xA;    def __init__(self, shape: tuple, features: int, *_,&#xA;                 max_epochs: int = None, init: str = 'uniform', learning_rate: float = 0.1):&#xA;        &quot;&quot;&quot;&#xA;        Self-organising map using TensorFlow.&#xA;&#xA;        :param shape: map dimensions&#xA;        :param features: number of input features&#xA;        :param _: used to force calling with keyword arguments below&#xA;        :param max_epochs: used to scale the neighbourhood and learning rate functions&#xA;        :param init: method of weight initialisation. 'uniform' for drawing from an uniform&#xA;            distribution between 0..1, 'normal' for drawing from X~N(0,1)&#xA;        :param learning_rate: initial learning rate multiplier&#xA;        &quot;&quot;&quot;&#xA;        self._weights = None&#xA;&#xA;        self._shape = shape&#xA;        self._features = features&#xA;        self._neighbour_shape = (len(shape),) + tuple(1 for _ in shape) + (-1,)&#xA;&#xA;        self._epochs = 0&#xA;        self._max_epochs = max_epochs&#xA;        self._initial_lr = learning_rate&#xA;&#xA;        if init == 'uniform':&#xA;            self._initialiser = tf.random_uniform_initializer&#xA;        elif init == 'normal':&#xA;            self._initialiser = tf.random_normal_initializer&#xA;        else:&#xA;            raise AssertionError('Unknown weights initialiser type &quot;%s&quot;!' % init)&#xA;&#xA;    @property&#xA;    def weights(self):&#xA;        if self._weights is None:&#xA;            raise ValueError('Map not fitted!')&#xA;        return self._weights&#xA;&#xA;    @property&#xA;    def initialiser(self):&#xA;        if self._weights is None:&#xA;            return self._initialiser&#xA;        else:&#xA;            return tf.convert_to_tensor(self._weights)&#xA;&#xA;    @property&#xA;    def shape(self): return self._shape&#xA;&#xA;    @property&#xA;    def n_nodes(self): return int(np.prod(self.shape))&#xA;&#xA;    @property&#xA;    def features(self): return self._features&#xA;&#xA;    @property&#xA;    def epochs(self): return self._epochs&#xA;&#xA;    @property&#xA;    def max_epochs(self): return self._max_epochs&#xA;&#xA;    def project(self, data: np.ndarray) -&amp;gt; np.array:&#xA;        &quot;&quot;&quot;&#xA;        Project data onto the map. NumPy implementation for simplicity.&#xA;&#xA;        :param data: samples&#xA;        :return: node indices&#xA;        &quot;&quot;&quot;&#xA;        diff = self.weights - data&#xA;        dist = np.sum(diff ** 2, axis=-1, keepdims=True)&#xA;        return np.array(np.unravel_index(&#xA;            np.argmin(dist.reshape((-1, data.shape[0])), axis=0), self.shape&#xA;        ))&#xA;&#xA;    def train(self, x: np.ndarray, epochs: int, batch_size: int = 1) -&amp;gt; None:&#xA;        &quot;&quot;&quot;&#xA;        Create training graph and train SOM.&#xA;&#xA;        :param x: training data&#xA;        :param epochs: number of epochs to train&#xA;        :param batch_size: number of training examples per step&#xA;        :return: None&#xA;        &quot;&quot;&quot;&#xA;        graph = tf.Graph()&#xA;        sess = tf.Session(graph=graph)&#xA;&#xA;        x = x.astype(np.float64)&#xA;&#xA;        if x.shape[0] % batch_size != 0:&#xA;            raise ValueError('Bad batch_size, last batch would be incomplete!')&#xA;&#xA;        # Construct graph&#xA;        with graph.as_default():&#xA;            indices = tf.convert_to_tensor(np.expand_dims(&#xA;                np.indices(self.shape, dtype=np.float64), axis=-1&#xA;            ))&#xA;            weights = tf.get_variable(&#xA;                'weights', (*self.shape, 1, self.features), initializer=self.initialiser, dtype=tf.float64&#xA;            )&#xA;&#xA;            with tf.name_scope('data'):&#xA;                data = tf.data.Dataset.from_tensor_slices(x)&#xA;                data = data.shuffle(buffer_size=10000).repeat(epochs)&#xA;                data = data.batch(batch_size, drop_remainder=True)&#xA;                data = data.make_one_shot_iterator().get_next()&#xA;&#xA;            with tf.name_scope('winner'):&#xA;                diff = weights - data&#xA;                dist = tf.reduce_sum(diff ** 2, axis=-1, keepdims=True)&#xA;                w_ix = tf.argmin(tf.reshape(dist, (self.n_nodes, data.shape[0])), axis=0)&#xA;                winner_op = tf.convert_to_tensor(tf.unravel_index(w_ix, self.shape))&#xA;&#xA;            with tf.name_scope('update'):&#xA;                curr_epoch = tf.placeholder(dtype=tf.int64, shape=())&#xA;&#xA;                idx_diff = indices - tf.reshape(tf.cast(&#xA;                    winner_op, dtype=tf.float64&#xA;                ), shape=self._neighbour_shape)&#xA;                idx_dist = tf.norm(idx_diff, axis=0)&#xA;&#xA;                l_rate = learning_rate(curr_epoch, self.max_epochs)&#xA;                n_hood = neighbourhood(&#xA;                    idx_dist, curr_epoch, self.max_epochs, max(self.shape)&#xA;                )&#xA;&#xA;                update = diff * l_rate * tf.expand_dims(n_hood, axis=-1)&#xA;                update_op = weights.assign(&#xA;                    weights - self._initial_lr * tf.reduce_sum(update, axis=-2, keepdims=True)&#xA;                )&#xA;&#xA;            init = tf.global_variables_initializer()&#xA;&#xA;        # Initialise all variables&#xA;        sess.run(init)&#xA;&#xA;        batches = int(np.ceil(x.shape[0] // batch_size))&#xA;        for i in tqdm(range(epochs)):&#xA;            for b in range(batches):&#xA;                sess.run(update_op, feed_dict={&#xA;                    curr_epoch: self.epochs + i&#xA;                })&#xA;&#xA;        self._weights = sess.run(weights)&#xA;        self._epochs += epochs&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's a short test snippet as well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.utils import shuffle&#xA;from sklearn.datasets import load_iris&#xA;from sklearn.preprocessing import RobustScaler&#xA;&#xA;a, _ = load_iris(True)&#xA;a, _ = shuffle(a, _)&#xA;a = RobustScaler().fit_transform(a)&#xA;&#xA;epochs = 100&#xA;som = SelfOrganisingMap((100, 100), 4, max_epochs=epochs, init='normal')&#xA;som.train(a, epochs, batch_size=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""165539"" LastEditorUserId=""22222"" LastEditDate=""2019-04-21T04:14:58.550"" LastActivityDate=""2019-04-21T04:14:58.550"" Title=""Simple Self-Organizing Map (SOM) in Tensorflow"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""237586"" PostTypeId=""1"" CreationDate=""2020-02-19T20:54:01.770"" Score=""3"" ViewCount=""62"" Body=""&lt;p&gt;I am trying to implement &lt;a href=&quot;https://arxiv.org/pdf/1712.04604&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deep Quaternion Networks&lt;/a&gt;. I was able to implement the batch normalization technique. But it requires a lot of GPU memory. Is there any way I can optimize the code provided below?&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt; class MyQuaternionBatchNorm2d(torch.nn.Module):&#xA;    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):&#xA;        super(MyQuaternionBatchNorm2d, self).__init__()&#xA;        self.num_features = num_features&#xA;        self.qnum_features = num_features//4&#xA;        self.eps = eps&#xA;        self.momentum = momentum&#xA;        self.affine = affine&#xA;        self.track_running_stats = track_running_stats&#xA;        &#xA;        if self.affine:&#xA;            self.weight = torch.nn.Parameter(torch.Tensor(self.qnum_features, 10))&#xA;            self.bias = torch.nn.Parameter(torch.Tensor(num_features))&#xA;        else:&#xA;            self.register_parameter('weight', None)&#xA;            self.register_parameter('bias', None)&#xA;            &#xA;        if self.track_running_stats:&#xA;            self.register_buffer('running_mean', torch.zeros(self.qnum_features,4))&#xA;            self.register_buffer('running_covar', torch.zeros(self.qnum_features,10))&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))&#xA;        else:&#xA;            self.register_buffer('running_mean',None)&#xA;            self.register_buffer('running_covar', None)&#xA;            self.register_parameter('num_batches_tracked', None)&#xA;        self.reset_parameters()&#xA;        &#xA;    def reset_running_stats(self):&#xA;        if self.track_running_stats:&#xA;            self.running_mean.zero_()&#xA;            self.running_covar.zero_()&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.num_batches_tracked.zero_()&#xA;&#xA;    def reset_parameters(self):&#xA;        self.reset_running_stats()&#xA;        &#xA;        if self.affine:&#xA;            torch.nn.init.zeros_(self.weight)&#xA;            torch.nn.init.constant_(self.weight[:,0], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,4], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,7], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,9], 1/ np.sqrt(4))&#xA;            torch.nn.init.zeros_(self.bias)&#xA;            &#xA;    def _check_input_dim(self, input):&#xA;        if input.dim() != 4:&#xA;            raise ValueError('expected 4D input (got {}D input)'&#xA;                             .format(input.dim()))&#xA;    &#xA;    @staticmethod&#xA;    def _decomposition_v1(r,i,j,k,Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk):&#xA;        Wrr = torch.sqrt(Vrr)&#xA;        Wri = (1.0 / Wrr) * (Vri)&#xA;        Wii = torch.sqrt((Vii - (Wri.pow(2))))&#xA;        Wrj = (1.0 / Wrr) * (Vrj)&#xA;        Wij = (1.0 / Wii) * (Vij - (Wri*Wrj))&#xA;        Wjj = torch.sqrt((Vjj - (Wij.pow(2) + Wrj.pow(2))))&#xA;        Wrk = (1.0 / Wrr) * (Vrk)&#xA;        Wik = (1.0 / Wii) * (Vik - (Wri*Wrk))&#xA;        Wjk = (1.0 / Wjj) * (Vjk - (Wij*Wik + Wrj*Wrk))&#xA;        Wkk = torch.sqrt((Vkk - (Wjk.pow(2) + Wik.pow(2) + Wrk.pow(2))))&#xA;        &#xA;        cat_W_1 = torch.cat([Wrr, Wri, Wrj, Wrk])&#xA;        cat_W_2 = torch.cat([Wri,Wii, Wij, Wik])&#xA;        cat_W_3 = torch.cat([Wrj, Wij, Wjj, Wjk])&#xA;        cat_W_4 = torch.cat([Wrk, Wik, Wjk, Wkk])&#xA;        &#xA;        output =  cat_W_1[None,:,None,None]  *  r.repeat(1,4,1,1) + cat_W_2[None,:,None,None] *   i.repeat(1,4,1,1) \&#xA;                    + cat_W_3[None,:,None,None]  *   j.repeat(1,4,1,1) +  cat_W_4[None,:,None,None]  *  k.repeat(1,4,1,1)&#xA;&#xA;        return output&#xA;    &#xA;    def forward(self, input):&#xA;        self._check_input_dim(input)&#xA;        r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;        &#xA;        exponential_average_factor = 0.0&#xA;&#xA;        if self.training and self.track_running_stats:&#xA;            if self.num_batches_tracked is not None:&#xA;                self.num_batches_tracked += 1&#xA;                if self.momentum is None:  # use cumulative moving average&#xA;                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)&#xA;                else:  # use exponential moving average&#xA;                    exponential_average_factor = self.momentum&#xA;&#xA;        # calculate running estimates&#xA;        if self.training:&#xA;            mean_r, mean_i, mean_j, mean_k = r.mean([0, 2, 3]),i.mean([0, 2, 3]),j.mean([0, 2, 3]),k.mean([0, 2, 3])&#xA;            n = input.numel() / input.size(1)&#xA;            mean = torch.stack((mean_r, mean_i, mean_j, mean_k), dim=1)&#xA;            # update running mean&#xA;            with torch.no_grad():&#xA;                self.running_mean = exponential_average_factor * mean + (1 - exponential_average_factor) * self.running_mean&#xA;                    &#xA;            r = r-mean_r[None, :, None, None]&#xA;            i = i-mean_i[None, :, None, None]&#xA;            j = j-mean_j[None, :, None, None]&#xA;            k = k-mean_k[None, :, None, None]&#xA;            &#xA;            Vrr = (r.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vii = (i.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vjj = (j.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vkk = (k.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vri = ((r*i).mean([0, 2, 3]))&#xA;            Vrj = ((r*j).mean([0, 2, 3]))&#xA;            Vrk = ((r*k).mean([0, 2, 3]))&#xA;            Vij = ((i*j).mean([0, 2, 3]))&#xA;            Vik = ((i*k).mean([0, 2, 3]))&#xA;            Vjk = ((j*k).mean([0, 2, 3])) &#xA;&#xA;            with torch.no_grad():&#xA;                self.running_covar[:,0] = exponential_average_factor * Vrr * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,0]&#xA;                self.running_covar[:,1] = exponential_average_factor * Vii * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,1]&#xA;                self.running_covar[:,2] = exponential_average_factor * Vjj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,2]&#xA;                self.running_covar[:,3] = exponential_average_factor * Vkk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,3]&#xA;                &#xA;                self.running_covar[:,4] = exponential_average_factor * Vri * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,4]&#xA;                self.running_covar[:,5] = exponential_average_factor * Vrj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,5]&#xA;                self.running_covar[:,6] = exponential_average_factor * Vrk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,6]&#xA;                self.running_covar[:,7] = exponential_average_factor * Vij * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,7]&#xA;                self.running_covar[:,8] = exponential_average_factor * Vik * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,8]&#xA;                self.running_covar[:,9] = exponential_average_factor * Vjk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,9]&#xA;        else:&#xA;            mean = self.running_mean&#xA;            Vrr = self.running_covar[:,0]+self.eps&#xA;            Vii = self.running_covar[:,1]+self.eps&#xA;            Vjj = self.running_covar[:,2]+self.eps&#xA;            Vkk = self.running_covar[:,3]+self.eps&#xA;            &#xA;            Vri = self.running_covar[:,4]+self.eps&#xA;            Vrj = self.running_covar[:,5]+self.eps&#xA;            Vrk = self.running_covar[:,6]+self.eps&#xA;            Vij = self.running_covar[:,7]+self.eps&#xA;            Vik = self.running_covar[:,8]+self.eps&#xA;            Vjk = self.running_covar[:,9]+self.eps&#xA;           &#xA;            r = r-mean[None,:,0,None,None]&#xA;            i = i-mean[None,:,1,None,None]&#xA;            j = j-mean[None,:,2,None,None]&#xA;            k = k-mean[None,:,3,None,None]&#xA;            &#xA;        # standardized_output&#xA;        input = self._decomposition_v1(r,i,j,k, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk)&#xA;        &#xA;        if self.affine:&#xA;            r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;            &#xA;            cat_gamma_1 = torch.cat([self.weight[:,0], self.weight[:,1], self.weight[:,2], self.weight[:,3]])&#xA;            cat_gamma_2 = torch.cat([self.weight[:,1], self.weight[:,4], self.weight[:,5], self.weight[:,6]])&#xA;            cat_gamma_3 = torch.cat([self.weight[:,2], self.weight[:,5], self.weight[:,7], self.weight[:,8]])&#xA;            cat_gamma_4 = torch.cat([self.weight[:,3], self.weight[:,6], self.weight[:,8], self.weight[:,9]])&#xA;&#xA;&#xA;            input =  cat_gamma_1[None,:,None,None] * r.repeat(1,4,1,1) \&#xA;                    + cat_gamma_2[None,:,None,None] * i.repeat(1,4,1,1) \&#xA;                    + cat_gamma_3[None,:,None,None] * j.repeat(1,4,1,1) \&#xA;                    + cat_gamma_4[None,:,None,None] * k.repeat(1,4,1,1) \&#xA;                    + self.bias[None, :, None, None]&#xA;        return input&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I will explain the forward section. So the basic formula for batch normalization is &lt;code&gt;x* = (x - E[x]) / sqrt(var(x))&lt;/code&gt;, where &lt;code&gt;x*&lt;/code&gt; is the new value of a single component, &lt;code&gt;E[x]&lt;/code&gt; is its mean within a batch and &lt;code&gt;var(x)&lt;/code&gt; is its variance within a batch.&lt;/p&gt;&#xA;&lt;p&gt;However, as it is quaternion batch normalization,it has 4 parts &lt;code&gt;r&lt;/code&gt; which is the real part and &lt;code&gt;i, j, and k&lt;/code&gt;, are the imaginary part.&lt;/p&gt;&#xA;&lt;p&gt;The equation extends to &lt;code&gt;x* = W(x - E[x]) / (var(x))&lt;/code&gt;. &lt;code&gt;W&lt;/code&gt; is one of the matrices from the Cholesky decomposition of &lt;code&gt;V^-1&lt;/code&gt; where &lt;code&gt;V&lt;/code&gt; is the variance.In the code,&lt;code&gt;E[x]&lt;/code&gt; is computed using the &lt;code&gt;mean&lt;/code&gt; variable. V is computed in &lt;code&gt;Vxy&lt;/code&gt; and &lt;code&gt;V^-1&lt;/code&gt; i.e. &lt;code&gt;W&lt;/code&gt; is computed in the &lt;code&gt;_decomposition_v1&lt;/code&gt; function. This is applied to the input.&lt;/p&gt;&#xA;&lt;p&gt;Finally, that formula further extends to &lt;code&gt;x** = gamma * x* + beta&lt;/code&gt;, where &lt;code&gt;x**&lt;/code&gt; is the final normalized value. &lt;code&gt;gamma&lt;/code&gt; i.e. &lt;code&gt;cat_gamma_x&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; i.e. &lt;code&gt;self.beta&lt;/code&gt; are learned per layer.&lt;/p&gt;&#xA;&lt;p&gt;Note: The num_features need to be in multiples of 4.&lt;/p&gt;&#xA;&lt;p&gt;Thank you,&#xA;Shreyas&lt;/p&gt;&#xA;"" OwnerUserId=""108888"" LastEditorUserId=""-1"" LastEditDate=""2020-06-10T13:24:26.273"" LastActivityDate=""2020-02-20T03:27:54.440"" Title=""Batch normalization code optimization?"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;memory-optimization&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""2"" FavoriteCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""237586"" PostTypeId=""1"" CreationDate=""2020-02-19T20:54:01.770"" Score=""3"" ViewCount=""62"" Body=""&lt;p&gt;I am trying to implement &lt;a href=&quot;https://arxiv.org/pdf/1712.04604&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deep Quaternion Networks&lt;/a&gt;. I was able to implement the batch normalization technique. But it requires a lot of GPU memory. Is there any way I can optimize the code provided below?&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt; class MyQuaternionBatchNorm2d(torch.nn.Module):&#xA;    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):&#xA;        super(MyQuaternionBatchNorm2d, self).__init__()&#xA;        self.num_features = num_features&#xA;        self.qnum_features = num_features//4&#xA;        self.eps = eps&#xA;        self.momentum = momentum&#xA;        self.affine = affine&#xA;        self.track_running_stats = track_running_stats&#xA;        &#xA;        if self.affine:&#xA;            self.weight = torch.nn.Parameter(torch.Tensor(self.qnum_features, 10))&#xA;            self.bias = torch.nn.Parameter(torch.Tensor(num_features))&#xA;        else:&#xA;            self.register_parameter('weight', None)&#xA;            self.register_parameter('bias', None)&#xA;            &#xA;        if self.track_running_stats:&#xA;            self.register_buffer('running_mean', torch.zeros(self.qnum_features,4))&#xA;            self.register_buffer('running_covar', torch.zeros(self.qnum_features,10))&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))&#xA;        else:&#xA;            self.register_buffer('running_mean',None)&#xA;            self.register_buffer('running_covar', None)&#xA;            self.register_parameter('num_batches_tracked', None)&#xA;        self.reset_parameters()&#xA;        &#xA;    def reset_running_stats(self):&#xA;        if self.track_running_stats:&#xA;            self.running_mean.zero_()&#xA;            self.running_covar.zero_()&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.num_batches_tracked.zero_()&#xA;&#xA;    def reset_parameters(self):&#xA;        self.reset_running_stats()&#xA;        &#xA;        if self.affine:&#xA;            torch.nn.init.zeros_(self.weight)&#xA;            torch.nn.init.constant_(self.weight[:,0], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,4], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,7], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,9], 1/ np.sqrt(4))&#xA;            torch.nn.init.zeros_(self.bias)&#xA;            &#xA;    def _check_input_dim(self, input):&#xA;        if input.dim() != 4:&#xA;            raise ValueError('expected 4D input (got {}D input)'&#xA;                             .format(input.dim()))&#xA;    &#xA;    @staticmethod&#xA;    def _decomposition_v1(r,i,j,k,Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk):&#xA;        Wrr = torch.sqrt(Vrr)&#xA;        Wri = (1.0 / Wrr) * (Vri)&#xA;        Wii = torch.sqrt((Vii - (Wri.pow(2))))&#xA;        Wrj = (1.0 / Wrr) * (Vrj)&#xA;        Wij = (1.0 / Wii) * (Vij - (Wri*Wrj))&#xA;        Wjj = torch.sqrt((Vjj - (Wij.pow(2) + Wrj.pow(2))))&#xA;        Wrk = (1.0 / Wrr) * (Vrk)&#xA;        Wik = (1.0 / Wii) * (Vik - (Wri*Wrk))&#xA;        Wjk = (1.0 / Wjj) * (Vjk - (Wij*Wik + Wrj*Wrk))&#xA;        Wkk = torch.sqrt((Vkk - (Wjk.pow(2) + Wik.pow(2) + Wrk.pow(2))))&#xA;        &#xA;        cat_W_1 = torch.cat([Wrr, Wri, Wrj, Wrk])&#xA;        cat_W_2 = torch.cat([Wri,Wii, Wij, Wik])&#xA;        cat_W_3 = torch.cat([Wrj, Wij, Wjj, Wjk])&#xA;        cat_W_4 = torch.cat([Wrk, Wik, Wjk, Wkk])&#xA;        &#xA;        output =  cat_W_1[None,:,None,None]  *  r.repeat(1,4,1,1) + cat_W_2[None,:,None,None] *   i.repeat(1,4,1,1) \&#xA;                    + cat_W_3[None,:,None,None]  *   j.repeat(1,4,1,1) +  cat_W_4[None,:,None,None]  *  k.repeat(1,4,1,1)&#xA;&#xA;        return output&#xA;    &#xA;    def forward(self, input):&#xA;        self._check_input_dim(input)&#xA;        r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;        &#xA;        exponential_average_factor = 0.0&#xA;&#xA;        if self.training and self.track_running_stats:&#xA;            if self.num_batches_tracked is not None:&#xA;                self.num_batches_tracked += 1&#xA;                if self.momentum is None:  # use cumulative moving average&#xA;                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)&#xA;                else:  # use exponential moving average&#xA;                    exponential_average_factor = self.momentum&#xA;&#xA;        # calculate running estimates&#xA;        if self.training:&#xA;            mean_r, mean_i, mean_j, mean_k = r.mean([0, 2, 3]),i.mean([0, 2, 3]),j.mean([0, 2, 3]),k.mean([0, 2, 3])&#xA;            n = input.numel() / input.size(1)&#xA;            mean = torch.stack((mean_r, mean_i, mean_j, mean_k), dim=1)&#xA;            # update running mean&#xA;            with torch.no_grad():&#xA;                self.running_mean = exponential_average_factor * mean + (1 - exponential_average_factor) * self.running_mean&#xA;                    &#xA;            r = r-mean_r[None, :, None, None]&#xA;            i = i-mean_i[None, :, None, None]&#xA;            j = j-mean_j[None, :, None, None]&#xA;            k = k-mean_k[None, :, None, None]&#xA;            &#xA;            Vrr = (r.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vii = (i.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vjj = (j.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vkk = (k.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vri = ((r*i).mean([0, 2, 3]))&#xA;            Vrj = ((r*j).mean([0, 2, 3]))&#xA;            Vrk = ((r*k).mean([0, 2, 3]))&#xA;            Vij = ((i*j).mean([0, 2, 3]))&#xA;            Vik = ((i*k).mean([0, 2, 3]))&#xA;            Vjk = ((j*k).mean([0, 2, 3])) &#xA;&#xA;            with torch.no_grad():&#xA;                self.running_covar[:,0] = exponential_average_factor * Vrr * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,0]&#xA;                self.running_covar[:,1] = exponential_average_factor * Vii * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,1]&#xA;                self.running_covar[:,2] = exponential_average_factor * Vjj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,2]&#xA;                self.running_covar[:,3] = exponential_average_factor * Vkk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,3]&#xA;                &#xA;                self.running_covar[:,4] = exponential_average_factor * Vri * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,4]&#xA;                self.running_covar[:,5] = exponential_average_factor * Vrj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,5]&#xA;                self.running_covar[:,6] = exponential_average_factor * Vrk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,6]&#xA;                self.running_covar[:,7] = exponential_average_factor * Vij * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,7]&#xA;                self.running_covar[:,8] = exponential_average_factor * Vik * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,8]&#xA;                self.running_covar[:,9] = exponential_average_factor * Vjk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,9]&#xA;        else:&#xA;            mean = self.running_mean&#xA;            Vrr = self.running_covar[:,0]+self.eps&#xA;            Vii = self.running_covar[:,1]+self.eps&#xA;            Vjj = self.running_covar[:,2]+self.eps&#xA;            Vkk = self.running_covar[:,3]+self.eps&#xA;            &#xA;            Vri = self.running_covar[:,4]+self.eps&#xA;            Vrj = self.running_covar[:,5]+self.eps&#xA;            Vrk = self.running_covar[:,6]+self.eps&#xA;            Vij = self.running_covar[:,7]+self.eps&#xA;            Vik = self.running_covar[:,8]+self.eps&#xA;            Vjk = self.running_covar[:,9]+self.eps&#xA;           &#xA;            r = r-mean[None,:,0,None,None]&#xA;            i = i-mean[None,:,1,None,None]&#xA;            j = j-mean[None,:,2,None,None]&#xA;            k = k-mean[None,:,3,None,None]&#xA;            &#xA;        # standardized_output&#xA;        input = self._decomposition_v1(r,i,j,k, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk)&#xA;        &#xA;        if self.affine:&#xA;            r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;            &#xA;            cat_gamma_1 = torch.cat([self.weight[:,0], self.weight[:,1], self.weight[:,2], self.weight[:,3]])&#xA;            cat_gamma_2 = torch.cat([self.weight[:,1], self.weight[:,4], self.weight[:,5], self.weight[:,6]])&#xA;            cat_gamma_3 = torch.cat([self.weight[:,2], self.weight[:,5], self.weight[:,7], self.weight[:,8]])&#xA;            cat_gamma_4 = torch.cat([self.weight[:,3], self.weight[:,6], self.weight[:,8], self.weight[:,9]])&#xA;&#xA;&#xA;            input =  cat_gamma_1[None,:,None,None] * r.repeat(1,4,1,1) \&#xA;                    + cat_gamma_2[None,:,None,None] * i.repeat(1,4,1,1) \&#xA;                    + cat_gamma_3[None,:,None,None] * j.repeat(1,4,1,1) \&#xA;                    + cat_gamma_4[None,:,None,None] * k.repeat(1,4,1,1) \&#xA;                    + self.bias[None, :, None, None]&#xA;        return input&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I will explain the forward section. So the basic formula for batch normalization is &lt;code&gt;x* = (x - E[x]) / sqrt(var(x))&lt;/code&gt;, where &lt;code&gt;x*&lt;/code&gt; is the new value of a single component, &lt;code&gt;E[x]&lt;/code&gt; is its mean within a batch and &lt;code&gt;var(x)&lt;/code&gt; is its variance within a batch.&lt;/p&gt;&#xA;&lt;p&gt;However, as it is quaternion batch normalization,it has 4 parts &lt;code&gt;r&lt;/code&gt; which is the real part and &lt;code&gt;i, j, and k&lt;/code&gt;, are the imaginary part.&lt;/p&gt;&#xA;&lt;p&gt;The equation extends to &lt;code&gt;x* = W(x - E[x]) / (var(x))&lt;/code&gt;. &lt;code&gt;W&lt;/code&gt; is one of the matrices from the Cholesky decomposition of &lt;code&gt;V^-1&lt;/code&gt; where &lt;code&gt;V&lt;/code&gt; is the variance.In the code,&lt;code&gt;E[x]&lt;/code&gt; is computed using the &lt;code&gt;mean&lt;/code&gt; variable. V is computed in &lt;code&gt;Vxy&lt;/code&gt; and &lt;code&gt;V^-1&lt;/code&gt; i.e. &lt;code&gt;W&lt;/code&gt; is computed in the &lt;code&gt;_decomposition_v1&lt;/code&gt; function. This is applied to the input.&lt;/p&gt;&#xA;&lt;p&gt;Finally, that formula further extends to &lt;code&gt;x** = gamma * x* + beta&lt;/code&gt;, where &lt;code&gt;x**&lt;/code&gt; is the final normalized value. &lt;code&gt;gamma&lt;/code&gt; i.e. &lt;code&gt;cat_gamma_x&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; i.e. &lt;code&gt;self.beta&lt;/code&gt; are learned per layer.&lt;/p&gt;&#xA;&lt;p&gt;Note: The num_features need to be in multiples of 4.&lt;/p&gt;&#xA;&lt;p&gt;Thank you,&#xA;Shreyas&lt;/p&gt;&#xA;"" OwnerUserId=""108888"" LastEditorUserId=""-1"" LastEditDate=""2020-06-10T13:24:26.273"" LastActivityDate=""2020-02-20T03:27:54.440"" Title=""Batch normalization code optimization?"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;memory-optimization&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""2"" FavoriteCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""237586"" PostTypeId=""1"" CreationDate=""2020-02-19T20:54:01.770"" Score=""3"" ViewCount=""62"" Body=""&lt;p&gt;I am trying to implement &lt;a href=&quot;https://arxiv.org/pdf/1712.04604&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deep Quaternion Networks&lt;/a&gt;. I was able to implement the batch normalization technique. But it requires a lot of GPU memory. Is there any way I can optimize the code provided below?&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt; class MyQuaternionBatchNorm2d(torch.nn.Module):&#xA;    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):&#xA;        super(MyQuaternionBatchNorm2d, self).__init__()&#xA;        self.num_features = num_features&#xA;        self.qnum_features = num_features//4&#xA;        self.eps = eps&#xA;        self.momentum = momentum&#xA;        self.affine = affine&#xA;        self.track_running_stats = track_running_stats&#xA;        &#xA;        if self.affine:&#xA;            self.weight = torch.nn.Parameter(torch.Tensor(self.qnum_features, 10))&#xA;            self.bias = torch.nn.Parameter(torch.Tensor(num_features))&#xA;        else:&#xA;            self.register_parameter('weight', None)&#xA;            self.register_parameter('bias', None)&#xA;            &#xA;        if self.track_running_stats:&#xA;            self.register_buffer('running_mean', torch.zeros(self.qnum_features,4))&#xA;            self.register_buffer('running_covar', torch.zeros(self.qnum_features,10))&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))&#xA;        else:&#xA;            self.register_buffer('running_mean',None)&#xA;            self.register_buffer('running_covar', None)&#xA;            self.register_parameter('num_batches_tracked', None)&#xA;        self.reset_parameters()&#xA;        &#xA;    def reset_running_stats(self):&#xA;        if self.track_running_stats:&#xA;            self.running_mean.zero_()&#xA;            self.running_covar.zero_()&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.num_batches_tracked.zero_()&#xA;&#xA;    def reset_parameters(self):&#xA;        self.reset_running_stats()&#xA;        &#xA;        if self.affine:&#xA;            torch.nn.init.zeros_(self.weight)&#xA;            torch.nn.init.constant_(self.weight[:,0], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,4], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,7], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,9], 1/ np.sqrt(4))&#xA;            torch.nn.init.zeros_(self.bias)&#xA;            &#xA;    def _check_input_dim(self, input):&#xA;        if input.dim() != 4:&#xA;            raise ValueError('expected 4D input (got {}D input)'&#xA;                             .format(input.dim()))&#xA;    &#xA;    @staticmethod&#xA;    def _decomposition_v1(r,i,j,k,Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk):&#xA;        Wrr = torch.sqrt(Vrr)&#xA;        Wri = (1.0 / Wrr) * (Vri)&#xA;        Wii = torch.sqrt((Vii - (Wri.pow(2))))&#xA;        Wrj = (1.0 / Wrr) * (Vrj)&#xA;        Wij = (1.0 / Wii) * (Vij - (Wri*Wrj))&#xA;        Wjj = torch.sqrt((Vjj - (Wij.pow(2) + Wrj.pow(2))))&#xA;        Wrk = (1.0 / Wrr) * (Vrk)&#xA;        Wik = (1.0 / Wii) * (Vik - (Wri*Wrk))&#xA;        Wjk = (1.0 / Wjj) * (Vjk - (Wij*Wik + Wrj*Wrk))&#xA;        Wkk = torch.sqrt((Vkk - (Wjk.pow(2) + Wik.pow(2) + Wrk.pow(2))))&#xA;        &#xA;        cat_W_1 = torch.cat([Wrr, Wri, Wrj, Wrk])&#xA;        cat_W_2 = torch.cat([Wri,Wii, Wij, Wik])&#xA;        cat_W_3 = torch.cat([Wrj, Wij, Wjj, Wjk])&#xA;        cat_W_4 = torch.cat([Wrk, Wik, Wjk, Wkk])&#xA;        &#xA;        output =  cat_W_1[None,:,None,None]  *  r.repeat(1,4,1,1) + cat_W_2[None,:,None,None] *   i.repeat(1,4,1,1) \&#xA;                    + cat_W_3[None,:,None,None]  *   j.repeat(1,4,1,1) +  cat_W_4[None,:,None,None]  *  k.repeat(1,4,1,1)&#xA;&#xA;        return output&#xA;    &#xA;    def forward(self, input):&#xA;        self._check_input_dim(input)&#xA;        r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;        &#xA;        exponential_average_factor = 0.0&#xA;&#xA;        if self.training and self.track_running_stats:&#xA;            if self.num_batches_tracked is not None:&#xA;                self.num_batches_tracked += 1&#xA;                if self.momentum is None:  # use cumulative moving average&#xA;                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)&#xA;                else:  # use exponential moving average&#xA;                    exponential_average_factor = self.momentum&#xA;&#xA;        # calculate running estimates&#xA;        if self.training:&#xA;            mean_r, mean_i, mean_j, mean_k = r.mean([0, 2, 3]),i.mean([0, 2, 3]),j.mean([0, 2, 3]),k.mean([0, 2, 3])&#xA;            n = input.numel() / input.size(1)&#xA;            mean = torch.stack((mean_r, mean_i, mean_j, mean_k), dim=1)&#xA;            # update running mean&#xA;            with torch.no_grad():&#xA;                self.running_mean = exponential_average_factor * mean + (1 - exponential_average_factor) * self.running_mean&#xA;                    &#xA;            r = r-mean_r[None, :, None, None]&#xA;            i = i-mean_i[None, :, None, None]&#xA;            j = j-mean_j[None, :, None, None]&#xA;            k = k-mean_k[None, :, None, None]&#xA;            &#xA;            Vrr = (r.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vii = (i.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vjj = (j.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vkk = (k.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vri = ((r*i).mean([0, 2, 3]))&#xA;            Vrj = ((r*j).mean([0, 2, 3]))&#xA;            Vrk = ((r*k).mean([0, 2, 3]))&#xA;            Vij = ((i*j).mean([0, 2, 3]))&#xA;            Vik = ((i*k).mean([0, 2, 3]))&#xA;            Vjk = ((j*k).mean([0, 2, 3])) &#xA;&#xA;            with torch.no_grad():&#xA;                self.running_covar[:,0] = exponential_average_factor * Vrr * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,0]&#xA;                self.running_covar[:,1] = exponential_average_factor * Vii * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,1]&#xA;                self.running_covar[:,2] = exponential_average_factor * Vjj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,2]&#xA;                self.running_covar[:,3] = exponential_average_factor * Vkk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,3]&#xA;                &#xA;                self.running_covar[:,4] = exponential_average_factor * Vri * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,4]&#xA;                self.running_covar[:,5] = exponential_average_factor * Vrj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,5]&#xA;                self.running_covar[:,6] = exponential_average_factor * Vrk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,6]&#xA;                self.running_covar[:,7] = exponential_average_factor * Vij * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,7]&#xA;                self.running_covar[:,8] = exponential_average_factor * Vik * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,8]&#xA;                self.running_covar[:,9] = exponential_average_factor * Vjk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,9]&#xA;        else:&#xA;            mean = self.running_mean&#xA;            Vrr = self.running_covar[:,0]+self.eps&#xA;            Vii = self.running_covar[:,1]+self.eps&#xA;            Vjj = self.running_covar[:,2]+self.eps&#xA;            Vkk = self.running_covar[:,3]+self.eps&#xA;            &#xA;            Vri = self.running_covar[:,4]+self.eps&#xA;            Vrj = self.running_covar[:,5]+self.eps&#xA;            Vrk = self.running_covar[:,6]+self.eps&#xA;            Vij = self.running_covar[:,7]+self.eps&#xA;            Vik = self.running_covar[:,8]+self.eps&#xA;            Vjk = self.running_covar[:,9]+self.eps&#xA;           &#xA;            r = r-mean[None,:,0,None,None]&#xA;            i = i-mean[None,:,1,None,None]&#xA;            j = j-mean[None,:,2,None,None]&#xA;            k = k-mean[None,:,3,None,None]&#xA;            &#xA;        # standardized_output&#xA;        input = self._decomposition_v1(r,i,j,k, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk)&#xA;        &#xA;        if self.affine:&#xA;            r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;            &#xA;            cat_gamma_1 = torch.cat([self.weight[:,0], self.weight[:,1], self.weight[:,2], self.weight[:,3]])&#xA;            cat_gamma_2 = torch.cat([self.weight[:,1], self.weight[:,4], self.weight[:,5], self.weight[:,6]])&#xA;            cat_gamma_3 = torch.cat([self.weight[:,2], self.weight[:,5], self.weight[:,7], self.weight[:,8]])&#xA;            cat_gamma_4 = torch.cat([self.weight[:,3], self.weight[:,6], self.weight[:,8], self.weight[:,9]])&#xA;&#xA;&#xA;            input =  cat_gamma_1[None,:,None,None] * r.repeat(1,4,1,1) \&#xA;                    + cat_gamma_2[None,:,None,None] * i.repeat(1,4,1,1) \&#xA;                    + cat_gamma_3[None,:,None,None] * j.repeat(1,4,1,1) \&#xA;                    + cat_gamma_4[None,:,None,None] * k.repeat(1,4,1,1) \&#xA;                    + self.bias[None, :, None, None]&#xA;        return input&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I will explain the forward section. So the basic formula for batch normalization is &lt;code&gt;x* = (x - E[x]) / sqrt(var(x))&lt;/code&gt;, where &lt;code&gt;x*&lt;/code&gt; is the new value of a single component, &lt;code&gt;E[x]&lt;/code&gt; is its mean within a batch and &lt;code&gt;var(x)&lt;/code&gt; is its variance within a batch.&lt;/p&gt;&#xA;&lt;p&gt;However, as it is quaternion batch normalization,it has 4 parts &lt;code&gt;r&lt;/code&gt; which is the real part and &lt;code&gt;i, j, and k&lt;/code&gt;, are the imaginary part.&lt;/p&gt;&#xA;&lt;p&gt;The equation extends to &lt;code&gt;x* = W(x - E[x]) / (var(x))&lt;/code&gt;. &lt;code&gt;W&lt;/code&gt; is one of the matrices from the Cholesky decomposition of &lt;code&gt;V^-1&lt;/code&gt; where &lt;code&gt;V&lt;/code&gt; is the variance.In the code,&lt;code&gt;E[x]&lt;/code&gt; is computed using the &lt;code&gt;mean&lt;/code&gt; variable. V is computed in &lt;code&gt;Vxy&lt;/code&gt; and &lt;code&gt;V^-1&lt;/code&gt; i.e. &lt;code&gt;W&lt;/code&gt; is computed in the &lt;code&gt;_decomposition_v1&lt;/code&gt; function. This is applied to the input.&lt;/p&gt;&#xA;&lt;p&gt;Finally, that formula further extends to &lt;code&gt;x** = gamma * x* + beta&lt;/code&gt;, where &lt;code&gt;x**&lt;/code&gt; is the final normalized value. &lt;code&gt;gamma&lt;/code&gt; i.e. &lt;code&gt;cat_gamma_x&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; i.e. &lt;code&gt;self.beta&lt;/code&gt; are learned per layer.&lt;/p&gt;&#xA;&lt;p&gt;Note: The num_features need to be in multiples of 4.&lt;/p&gt;&#xA;&lt;p&gt;Thank you,&#xA;Shreyas&lt;/p&gt;&#xA;"" OwnerUserId=""108888"" LastEditorUserId=""-1"" LastEditDate=""2020-06-10T13:24:26.273"" LastActivityDate=""2020-02-20T03:27:54.440"" Title=""Batch normalization code optimization?"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;memory-optimization&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""2"" FavoriteCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\codereview.stackexchange.com,"  <row Id=""237586"" PostTypeId=""1"" CreationDate=""2020-02-19T20:54:01.770"" Score=""3"" ViewCount=""62"" Body=""&lt;p&gt;I am trying to implement &lt;a href=&quot;https://arxiv.org/pdf/1712.04604&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deep Quaternion Networks&lt;/a&gt;. I was able to implement the batch normalization technique. But it requires a lot of GPU memory. Is there any way I can optimize the code provided below?&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt; class MyQuaternionBatchNorm2d(torch.nn.Module):&#xA;    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):&#xA;        super(MyQuaternionBatchNorm2d, self).__init__()&#xA;        self.num_features = num_features&#xA;        self.qnum_features = num_features//4&#xA;        self.eps = eps&#xA;        self.momentum = momentum&#xA;        self.affine = affine&#xA;        self.track_running_stats = track_running_stats&#xA;        &#xA;        if self.affine:&#xA;            self.weight = torch.nn.Parameter(torch.Tensor(self.qnum_features, 10))&#xA;            self.bias = torch.nn.Parameter(torch.Tensor(num_features))&#xA;        else:&#xA;            self.register_parameter('weight', None)&#xA;            self.register_parameter('bias', None)&#xA;            &#xA;        if self.track_running_stats:&#xA;            self.register_buffer('running_mean', torch.zeros(self.qnum_features,4))&#xA;            self.register_buffer('running_covar', torch.zeros(self.qnum_features,10))&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))&#xA;        else:&#xA;            self.register_buffer('running_mean',None)&#xA;            self.register_buffer('running_covar', None)&#xA;            self.register_parameter('num_batches_tracked', None)&#xA;        self.reset_parameters()&#xA;        &#xA;    def reset_running_stats(self):&#xA;        if self.track_running_stats:&#xA;            self.running_mean.zero_()&#xA;            self.running_covar.zero_()&#xA;            self.running_covar[:,0] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,1] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,2] = 1/ np.sqrt(4)&#xA;            self.running_covar[:,3] = 1/ np.sqrt(4)&#xA;            self.num_batches_tracked.zero_()&#xA;&#xA;    def reset_parameters(self):&#xA;        self.reset_running_stats()&#xA;        &#xA;        if self.affine:&#xA;            torch.nn.init.zeros_(self.weight)&#xA;            torch.nn.init.constant_(self.weight[:,0], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,4], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,7], 1/ np.sqrt(4))&#xA;            torch.nn.init.constant_(self.weight[:,9], 1/ np.sqrt(4))&#xA;            torch.nn.init.zeros_(self.bias)&#xA;            &#xA;    def _check_input_dim(self, input):&#xA;        if input.dim() != 4:&#xA;            raise ValueError('expected 4D input (got {}D input)'&#xA;                             .format(input.dim()))&#xA;    &#xA;    @staticmethod&#xA;    def _decomposition_v1(r,i,j,k,Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk):&#xA;        Wrr = torch.sqrt(Vrr)&#xA;        Wri = (1.0 / Wrr) * (Vri)&#xA;        Wii = torch.sqrt((Vii - (Wri.pow(2))))&#xA;        Wrj = (1.0 / Wrr) * (Vrj)&#xA;        Wij = (1.0 / Wii) * (Vij - (Wri*Wrj))&#xA;        Wjj = torch.sqrt((Vjj - (Wij.pow(2) + Wrj.pow(2))))&#xA;        Wrk = (1.0 / Wrr) * (Vrk)&#xA;        Wik = (1.0 / Wii) * (Vik - (Wri*Wrk))&#xA;        Wjk = (1.0 / Wjj) * (Vjk - (Wij*Wik + Wrj*Wrk))&#xA;        Wkk = torch.sqrt((Vkk - (Wjk.pow(2) + Wik.pow(2) + Wrk.pow(2))))&#xA;        &#xA;        cat_W_1 = torch.cat([Wrr, Wri, Wrj, Wrk])&#xA;        cat_W_2 = torch.cat([Wri,Wii, Wij, Wik])&#xA;        cat_W_3 = torch.cat([Wrj, Wij, Wjj, Wjk])&#xA;        cat_W_4 = torch.cat([Wrk, Wik, Wjk, Wkk])&#xA;        &#xA;        output =  cat_W_1[None,:,None,None]  *  r.repeat(1,4,1,1) + cat_W_2[None,:,None,None] *   i.repeat(1,4,1,1) \&#xA;                    + cat_W_3[None,:,None,None]  *   j.repeat(1,4,1,1) +  cat_W_4[None,:,None,None]  *  k.repeat(1,4,1,1)&#xA;&#xA;        return output&#xA;    &#xA;    def forward(self, input):&#xA;        self._check_input_dim(input)&#xA;        r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;        &#xA;        exponential_average_factor = 0.0&#xA;&#xA;        if self.training and self.track_running_stats:&#xA;            if self.num_batches_tracked is not None:&#xA;                self.num_batches_tracked += 1&#xA;                if self.momentum is None:  # use cumulative moving average&#xA;                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)&#xA;                else:  # use exponential moving average&#xA;                    exponential_average_factor = self.momentum&#xA;&#xA;        # calculate running estimates&#xA;        if self.training:&#xA;            mean_r, mean_i, mean_j, mean_k = r.mean([0, 2, 3]),i.mean([0, 2, 3]),j.mean([0, 2, 3]),k.mean([0, 2, 3])&#xA;            n = input.numel() / input.size(1)&#xA;            mean = torch.stack((mean_r, mean_i, mean_j, mean_k), dim=1)&#xA;            # update running mean&#xA;            with torch.no_grad():&#xA;                self.running_mean = exponential_average_factor * mean + (1 - exponential_average_factor) * self.running_mean&#xA;                    &#xA;            r = r-mean_r[None, :, None, None]&#xA;            i = i-mean_i[None, :, None, None]&#xA;            j = j-mean_j[None, :, None, None]&#xA;            k = k-mean_k[None, :, None, None]&#xA;            &#xA;            Vrr = (r.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vii = (i.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vjj = (j.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vkk = (k.pow(2).mean([0, 2, 3])) + self.eps&#xA;            Vri = ((r*i).mean([0, 2, 3]))&#xA;            Vrj = ((r*j).mean([0, 2, 3]))&#xA;            Vrk = ((r*k).mean([0, 2, 3]))&#xA;            Vij = ((i*j).mean([0, 2, 3]))&#xA;            Vik = ((i*k).mean([0, 2, 3]))&#xA;            Vjk = ((j*k).mean([0, 2, 3])) &#xA;&#xA;            with torch.no_grad():&#xA;                self.running_covar[:,0] = exponential_average_factor * Vrr * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,0]&#xA;                self.running_covar[:,1] = exponential_average_factor * Vii * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,1]&#xA;                self.running_covar[:,2] = exponential_average_factor * Vjj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,2]&#xA;                self.running_covar[:,3] = exponential_average_factor * Vkk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,3]&#xA;                &#xA;                self.running_covar[:,4] = exponential_average_factor * Vri * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,4]&#xA;                self.running_covar[:,5] = exponential_average_factor * Vrj * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,5]&#xA;                self.running_covar[:,6] = exponential_average_factor * Vrk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,6]&#xA;                self.running_covar[:,7] = exponential_average_factor * Vij * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,7]&#xA;                self.running_covar[:,8] = exponential_average_factor * Vik * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,8]&#xA;                self.running_covar[:,9] = exponential_average_factor * Vjk * n / (n - 1) + (1 - exponential_average_factor) * self.running_covar[:,9]&#xA;        else:&#xA;            mean = self.running_mean&#xA;            Vrr = self.running_covar[:,0]+self.eps&#xA;            Vii = self.running_covar[:,1]+self.eps&#xA;            Vjj = self.running_covar[:,2]+self.eps&#xA;            Vkk = self.running_covar[:,3]+self.eps&#xA;            &#xA;            Vri = self.running_covar[:,4]+self.eps&#xA;            Vrj = self.running_covar[:,5]+self.eps&#xA;            Vrk = self.running_covar[:,6]+self.eps&#xA;            Vij = self.running_covar[:,7]+self.eps&#xA;            Vik = self.running_covar[:,8]+self.eps&#xA;            Vjk = self.running_covar[:,9]+self.eps&#xA;           &#xA;            r = r-mean[None,:,0,None,None]&#xA;            i = i-mean[None,:,1,None,None]&#xA;            j = j-mean[None,:,2,None,None]&#xA;            k = k-mean[None,:,3,None,None]&#xA;            &#xA;        # standardized_output&#xA;        input = self._decomposition_v1(r,i,j,k, Vrr, Vri, Vrj, Vrk, Vii, Vij, Vik, Vjj, Vjk, Vkk)&#xA;        &#xA;        if self.affine:&#xA;            r,i,j,k = torch.chunk(input, 4, dim=1)&#xA;            &#xA;            cat_gamma_1 = torch.cat([self.weight[:,0], self.weight[:,1], self.weight[:,2], self.weight[:,3]])&#xA;            cat_gamma_2 = torch.cat([self.weight[:,1], self.weight[:,4], self.weight[:,5], self.weight[:,6]])&#xA;            cat_gamma_3 = torch.cat([self.weight[:,2], self.weight[:,5], self.weight[:,7], self.weight[:,8]])&#xA;            cat_gamma_4 = torch.cat([self.weight[:,3], self.weight[:,6], self.weight[:,8], self.weight[:,9]])&#xA;&#xA;&#xA;            input =  cat_gamma_1[None,:,None,None] * r.repeat(1,4,1,1) \&#xA;                    + cat_gamma_2[None,:,None,None] * i.repeat(1,4,1,1) \&#xA;                    + cat_gamma_3[None,:,None,None] * j.repeat(1,4,1,1) \&#xA;                    + cat_gamma_4[None,:,None,None] * k.repeat(1,4,1,1) \&#xA;                    + self.bias[None, :, None, None]&#xA;        return input&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I will explain the forward section. So the basic formula for batch normalization is &lt;code&gt;x* = (x - E[x]) / sqrt(var(x))&lt;/code&gt;, where &lt;code&gt;x*&lt;/code&gt; is the new value of a single component, &lt;code&gt;E[x]&lt;/code&gt; is its mean within a batch and &lt;code&gt;var(x)&lt;/code&gt; is its variance within a batch.&lt;/p&gt;&#xA;&lt;p&gt;However, as it is quaternion batch normalization,it has 4 parts &lt;code&gt;r&lt;/code&gt; which is the real part and &lt;code&gt;i, j, and k&lt;/code&gt;, are the imaginary part.&lt;/p&gt;&#xA;&lt;p&gt;The equation extends to &lt;code&gt;x* = W(x - E[x]) / (var(x))&lt;/code&gt;. &lt;code&gt;W&lt;/code&gt; is one of the matrices from the Cholesky decomposition of &lt;code&gt;V^-1&lt;/code&gt; where &lt;code&gt;V&lt;/code&gt; is the variance.In the code,&lt;code&gt;E[x]&lt;/code&gt; is computed using the &lt;code&gt;mean&lt;/code&gt; variable. V is computed in &lt;code&gt;Vxy&lt;/code&gt; and &lt;code&gt;V^-1&lt;/code&gt; i.e. &lt;code&gt;W&lt;/code&gt; is computed in the &lt;code&gt;_decomposition_v1&lt;/code&gt; function. This is applied to the input.&lt;/p&gt;&#xA;&lt;p&gt;Finally, that formula further extends to &lt;code&gt;x** = gamma * x* + beta&lt;/code&gt;, where &lt;code&gt;x**&lt;/code&gt; is the final normalized value. &lt;code&gt;gamma&lt;/code&gt; i.e. &lt;code&gt;cat_gamma_x&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; i.e. &lt;code&gt;self.beta&lt;/code&gt; are learned per layer.&lt;/p&gt;&#xA;&lt;p&gt;Note: The num_features need to be in multiples of 4.&lt;/p&gt;&#xA;&lt;p&gt;Thank you,&#xA;Shreyas&lt;/p&gt;&#xA;"" OwnerUserId=""108888"" LastEditorUserId=""-1"" LastEditDate=""2020-06-10T13:24:26.273"" LastActivityDate=""2020-02-20T03:27:54.440"" Title=""Batch normalization code optimization?"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;memory-optimization&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""2"" FavoriteCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""23848"" PostTypeId=""1"" CreationDate=""2017-10-16T17:23:21.237"" Score=""1"" ViewCount=""1140"" Body=""&lt;p&gt;I have not GPU support so it often happens that my model takes hours to train. Can I train my model in batches , for example if I want to have 100 epochs for my model,but due to power cut my training stops(at 50th epoch) but when I retrain my model I want to train it from where it was left (from 50th epoch).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be much appreciated if anyone can explain it by some example.&lt;/p&gt;&#xA;"" OwnerUserId=""37502"" LastEditorUserId=""14675"" LastEditDate=""2017-10-16T20:58:26.283"" LastActivityDate=""2019-04-07T17:23:22.627"" Title=""How to resume training of a model?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;neural-network&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""4"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""23848"" PostTypeId=""1"" CreationDate=""2017-10-16T17:23:21.237"" Score=""1"" ViewCount=""1140"" Body=""&lt;p&gt;I have not GPU support so it often happens that my model takes hours to train. Can I train my model in batches , for example if I want to have 100 epochs for my model,but due to power cut my training stops(at 50th epoch) but when I retrain my model I want to train it from where it was left (from 50th epoch).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be much appreciated if anyone can explain it by some example.&lt;/p&gt;&#xA;"" OwnerUserId=""37502"" LastEditorUserId=""14675"" LastEditDate=""2017-10-16T20:58:26.283"" LastActivityDate=""2019-04-07T17:23:22.627"" Title=""How to resume training of a model?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;neural-network&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""4"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""23848"" PostTypeId=""1"" CreationDate=""2017-10-16T17:23:21.237"" Score=""1"" ViewCount=""1140"" Body=""&lt;p&gt;I have not GPU support so it often happens that my model takes hours to train. Can I train my model in batches , for example if I want to have 100 epochs for my model,but due to power cut my training stops(at 50th epoch) but when I retrain my model I want to train it from where it was left (from 50th epoch).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be much appreciated if anyone can explain it by some example.&lt;/p&gt;&#xA;"" OwnerUserId=""37502"" LastEditorUserId=""14675"" LastEditDate=""2017-10-16T20:58:26.283"" LastActivityDate=""2019-04-07T17:23:22.627"" Title=""How to resume training of a model?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;neural-network&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""4"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""23848"" PostTypeId=""1"" CreationDate=""2017-10-16T17:23:21.237"" Score=""1"" ViewCount=""1140"" Body=""&lt;p&gt;I have not GPU support so it often happens that my model takes hours to train. Can I train my model in batches , for example if I want to have 100 epochs for my model,but due to power cut my training stops(at 50th epoch) but when I retrain my model I want to train it from where it was left (from 50th epoch).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be much appreciated if anyone can explain it by some example.&lt;/p&gt;&#xA;"" OwnerUserId=""37502"" LastEditorUserId=""14675"" LastEditDate=""2017-10-16T20:58:26.283"" LastActivityDate=""2019-04-07T17:23:22.627"" Title=""How to resume training of a model?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;neural-network&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""4"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""26622"" PostTypeId=""1"" AcceptedAnswerId=""26737"" CreationDate=""2018-01-14T19:15:45.697"" Score=""2"" ViewCount=""1055"" Body=""&lt;p&gt;I have collected the &lt;a href=&quot;https://github.com/tflearn/tflearn/blob/master/examples/images/dcgan.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;TFLearn DCGAN example&lt;/a&gt; code and put it into my local Jupyter environment. Furthermore, I have changed some comments and added &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; right before calling &lt;code&gt;gan.fit(...)&lt;/code&gt;, resulting in the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;&#xA;# In[1]:&#xA;&#xA;&#xA;get_ipython().magic('matplotlib inline')&#xA;from __future__ import division, print_function, absolute_import&#xA;&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;import tflearn&#xA;&#xA;&#xA;# In[2]:&#xA;&#xA;&#xA;# Data loading and preprocessing&#xA;import tflearn.datasets.mnist as mnist&#xA;X, Y, testX, testY = mnist.load_data()&#xA;X = np.reshape(X, newshape=[-1, 28, 28, 1])&#xA;&#xA;&#xA;# In[3]:&#xA;&#xA;&#xA;# Noise data input&#xA;z_dim = 200&#xA;total_samples = len(X)&#xA;&#xA;&#xA;# In[4]:&#xA;&#xA;&#xA;# Generator&#xA;def generator(x, reuse=False):&#xA;    with tf.variable_scope('Generator', reuse=reuse):&#xA;        x = tflearn.fully_connected(x, n_units=7 * 7 * 128)&#xA;        x = tflearn.batch_normalization(x)&#xA;        x = tf.nn.tanh(x)&#xA;        x = tf.reshape(x, shape=[-1, 7, 7, 128])&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 1, 5, activation='sigmoid')&#xA;        return x&#xA;&#xA;&#xA;# In[5]:&#xA;&#xA;&#xA;# Discriminator&#xA;def discriminator(x, reuse=False):&#xA;    with tf.variable_scope('Discriminator', reuse=reuse):&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 128, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.fully_connected(x, 1028, activation='tanh')&#xA;        x = tflearn.fully_connected(x, 2)&#xA;        x = tf.nn.softmax(x)&#xA;        return x&#xA;&#xA;&#xA;# In[6]:&#xA;&#xA;&#xA;# Input data&#xA;gen_input = tflearn.input_data(shape=[None, z_dim], name='input_gen_noise')&#xA;input_disc_noise = tflearn.input_data(shape=[None, z_dim], name='input_disc_noise')&#xA;input_disc_real = tflearn.input_data(shape=[None, 28, 28, 1], name='input_disc_real')&#xA;&#xA;&#xA;# In[7]:&#xA;&#xA;&#xA;# Build discriminator&#xA;disc_fake = discriminator(generator(input_disc_noise))&#xA;disc_real = discriminator(input_disc_real, reuse=True)&#xA;disc_net = tf.concat([disc_fake, disc_real], axis=0)&#xA;&#xA;&#xA;# In[8]:&#xA;&#xA;&#xA;# Build stacked Generator/Discriminator&#xA;gen_net = generator(gen_input, reuse=True)&#xA;stacked_gan_net = discriminator(gen_net, reuse=True)&#xA;&#xA;&#xA;# In[9]:&#xA;&#xA;&#xA;# Build training ops for Discriminator&#xA;# Each network optimization should only update its own variable, thus we need&#xA;# to retrieve each network variable (with get_layer_variables_by_name)&#xA;disc_vars = tflearn.get_layer_variables_by_name('Discriminator')&#xA;# We need 2 target placeholders, for both the real and fake image target&#xA;disc_target = tflearn.multi_target_data(['target_disc_fake', 'target_disc_real'],&#xA;                                        shape=[None, 2])&#xA;disc_model = tflearn.regression(disc_net, optimizer='adam',&#xA;                                placeholder=disc_target,&#xA;                                loss='categorical_crossentropy',&#xA;                                trainable_vars=disc_vars,&#xA;                                batch_size=64, name='target_disc',&#xA;                                op_name='DISC')&#xA;&#xA;&#xA;# In[10]:&#xA;&#xA;&#xA;# Build training ops for Generator&#xA;gen_vars = tflearn.get_layer_variables_by_name('Generator')&#xA;gan_model = tflearn.regression(stacked_gan_net, optimizer='adam',&#xA;                               loss='categorical_crossentropy',&#xA;                               trainable_vars=gen_vars,&#xA;                               batch_size=64, name='target_gen',&#xA;                               op_name='GEN')&#xA;&#xA;&#xA;# In[11]:&#xA;&#xA;&#xA;# Define GAN model, that outputs the generated images&#xA;gan = tflearn.DNN(gan_model, tensorboard_verbose=3)&#xA;&#xA;&#xA;# In[12]:&#xA;&#xA;&#xA;# Training&#xA;# Prepare input data to feed to the discriminator&#xA;disc_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the discriminator (0: fake image, 1: real image)&#xA;y_disc_fake = np.zeros(shape=[total_samples])&#xA;y_disc_real = np.ones(shape=[total_samples])&#xA;y_disc_fake = tflearn.data_utils.to_categorical(y_disc_fake, 2)&#xA;y_disc_real = tflearn.data_utils.to_categorical(y_disc_real, 2)&#xA;&#xA;&#xA;# In[13]:&#xA;&#xA;&#xA;# Prepare input data to feed to the stacked Generator/Discriminator&#xA;gen_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the Discriminator&#xA;# The Generator tries to fool the Discriminator, thus target is 1 (real images)&#xA;y_gen = np.ones(shape=[total_samples])&#xA;y_gen = tflearn.data_utils.to_categorical(y_gen, 2)&#xA;&#xA;&#xA;# In[14]:&#xA;&#xA;&#xA;# Start training, feed both noise and real images&#xA;with tf.device('/gpu:0'):&#xA;    gan.fit(X_inputs={'input_gen_noise': gen_noise,&#xA;                      'input_disc_noise': disc_noise,&#xA;                      'input_disc_real': X},&#xA;            Y_targets={'target_gen': y_gen,&#xA;                       'target_disc_fake': y_disc_fake,&#xA;                       'target_disc_real': y_disc_real},&#xA;            n_epoch=10)&#xA;&#xA;&#xA;# In[15]:&#xA;&#xA;&#xA;# Create another model from the Generator graph to generate some samples&#xA;# for testing (re-using the same session to re-use the weights learnt)&#xA;gen = tflearn.DNN(gen_net, session=gan.session)&#xA;&#xA;&#xA;# In[16]:&#xA;&#xA;&#xA;f, a = plt.subplots(4, 10, figsize=(10, 4))&#xA;for i in range(10):&#xA;    # Noise input&#xA;    z = np.random.uniform(-1., 1., size=[4, z_dim])&#xA;    g = np.array(gen.predict({'input_gen_noise': z}))&#xA;    for j in range(4):&#xA;        # Generate image from noise. Extend to 3 channels for matplot figure.&#xA;        img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),&#xA;                         newshape=(28, 28, 3))&#xA;        a[j][i].imshow(img)&#xA;&#xA;f.show()&#xA;plt.draw()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to run this code on my NVIDIA GPU. I already have CUDA and cuDNN installed on my machine. Upon examining Windows Task Manager during training, I see that my CPU is stressed and my GPU lies dorment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; alt=&quot;Windows Task Manager during training&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give advice on how to properly implement &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; as it's clear that the above code does not run on my NVIDIA GPU?&lt;/p&gt;&#xA;"" OwnerUserId=""44130"" LastActivityDate=""2018-01-17T09:42:46.727"" Title=""Why does TFLearn DCGAN not run on my GPU (on Windows)?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;gan&gt;&lt;tflearn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""26622"" PostTypeId=""1"" AcceptedAnswerId=""26737"" CreationDate=""2018-01-14T19:15:45.697"" Score=""2"" ViewCount=""1055"" Body=""&lt;p&gt;I have collected the &lt;a href=&quot;https://github.com/tflearn/tflearn/blob/master/examples/images/dcgan.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;TFLearn DCGAN example&lt;/a&gt; code and put it into my local Jupyter environment. Furthermore, I have changed some comments and added &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; right before calling &lt;code&gt;gan.fit(...)&lt;/code&gt;, resulting in the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;&#xA;# In[1]:&#xA;&#xA;&#xA;get_ipython().magic('matplotlib inline')&#xA;from __future__ import division, print_function, absolute_import&#xA;&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;import tflearn&#xA;&#xA;&#xA;# In[2]:&#xA;&#xA;&#xA;# Data loading and preprocessing&#xA;import tflearn.datasets.mnist as mnist&#xA;X, Y, testX, testY = mnist.load_data()&#xA;X = np.reshape(X, newshape=[-1, 28, 28, 1])&#xA;&#xA;&#xA;# In[3]:&#xA;&#xA;&#xA;# Noise data input&#xA;z_dim = 200&#xA;total_samples = len(X)&#xA;&#xA;&#xA;# In[4]:&#xA;&#xA;&#xA;# Generator&#xA;def generator(x, reuse=False):&#xA;    with tf.variable_scope('Generator', reuse=reuse):&#xA;        x = tflearn.fully_connected(x, n_units=7 * 7 * 128)&#xA;        x = tflearn.batch_normalization(x)&#xA;        x = tf.nn.tanh(x)&#xA;        x = tf.reshape(x, shape=[-1, 7, 7, 128])&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 1, 5, activation='sigmoid')&#xA;        return x&#xA;&#xA;&#xA;# In[5]:&#xA;&#xA;&#xA;# Discriminator&#xA;def discriminator(x, reuse=False):&#xA;    with tf.variable_scope('Discriminator', reuse=reuse):&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 128, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.fully_connected(x, 1028, activation='tanh')&#xA;        x = tflearn.fully_connected(x, 2)&#xA;        x = tf.nn.softmax(x)&#xA;        return x&#xA;&#xA;&#xA;# In[6]:&#xA;&#xA;&#xA;# Input data&#xA;gen_input = tflearn.input_data(shape=[None, z_dim], name='input_gen_noise')&#xA;input_disc_noise = tflearn.input_data(shape=[None, z_dim], name='input_disc_noise')&#xA;input_disc_real = tflearn.input_data(shape=[None, 28, 28, 1], name='input_disc_real')&#xA;&#xA;&#xA;# In[7]:&#xA;&#xA;&#xA;# Build discriminator&#xA;disc_fake = discriminator(generator(input_disc_noise))&#xA;disc_real = discriminator(input_disc_real, reuse=True)&#xA;disc_net = tf.concat([disc_fake, disc_real], axis=0)&#xA;&#xA;&#xA;# In[8]:&#xA;&#xA;&#xA;# Build stacked Generator/Discriminator&#xA;gen_net = generator(gen_input, reuse=True)&#xA;stacked_gan_net = discriminator(gen_net, reuse=True)&#xA;&#xA;&#xA;# In[9]:&#xA;&#xA;&#xA;# Build training ops for Discriminator&#xA;# Each network optimization should only update its own variable, thus we need&#xA;# to retrieve each network variable (with get_layer_variables_by_name)&#xA;disc_vars = tflearn.get_layer_variables_by_name('Discriminator')&#xA;# We need 2 target placeholders, for both the real and fake image target&#xA;disc_target = tflearn.multi_target_data(['target_disc_fake', 'target_disc_real'],&#xA;                                        shape=[None, 2])&#xA;disc_model = tflearn.regression(disc_net, optimizer='adam',&#xA;                                placeholder=disc_target,&#xA;                                loss='categorical_crossentropy',&#xA;                                trainable_vars=disc_vars,&#xA;                                batch_size=64, name='target_disc',&#xA;                                op_name='DISC')&#xA;&#xA;&#xA;# In[10]:&#xA;&#xA;&#xA;# Build training ops for Generator&#xA;gen_vars = tflearn.get_layer_variables_by_name('Generator')&#xA;gan_model = tflearn.regression(stacked_gan_net, optimizer='adam',&#xA;                               loss='categorical_crossentropy',&#xA;                               trainable_vars=gen_vars,&#xA;                               batch_size=64, name='target_gen',&#xA;                               op_name='GEN')&#xA;&#xA;&#xA;# In[11]:&#xA;&#xA;&#xA;# Define GAN model, that outputs the generated images&#xA;gan = tflearn.DNN(gan_model, tensorboard_verbose=3)&#xA;&#xA;&#xA;# In[12]:&#xA;&#xA;&#xA;# Training&#xA;# Prepare input data to feed to the discriminator&#xA;disc_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the discriminator (0: fake image, 1: real image)&#xA;y_disc_fake = np.zeros(shape=[total_samples])&#xA;y_disc_real = np.ones(shape=[total_samples])&#xA;y_disc_fake = tflearn.data_utils.to_categorical(y_disc_fake, 2)&#xA;y_disc_real = tflearn.data_utils.to_categorical(y_disc_real, 2)&#xA;&#xA;&#xA;# In[13]:&#xA;&#xA;&#xA;# Prepare input data to feed to the stacked Generator/Discriminator&#xA;gen_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the Discriminator&#xA;# The Generator tries to fool the Discriminator, thus target is 1 (real images)&#xA;y_gen = np.ones(shape=[total_samples])&#xA;y_gen = tflearn.data_utils.to_categorical(y_gen, 2)&#xA;&#xA;&#xA;# In[14]:&#xA;&#xA;&#xA;# Start training, feed both noise and real images&#xA;with tf.device('/gpu:0'):&#xA;    gan.fit(X_inputs={'input_gen_noise': gen_noise,&#xA;                      'input_disc_noise': disc_noise,&#xA;                      'input_disc_real': X},&#xA;            Y_targets={'target_gen': y_gen,&#xA;                       'target_disc_fake': y_disc_fake,&#xA;                       'target_disc_real': y_disc_real},&#xA;            n_epoch=10)&#xA;&#xA;&#xA;# In[15]:&#xA;&#xA;&#xA;# Create another model from the Generator graph to generate some samples&#xA;# for testing (re-using the same session to re-use the weights learnt)&#xA;gen = tflearn.DNN(gen_net, session=gan.session)&#xA;&#xA;&#xA;# In[16]:&#xA;&#xA;&#xA;f, a = plt.subplots(4, 10, figsize=(10, 4))&#xA;for i in range(10):&#xA;    # Noise input&#xA;    z = np.random.uniform(-1., 1., size=[4, z_dim])&#xA;    g = np.array(gen.predict({'input_gen_noise': z}))&#xA;    for j in range(4):&#xA;        # Generate image from noise. Extend to 3 channels for matplot figure.&#xA;        img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),&#xA;                         newshape=(28, 28, 3))&#xA;        a[j][i].imshow(img)&#xA;&#xA;f.show()&#xA;plt.draw()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to run this code on my NVIDIA GPU. I already have CUDA and cuDNN installed on my machine. Upon examining Windows Task Manager during training, I see that my CPU is stressed and my GPU lies dorment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; alt=&quot;Windows Task Manager during training&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give advice on how to properly implement &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; as it's clear that the above code does not run on my NVIDIA GPU?&lt;/p&gt;&#xA;"" OwnerUserId=""44130"" LastActivityDate=""2018-01-17T09:42:46.727"" Title=""Why does TFLearn DCGAN not run on my GPU (on Windows)?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;gan&gt;&lt;tflearn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""26622"" PostTypeId=""1"" AcceptedAnswerId=""26737"" CreationDate=""2018-01-14T19:15:45.697"" Score=""2"" ViewCount=""1055"" Body=""&lt;p&gt;I have collected the &lt;a href=&quot;https://github.com/tflearn/tflearn/blob/master/examples/images/dcgan.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;TFLearn DCGAN example&lt;/a&gt; code and put it into my local Jupyter environment. Furthermore, I have changed some comments and added &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; right before calling &lt;code&gt;gan.fit(...)&lt;/code&gt;, resulting in the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;&#xA;# In[1]:&#xA;&#xA;&#xA;get_ipython().magic('matplotlib inline')&#xA;from __future__ import division, print_function, absolute_import&#xA;&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;import tflearn&#xA;&#xA;&#xA;# In[2]:&#xA;&#xA;&#xA;# Data loading and preprocessing&#xA;import tflearn.datasets.mnist as mnist&#xA;X, Y, testX, testY = mnist.load_data()&#xA;X = np.reshape(X, newshape=[-1, 28, 28, 1])&#xA;&#xA;&#xA;# In[3]:&#xA;&#xA;&#xA;# Noise data input&#xA;z_dim = 200&#xA;total_samples = len(X)&#xA;&#xA;&#xA;# In[4]:&#xA;&#xA;&#xA;# Generator&#xA;def generator(x, reuse=False):&#xA;    with tf.variable_scope('Generator', reuse=reuse):&#xA;        x = tflearn.fully_connected(x, n_units=7 * 7 * 128)&#xA;        x = tflearn.batch_normalization(x)&#xA;        x = tf.nn.tanh(x)&#xA;        x = tf.reshape(x, shape=[-1, 7, 7, 128])&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 1, 5, activation='sigmoid')&#xA;        return x&#xA;&#xA;&#xA;# In[5]:&#xA;&#xA;&#xA;# Discriminator&#xA;def discriminator(x, reuse=False):&#xA;    with tf.variable_scope('Discriminator', reuse=reuse):&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 128, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.fully_connected(x, 1028, activation='tanh')&#xA;        x = tflearn.fully_connected(x, 2)&#xA;        x = tf.nn.softmax(x)&#xA;        return x&#xA;&#xA;&#xA;# In[6]:&#xA;&#xA;&#xA;# Input data&#xA;gen_input = tflearn.input_data(shape=[None, z_dim], name='input_gen_noise')&#xA;input_disc_noise = tflearn.input_data(shape=[None, z_dim], name='input_disc_noise')&#xA;input_disc_real = tflearn.input_data(shape=[None, 28, 28, 1], name='input_disc_real')&#xA;&#xA;&#xA;# In[7]:&#xA;&#xA;&#xA;# Build discriminator&#xA;disc_fake = discriminator(generator(input_disc_noise))&#xA;disc_real = discriminator(input_disc_real, reuse=True)&#xA;disc_net = tf.concat([disc_fake, disc_real], axis=0)&#xA;&#xA;&#xA;# In[8]:&#xA;&#xA;&#xA;# Build stacked Generator/Discriminator&#xA;gen_net = generator(gen_input, reuse=True)&#xA;stacked_gan_net = discriminator(gen_net, reuse=True)&#xA;&#xA;&#xA;# In[9]:&#xA;&#xA;&#xA;# Build training ops for Discriminator&#xA;# Each network optimization should only update its own variable, thus we need&#xA;# to retrieve each network variable (with get_layer_variables_by_name)&#xA;disc_vars = tflearn.get_layer_variables_by_name('Discriminator')&#xA;# We need 2 target placeholders, for both the real and fake image target&#xA;disc_target = tflearn.multi_target_data(['target_disc_fake', 'target_disc_real'],&#xA;                                        shape=[None, 2])&#xA;disc_model = tflearn.regression(disc_net, optimizer='adam',&#xA;                                placeholder=disc_target,&#xA;                                loss='categorical_crossentropy',&#xA;                                trainable_vars=disc_vars,&#xA;                                batch_size=64, name='target_disc',&#xA;                                op_name='DISC')&#xA;&#xA;&#xA;# In[10]:&#xA;&#xA;&#xA;# Build training ops for Generator&#xA;gen_vars = tflearn.get_layer_variables_by_name('Generator')&#xA;gan_model = tflearn.regression(stacked_gan_net, optimizer='adam',&#xA;                               loss='categorical_crossentropy',&#xA;                               trainable_vars=gen_vars,&#xA;                               batch_size=64, name='target_gen',&#xA;                               op_name='GEN')&#xA;&#xA;&#xA;# In[11]:&#xA;&#xA;&#xA;# Define GAN model, that outputs the generated images&#xA;gan = tflearn.DNN(gan_model, tensorboard_verbose=3)&#xA;&#xA;&#xA;# In[12]:&#xA;&#xA;&#xA;# Training&#xA;# Prepare input data to feed to the discriminator&#xA;disc_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the discriminator (0: fake image, 1: real image)&#xA;y_disc_fake = np.zeros(shape=[total_samples])&#xA;y_disc_real = np.ones(shape=[total_samples])&#xA;y_disc_fake = tflearn.data_utils.to_categorical(y_disc_fake, 2)&#xA;y_disc_real = tflearn.data_utils.to_categorical(y_disc_real, 2)&#xA;&#xA;&#xA;# In[13]:&#xA;&#xA;&#xA;# Prepare input data to feed to the stacked Generator/Discriminator&#xA;gen_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the Discriminator&#xA;# The Generator tries to fool the Discriminator, thus target is 1 (real images)&#xA;y_gen = np.ones(shape=[total_samples])&#xA;y_gen = tflearn.data_utils.to_categorical(y_gen, 2)&#xA;&#xA;&#xA;# In[14]:&#xA;&#xA;&#xA;# Start training, feed both noise and real images&#xA;with tf.device('/gpu:0'):&#xA;    gan.fit(X_inputs={'input_gen_noise': gen_noise,&#xA;                      'input_disc_noise': disc_noise,&#xA;                      'input_disc_real': X},&#xA;            Y_targets={'target_gen': y_gen,&#xA;                       'target_disc_fake': y_disc_fake,&#xA;                       'target_disc_real': y_disc_real},&#xA;            n_epoch=10)&#xA;&#xA;&#xA;# In[15]:&#xA;&#xA;&#xA;# Create another model from the Generator graph to generate some samples&#xA;# for testing (re-using the same session to re-use the weights learnt)&#xA;gen = tflearn.DNN(gen_net, session=gan.session)&#xA;&#xA;&#xA;# In[16]:&#xA;&#xA;&#xA;f, a = plt.subplots(4, 10, figsize=(10, 4))&#xA;for i in range(10):&#xA;    # Noise input&#xA;    z = np.random.uniform(-1., 1., size=[4, z_dim])&#xA;    g = np.array(gen.predict({'input_gen_noise': z}))&#xA;    for j in range(4):&#xA;        # Generate image from noise. Extend to 3 channels for matplot figure.&#xA;        img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),&#xA;                         newshape=(28, 28, 3))&#xA;        a[j][i].imshow(img)&#xA;&#xA;f.show()&#xA;plt.draw()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to run this code on my NVIDIA GPU. I already have CUDA and cuDNN installed on my machine. Upon examining Windows Task Manager during training, I see that my CPU is stressed and my GPU lies dorment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; alt=&quot;Windows Task Manager during training&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give advice on how to properly implement &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; as it's clear that the above code does not run on my NVIDIA GPU?&lt;/p&gt;&#xA;"" OwnerUserId=""44130"" LastActivityDate=""2018-01-17T09:42:46.727"" Title=""Why does TFLearn DCGAN not run on my GPU (on Windows)?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;gan&gt;&lt;tflearn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""26622"" PostTypeId=""1"" AcceptedAnswerId=""26737"" CreationDate=""2018-01-14T19:15:45.697"" Score=""2"" ViewCount=""1055"" Body=""&lt;p&gt;I have collected the &lt;a href=&quot;https://github.com/tflearn/tflearn/blob/master/examples/images/dcgan.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;TFLearn DCGAN example&lt;/a&gt; code and put it into my local Jupyter environment. Furthermore, I have changed some comments and added &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; right before calling &lt;code&gt;gan.fit(...)&lt;/code&gt;, resulting in the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;&#xA;# In[1]:&#xA;&#xA;&#xA;get_ipython().magic('matplotlib inline')&#xA;from __future__ import division, print_function, absolute_import&#xA;&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;import tflearn&#xA;&#xA;&#xA;# In[2]:&#xA;&#xA;&#xA;# Data loading and preprocessing&#xA;import tflearn.datasets.mnist as mnist&#xA;X, Y, testX, testY = mnist.load_data()&#xA;X = np.reshape(X, newshape=[-1, 28, 28, 1])&#xA;&#xA;&#xA;# In[3]:&#xA;&#xA;&#xA;# Noise data input&#xA;z_dim = 200&#xA;total_samples = len(X)&#xA;&#xA;&#xA;# In[4]:&#xA;&#xA;&#xA;# Generator&#xA;def generator(x, reuse=False):&#xA;    with tf.variable_scope('Generator', reuse=reuse):&#xA;        x = tflearn.fully_connected(x, n_units=7 * 7 * 128)&#xA;        x = tflearn.batch_normalization(x)&#xA;        x = tf.nn.tanh(x)&#xA;        x = tf.reshape(x, shape=[-1, 7, 7, 128])&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.upsample_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 1, 5, activation='sigmoid')&#xA;        return x&#xA;&#xA;&#xA;# In[5]:&#xA;&#xA;&#xA;# Discriminator&#xA;def discriminator(x, reuse=False):&#xA;    with tf.variable_scope('Discriminator', reuse=reuse):&#xA;        x = tflearn.conv_2d(x, 64, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.conv_2d(x, 128, 5, activation='tanh')&#xA;        x = tflearn.avg_pool_2d(x, 2)&#xA;        x = tflearn.fully_connected(x, 1028, activation='tanh')&#xA;        x = tflearn.fully_connected(x, 2)&#xA;        x = tf.nn.softmax(x)&#xA;        return x&#xA;&#xA;&#xA;# In[6]:&#xA;&#xA;&#xA;# Input data&#xA;gen_input = tflearn.input_data(shape=[None, z_dim], name='input_gen_noise')&#xA;input_disc_noise = tflearn.input_data(shape=[None, z_dim], name='input_disc_noise')&#xA;input_disc_real = tflearn.input_data(shape=[None, 28, 28, 1], name='input_disc_real')&#xA;&#xA;&#xA;# In[7]:&#xA;&#xA;&#xA;# Build discriminator&#xA;disc_fake = discriminator(generator(input_disc_noise))&#xA;disc_real = discriminator(input_disc_real, reuse=True)&#xA;disc_net = tf.concat([disc_fake, disc_real], axis=0)&#xA;&#xA;&#xA;# In[8]:&#xA;&#xA;&#xA;# Build stacked Generator/Discriminator&#xA;gen_net = generator(gen_input, reuse=True)&#xA;stacked_gan_net = discriminator(gen_net, reuse=True)&#xA;&#xA;&#xA;# In[9]:&#xA;&#xA;&#xA;# Build training ops for Discriminator&#xA;# Each network optimization should only update its own variable, thus we need&#xA;# to retrieve each network variable (with get_layer_variables_by_name)&#xA;disc_vars = tflearn.get_layer_variables_by_name('Discriminator')&#xA;# We need 2 target placeholders, for both the real and fake image target&#xA;disc_target = tflearn.multi_target_data(['target_disc_fake', 'target_disc_real'],&#xA;                                        shape=[None, 2])&#xA;disc_model = tflearn.regression(disc_net, optimizer='adam',&#xA;                                placeholder=disc_target,&#xA;                                loss='categorical_crossentropy',&#xA;                                trainable_vars=disc_vars,&#xA;                                batch_size=64, name='target_disc',&#xA;                                op_name='DISC')&#xA;&#xA;&#xA;# In[10]:&#xA;&#xA;&#xA;# Build training ops for Generator&#xA;gen_vars = tflearn.get_layer_variables_by_name('Generator')&#xA;gan_model = tflearn.regression(stacked_gan_net, optimizer='adam',&#xA;                               loss='categorical_crossentropy',&#xA;                               trainable_vars=gen_vars,&#xA;                               batch_size=64, name='target_gen',&#xA;                               op_name='GEN')&#xA;&#xA;&#xA;# In[11]:&#xA;&#xA;&#xA;# Define GAN model, that outputs the generated images&#xA;gan = tflearn.DNN(gan_model, tensorboard_verbose=3)&#xA;&#xA;&#xA;# In[12]:&#xA;&#xA;&#xA;# Training&#xA;# Prepare input data to feed to the discriminator&#xA;disc_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the discriminator (0: fake image, 1: real image)&#xA;y_disc_fake = np.zeros(shape=[total_samples])&#xA;y_disc_real = np.ones(shape=[total_samples])&#xA;y_disc_fake = tflearn.data_utils.to_categorical(y_disc_fake, 2)&#xA;y_disc_real = tflearn.data_utils.to_categorical(y_disc_real, 2)&#xA;&#xA;&#xA;# In[13]:&#xA;&#xA;&#xA;# Prepare input data to feed to the stacked Generator/Discriminator&#xA;gen_noise = np.random.uniform(-1., 1., size=[total_samples, z_dim])&#xA;# Prepare target data to feed to the Discriminator&#xA;# The Generator tries to fool the Discriminator, thus target is 1 (real images)&#xA;y_gen = np.ones(shape=[total_samples])&#xA;y_gen = tflearn.data_utils.to_categorical(y_gen, 2)&#xA;&#xA;&#xA;# In[14]:&#xA;&#xA;&#xA;# Start training, feed both noise and real images&#xA;with tf.device('/gpu:0'):&#xA;    gan.fit(X_inputs={'input_gen_noise': gen_noise,&#xA;                      'input_disc_noise': disc_noise,&#xA;                      'input_disc_real': X},&#xA;            Y_targets={'target_gen': y_gen,&#xA;                       'target_disc_fake': y_disc_fake,&#xA;                       'target_disc_real': y_disc_real},&#xA;            n_epoch=10)&#xA;&#xA;&#xA;# In[15]:&#xA;&#xA;&#xA;# Create another model from the Generator graph to generate some samples&#xA;# for testing (re-using the same session to re-use the weights learnt)&#xA;gen = tflearn.DNN(gen_net, session=gan.session)&#xA;&#xA;&#xA;# In[16]:&#xA;&#xA;&#xA;f, a = plt.subplots(4, 10, figsize=(10, 4))&#xA;for i in range(10):&#xA;    # Noise input&#xA;    z = np.random.uniform(-1., 1., size=[4, z_dim])&#xA;    g = np.array(gen.predict({'input_gen_noise': z}))&#xA;    for j in range(4):&#xA;        # Generate image from noise. Extend to 3 channels for matplot figure.&#xA;        img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),&#xA;                         newshape=(28, 28, 3))&#xA;        a[j][i].imshow(img)&#xA;&#xA;f.show()&#xA;plt.draw()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to run this code on my NVIDIA GPU. I already have CUDA and cuDNN installed on my machine. Upon examining Windows Task Manager during training, I see that my CPU is stressed and my GPU lies dorment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RWZ1k.png&quot; alt=&quot;Windows Task Manager during training&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give advice on how to properly implement &lt;code&gt;with tf.device('/gpu:0'):&lt;/code&gt; as it's clear that the above code does not run on my NVIDIA GPU?&lt;/p&gt;&#xA;"" OwnerUserId=""44130"" LastActivityDate=""2018-01-17T09:42:46.727"" Title=""Why does TFLearn DCGAN not run on my GPU (on Windows)?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;gan&gt;&lt;tflearn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""33962"" PostTypeId=""1"" CreationDate=""2018-07-04T01:23:28.373"" Score=""1"" ViewCount=""3693"" Body=""&lt;p&gt;I am currently training a few custom models that require about 12Gb GPU memory at the most. My setup has about 96Gb of GPU memory and python/Jupyter still manages to hog up all the gpu memory to the point that I get the Resource exhausted error thrown at me. I am stuck at this peculiar issue for a while and hence any help will be appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when loading a vgg based model similar to this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.models import Model&#xA;&#xA;import keras&#xA;&#xA;from keras.models import Model, Sequential&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = VGG16(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I just run a jupyter cell with this code and monitor the GPU usage using nvidia-smi, it is 0% .&#xA;However, I replace the code in the above Jupyter cell with the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.inception_v3 import InceptionV3&#xA;from keras.models import Model&#xA;import keras&#xA;from keras.models import Model&#xA;from keras.models import Sequential&#xA;&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The GPU usage goes crazy and suddenly almost all the memory is over in all the GPUs even before I do model.compile() or model.fit() in Keras!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried both allow_growth and per_process_gpu_memory_fraction in Tensorflow as well. I still get the resource exhausted error the moment I run model.fit when using the Inception based model.&#xA;Please note that I do not think this is a GPU memory error as I have about 96GB of GPU memory using an instance with 8 Tesla K80s .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also kindly note that my batch size is 2.&lt;/p&gt;&#xA;"" OwnerUserId=""49441"" LastActivityDate=""2023-10-12T03:03:10.353"" Title=""Training Inception V3 based model using Keras with Tensorflow Backend"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;computer-vision&gt;&lt;inception&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""33962"" PostTypeId=""1"" CreationDate=""2018-07-04T01:23:28.373"" Score=""1"" ViewCount=""3693"" Body=""&lt;p&gt;I am currently training a few custom models that require about 12Gb GPU memory at the most. My setup has about 96Gb of GPU memory and python/Jupyter still manages to hog up all the gpu memory to the point that I get the Resource exhausted error thrown at me. I am stuck at this peculiar issue for a while and hence any help will be appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when loading a vgg based model similar to this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.models import Model&#xA;&#xA;import keras&#xA;&#xA;from keras.models import Model, Sequential&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = VGG16(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I just run a jupyter cell with this code and monitor the GPU usage using nvidia-smi, it is 0% .&#xA;However, I replace the code in the above Jupyter cell with the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.inception_v3 import InceptionV3&#xA;from keras.models import Model&#xA;import keras&#xA;from keras.models import Model&#xA;from keras.models import Sequential&#xA;&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The GPU usage goes crazy and suddenly almost all the memory is over in all the GPUs even before I do model.compile() or model.fit() in Keras!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried both allow_growth and per_process_gpu_memory_fraction in Tensorflow as well. I still get the resource exhausted error the moment I run model.fit when using the Inception based model.&#xA;Please note that I do not think this is a GPU memory error as I have about 96GB of GPU memory using an instance with 8 Tesla K80s .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also kindly note that my batch size is 2.&lt;/p&gt;&#xA;"" OwnerUserId=""49441"" LastActivityDate=""2023-10-12T03:03:10.353"" Title=""Training Inception V3 based model using Keras with Tensorflow Backend"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;computer-vision&gt;&lt;inception&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""33962"" PostTypeId=""1"" CreationDate=""2018-07-04T01:23:28.373"" Score=""1"" ViewCount=""3693"" Body=""&lt;p&gt;I am currently training a few custom models that require about 12Gb GPU memory at the most. My setup has about 96Gb of GPU memory and python/Jupyter still manages to hog up all the gpu memory to the point that I get the Resource exhausted error thrown at me. I am stuck at this peculiar issue for a while and hence any help will be appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when loading a vgg based model similar to this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.models import Model&#xA;&#xA;import keras&#xA;&#xA;from keras.models import Model, Sequential&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = VGG16(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I just run a jupyter cell with this code and monitor the GPU usage using nvidia-smi, it is 0% .&#xA;However, I replace the code in the above Jupyter cell with the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.inception_v3 import InceptionV3&#xA;from keras.models import Model&#xA;import keras&#xA;from keras.models import Model&#xA;from keras.models import Sequential&#xA;&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The GPU usage goes crazy and suddenly almost all the memory is over in all the GPUs even before I do model.compile() or model.fit() in Keras!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried both allow_growth and per_process_gpu_memory_fraction in Tensorflow as well. I still get the resource exhausted error the moment I run model.fit when using the Inception based model.&#xA;Please note that I do not think this is a GPU memory error as I have about 96GB of GPU memory using an instance with 8 Tesla K80s .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also kindly note that my batch size is 2.&lt;/p&gt;&#xA;"" OwnerUserId=""49441"" LastActivityDate=""2023-10-12T03:03:10.353"" Title=""Training Inception V3 based model using Keras with Tensorflow Backend"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;computer-vision&gt;&lt;inception&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""33962"" PostTypeId=""1"" CreationDate=""2018-07-04T01:23:28.373"" Score=""1"" ViewCount=""3693"" Body=""&lt;p&gt;I am currently training a few custom models that require about 12Gb GPU memory at the most. My setup has about 96Gb of GPU memory and python/Jupyter still manages to hog up all the gpu memory to the point that I get the Resource exhausted error thrown at me. I am stuck at this peculiar issue for a while and hence any help will be appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, when loading a vgg based model similar to this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.vgg16 import VGG16&#xA;from keras.models import Model&#xA;&#xA;import keras&#xA;&#xA;from keras.models import Model, Sequential&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = VGG16(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I just run a jupyter cell with this code and monitor the GPU usage using nvidia-smi, it is 0% .&#xA;However, I replace the code in the above Jupyter cell with the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from keras.applications.inception_v3 import InceptionV3&#xA;from keras.models import Model&#xA;import keras&#xA;from keras.models import Model&#xA;from keras.models import Sequential&#xA;&#xA;from keras.models import Input&#xA;input_shape = (512, 512, 3)&#xA;base_model = InceptionV3(input_shape=input_shape, weights=None, include_top=False)&#xA;&#xA;pixel_branch = base_model.output&#xA;pixel_branch = Flatten()(pixel_branch)&#xA;&#xA;new_model = Model(inputs=base_model.input, outputs=pixel_branch)&#xA;&#xA;text_branch = Sequential()&#xA;text_branch.add(Dense(32, input_shape=(1,), activation='relu'))&#xA;&#xA;# merged = Merge([new_model, text_branch], mode='concat')&#xA;merged = keras.layers.concatenate([new_model.output, text_branch.output])&#xA;&#xA;age = Dense(1000, activation='relu')(merged)&#xA;age = Dense(1000, activation='relu')(age)&#xA;age = Dense(1)(age)&#xA;&#xA;# show model&#xA;# model.summary()&#xA;model = Model(inputs=[base_model.input, text_branch.input], outputs=age)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The GPU usage goes crazy and suddenly almost all the memory is over in all the GPUs even before I do model.compile() or model.fit() in Keras!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried both allow_growth and per_process_gpu_memory_fraction in Tensorflow as well. I still get the resource exhausted error the moment I run model.fit when using the Inception based model.&#xA;Please note that I do not think this is a GPU memory error as I have about 96GB of GPU memory using an instance with 8 Tesla K80s .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also kindly note that my batch size is 2.&lt;/p&gt;&#xA;"" OwnerUserId=""49441"" LastActivityDate=""2023-10-12T03:03:10.353"" Title=""Training Inception V3 based model using Keras with Tensorflow Backend"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;computer-vision&gt;&lt;inception&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""36781"" PostTypeId=""1"" CreationDate=""2018-08-11T11:51:28.997"" Score=""1"" ViewCount=""619"" Body=""&lt;p&gt;I am using Keras with a Tensorflow backend to train an Image Classification model on a GPU. I have read somewhere that training uses roughly twice (both forward and back props) the GPU memory of validating, so therefore the training batch size should be the half of the validation batch size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, on many blogs and tutorials, I see that people use the same batch size for training and validating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it true that training uses twice the GPU memory, because of the forward and backward pass, or is this false?&lt;/p&gt;&#xA;"" OwnerUserId=""57524"" LastActivityDate=""2020-02-15T08:01:40.130"" Title=""Setting batch size: training requires twice as much memory as validating"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;mini-batch-gradient-descent&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""36781"" PostTypeId=""1"" CreationDate=""2018-08-11T11:51:28.997"" Score=""1"" ViewCount=""619"" Body=""&lt;p&gt;I am using Keras with a Tensorflow backend to train an Image Classification model on a GPU. I have read somewhere that training uses roughly twice (both forward and back props) the GPU memory of validating, so therefore the training batch size should be the half of the validation batch size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, on many blogs and tutorials, I see that people use the same batch size for training and validating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it true that training uses twice the GPU memory, because of the forward and backward pass, or is this false?&lt;/p&gt;&#xA;"" OwnerUserId=""57524"" LastActivityDate=""2020-02-15T08:01:40.130"" Title=""Setting batch size: training requires twice as much memory as validating"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;mini-batch-gradient-descent&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""36781"" PostTypeId=""1"" CreationDate=""2018-08-11T11:51:28.997"" Score=""1"" ViewCount=""619"" Body=""&lt;p&gt;I am using Keras with a Tensorflow backend to train an Image Classification model on a GPU. I have read somewhere that training uses roughly twice (both forward and back props) the GPU memory of validating, so therefore the training batch size should be the half of the validation batch size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, on many blogs and tutorials, I see that people use the same batch size for training and validating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it true that training uses twice the GPU memory, because of the forward and backward pass, or is this false?&lt;/p&gt;&#xA;"" OwnerUserId=""57524"" LastActivityDate=""2020-02-15T08:01:40.130"" Title=""Setting batch size: training requires twice as much memory as validating"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;mini-batch-gradient-descent&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""84614"" PostTypeId=""1"" CreationDate=""2020-10-28T14:21:09.723"" Score=""1"" ViewCount=""160"" Body=""&lt;p&gt;I am using a pre-trained Model. Firstly, I trained the Model in python which came with 94% accuracy. Now, I am trying to reproduce the same results with same configuration of the model in R, but it is giving terrible results and not crossing the accuracy of 83%.&lt;/p&gt;&#xA;&lt;p&gt;There is a difference in the data type of both languages with which I train the model: R uses data type &lt;strong&gt;Double&lt;/strong&gt; while Python uses data type &lt;strong&gt;float32&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The second key difference is, I was running Python Notebook on GPU and TPU but the Notebook in R running on CPU because of Kaggle GPU memory issue.&lt;/p&gt;&#xA;&lt;p&gt;Do the above-mentioned differences cause performance gaps? If not, then what could be the problem?&lt;/p&gt;&#xA;"" OwnerUserId=""65908"" LastEditorUserId=""33551"" LastEditDate=""2020-10-28T14:27:56.547"" LastActivityDate=""2020-10-28T14:27:56.547"" Title=""Different Results from Pre-Trained Model Between Python vs R"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;r&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""84614"" PostTypeId=""1"" CreationDate=""2020-10-28T14:21:09.723"" Score=""1"" ViewCount=""160"" Body=""&lt;p&gt;I am using a pre-trained Model. Firstly, I trained the Model in python which came with 94% accuracy. Now, I am trying to reproduce the same results with same configuration of the model in R, but it is giving terrible results and not crossing the accuracy of 83%.&lt;/p&gt;&#xA;&lt;p&gt;There is a difference in the data type of both languages with which I train the model: R uses data type &lt;strong&gt;Double&lt;/strong&gt; while Python uses data type &lt;strong&gt;float32&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The second key difference is, I was running Python Notebook on GPU and TPU but the Notebook in R running on CPU because of Kaggle GPU memory issue.&lt;/p&gt;&#xA;&lt;p&gt;Do the above-mentioned differences cause performance gaps? If not, then what could be the problem?&lt;/p&gt;&#xA;"" OwnerUserId=""65908"" LastEditorUserId=""33551"" LastEditDate=""2020-10-28T14:27:56.547"" LastActivityDate=""2020-10-28T14:27:56.547"" Title=""Different Results from Pre-Trained Model Between Python vs R"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;r&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""84614"" PostTypeId=""1"" CreationDate=""2020-10-28T14:21:09.723"" Score=""1"" ViewCount=""160"" Body=""&lt;p&gt;I am using a pre-trained Model. Firstly, I trained the Model in python which came with 94% accuracy. Now, I am trying to reproduce the same results with same configuration of the model in R, but it is giving terrible results and not crossing the accuracy of 83%.&lt;/p&gt;&#xA;&lt;p&gt;There is a difference in the data type of both languages with which I train the model: R uses data type &lt;strong&gt;Double&lt;/strong&gt; while Python uses data type &lt;strong&gt;float32&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The second key difference is, I was running Python Notebook on GPU and TPU but the Notebook in R running on CPU because of Kaggle GPU memory issue.&lt;/p&gt;&#xA;&lt;p&gt;Do the above-mentioned differences cause performance gaps? If not, then what could be the problem?&lt;/p&gt;&#xA;"" OwnerUserId=""65908"" LastEditorUserId=""33551"" LastEditDate=""2020-10-28T14:27:56.547"" LastActivityDate=""2020-10-28T14:27:56.547"" Title=""Different Results from Pre-Trained Model Between Python vs R"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;r&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""84614"" PostTypeId=""1"" CreationDate=""2020-10-28T14:21:09.723"" Score=""1"" ViewCount=""160"" Body=""&lt;p&gt;I am using a pre-trained Model. Firstly, I trained the Model in python which came with 94% accuracy. Now, I am trying to reproduce the same results with same configuration of the model in R, but it is giving terrible results and not crossing the accuracy of 83%.&lt;/p&gt;&#xA;&lt;p&gt;There is a difference in the data type of both languages with which I train the model: R uses data type &lt;strong&gt;Double&lt;/strong&gt; while Python uses data type &lt;strong&gt;float32&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The second key difference is, I was running Python Notebook on GPU and TPU but the Notebook in R running on CPU because of Kaggle GPU memory issue.&lt;/p&gt;&#xA;&lt;p&gt;Do the above-mentioned differences cause performance gaps? If not, then what could be the problem?&lt;/p&gt;&#xA;"" OwnerUserId=""65908"" LastEditorUserId=""33551"" LastEditDate=""2020-10-28T14:27:56.547"" LastActivityDate=""2020-10-28T14:27:56.547"" Title=""Different Results from Pre-Trained Model Between Python vs R"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;keras&gt;&lt;tensorflow&gt;&lt;r&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""89426"" PostTypeId=""1"" CreationDate=""2021-02-16T07:45:53.637"" Score=""0"" ViewCount=""105"" Body=""&lt;p&gt;I am performing an NLP task using &lt;code&gt;Elmo&lt;/code&gt; model. Whenever I load the Elmo model, it occupies the 15 GB of my GPU memory. How can I reduce it ?&lt;/p&gt;&#xA;&lt;p&gt;Below is my code&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow.compat.v1 as tf&#xA;import tensorflow_hub as hub&#xA;tf.disable_eager_execution()&#xA;from tensorflow.compat.v1.keras import backend as K&#xA;sess = tf.Session()&#xA;K.set_session(sess)&#xA;&#xA;elmo_model = hub.Module(&amp;quot;https://tfhub.dev/google/elmo/2&amp;quot;, trainable=True)&#xA;sess.run(tf.global_variables_initializer())&#xA;sess.run(tf.tables_initializer())&#xA;&#xA;&#xA;def ElmoEmbedding(x):&#xA;    return elmo_model(inputs={&#xA;                    &amp;quot;tokens&amp;quot;: tf.squeeze(tf.cast(x, tf.string)),&#xA;                    &amp;quot;sequence_len&amp;quot;: tf.constant(batch_size*[maxlen])&#xA;              },&#xA;              signature=&amp;quot;tokens&amp;quot;,&#xA;              as_dict=True)[&amp;quot;elmo&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And then I am passing the ElmoEmbedding in the Lambda layer as below&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;input_text = Input(shape=(maxlen,), dtype=tf.string)&#xA;&#xA;embedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)&#xA;&#xA;x = Bidirectional(LSTM(units=512, return_sequences=True,&#xA;               recurrent_dropout=0.2, dropout=0.2))(embedding)&#xA;&#xA;.....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;What do I need to change in the above code ?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""91801"" LastActivityDate=""2021-02-16T07:55:44.873"" Title=""How to reduce the GPU consumption size while using Elmo Model?"" Tags=""&lt;deep-learning&gt;&lt;tensorflow&gt;&lt;nlp&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""89426"" PostTypeId=""1"" CreationDate=""2021-02-16T07:45:53.637"" Score=""0"" ViewCount=""105"" Body=""&lt;p&gt;I am performing an NLP task using &lt;code&gt;Elmo&lt;/code&gt; model. Whenever I load the Elmo model, it occupies the 15 GB of my GPU memory. How can I reduce it ?&lt;/p&gt;&#xA;&lt;p&gt;Below is my code&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow.compat.v1 as tf&#xA;import tensorflow_hub as hub&#xA;tf.disable_eager_execution()&#xA;from tensorflow.compat.v1.keras import backend as K&#xA;sess = tf.Session()&#xA;K.set_session(sess)&#xA;&#xA;elmo_model = hub.Module(&amp;quot;https://tfhub.dev/google/elmo/2&amp;quot;, trainable=True)&#xA;sess.run(tf.global_variables_initializer())&#xA;sess.run(tf.tables_initializer())&#xA;&#xA;&#xA;def ElmoEmbedding(x):&#xA;    return elmo_model(inputs={&#xA;                    &amp;quot;tokens&amp;quot;: tf.squeeze(tf.cast(x, tf.string)),&#xA;                    &amp;quot;sequence_len&amp;quot;: tf.constant(batch_size*[maxlen])&#xA;              },&#xA;              signature=&amp;quot;tokens&amp;quot;,&#xA;              as_dict=True)[&amp;quot;elmo&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And then I am passing the ElmoEmbedding in the Lambda layer as below&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;input_text = Input(shape=(maxlen,), dtype=tf.string)&#xA;&#xA;embedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)&#xA;&#xA;x = Bidirectional(LSTM(units=512, return_sequences=True,&#xA;               recurrent_dropout=0.2, dropout=0.2))(embedding)&#xA;&#xA;.....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;What do I need to change in the above code ?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""91801"" LastActivityDate=""2021-02-16T07:55:44.873"" Title=""How to reduce the GPU consumption size while using Elmo Model?"" Tags=""&lt;deep-learning&gt;&lt;tensorflow&gt;&lt;nlp&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""89426"" PostTypeId=""1"" CreationDate=""2021-02-16T07:45:53.637"" Score=""0"" ViewCount=""105"" Body=""&lt;p&gt;I am performing an NLP task using &lt;code&gt;Elmo&lt;/code&gt; model. Whenever I load the Elmo model, it occupies the 15 GB of my GPU memory. How can I reduce it ?&lt;/p&gt;&#xA;&lt;p&gt;Below is my code&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow.compat.v1 as tf&#xA;import tensorflow_hub as hub&#xA;tf.disable_eager_execution()&#xA;from tensorflow.compat.v1.keras import backend as K&#xA;sess = tf.Session()&#xA;K.set_session(sess)&#xA;&#xA;elmo_model = hub.Module(&amp;quot;https://tfhub.dev/google/elmo/2&amp;quot;, trainable=True)&#xA;sess.run(tf.global_variables_initializer())&#xA;sess.run(tf.tables_initializer())&#xA;&#xA;&#xA;def ElmoEmbedding(x):&#xA;    return elmo_model(inputs={&#xA;                    &amp;quot;tokens&amp;quot;: tf.squeeze(tf.cast(x, tf.string)),&#xA;                    &amp;quot;sequence_len&amp;quot;: tf.constant(batch_size*[maxlen])&#xA;              },&#xA;              signature=&amp;quot;tokens&amp;quot;,&#xA;              as_dict=True)[&amp;quot;elmo&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And then I am passing the ElmoEmbedding in the Lambda layer as below&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;input_text = Input(shape=(maxlen,), dtype=tf.string)&#xA;&#xA;embedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)&#xA;&#xA;x = Bidirectional(LSTM(units=512, return_sequences=True,&#xA;               recurrent_dropout=0.2, dropout=0.2))(embedding)&#xA;&#xA;.....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;What do I need to change in the above code ?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""91801"" LastActivityDate=""2021-02-16T07:55:44.873"" Title=""How to reduce the GPU consumption size while using Elmo Model?"" Tags=""&lt;deep-learning&gt;&lt;tensorflow&gt;&lt;nlp&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""89426"" PostTypeId=""1"" CreationDate=""2021-02-16T07:45:53.637"" Score=""0"" ViewCount=""105"" Body=""&lt;p&gt;I am performing an NLP task using &lt;code&gt;Elmo&lt;/code&gt; model. Whenever I load the Elmo model, it occupies the 15 GB of my GPU memory. How can I reduce it ?&lt;/p&gt;&#xA;&lt;p&gt;Below is my code&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow.compat.v1 as tf&#xA;import tensorflow_hub as hub&#xA;tf.disable_eager_execution()&#xA;from tensorflow.compat.v1.keras import backend as K&#xA;sess = tf.Session()&#xA;K.set_session(sess)&#xA;&#xA;elmo_model = hub.Module(&amp;quot;https://tfhub.dev/google/elmo/2&amp;quot;, trainable=True)&#xA;sess.run(tf.global_variables_initializer())&#xA;sess.run(tf.tables_initializer())&#xA;&#xA;&#xA;def ElmoEmbedding(x):&#xA;    return elmo_model(inputs={&#xA;                    &amp;quot;tokens&amp;quot;: tf.squeeze(tf.cast(x, tf.string)),&#xA;                    &amp;quot;sequence_len&amp;quot;: tf.constant(batch_size*[maxlen])&#xA;              },&#xA;              signature=&amp;quot;tokens&amp;quot;,&#xA;              as_dict=True)[&amp;quot;elmo&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And then I am passing the ElmoEmbedding in the Lambda layer as below&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;input_text = Input(shape=(maxlen,), dtype=tf.string)&#xA;&#xA;embedding = Lambda(ElmoEmbedding, output_shape=(maxlen, 1024))(input_text)&#xA;&#xA;x = Bidirectional(LSTM(units=512, return_sequences=True,&#xA;               recurrent_dropout=0.2, dropout=0.2))(embedding)&#xA;&#xA;.....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;What do I need to change in the above code ?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""91801"" LastActivityDate=""2021-02-16T07:55:44.873"" Title=""How to reduce the GPU consumption size while using Elmo Model?"" Tags=""&lt;deep-learning&gt;&lt;tensorflow&gt;&lt;nlp&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""93613"" PostTypeId=""1"" AcceptedAnswerId=""93614"" CreationDate=""2021-04-27T10:07:25.273"" Score=""0"" ViewCount=""780"" Body=""&lt;p&gt;I am facing a memory issue while trying to initialize a torch.zeros -&#xA;&lt;code&gt;torch.zeros((2000,2000,3200), device=device)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Getting the following error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA out of memory. Tried to allocate 47.69 GiB (GPU 0; 8.00 GiB total capacity; 1.50 KiB already allocated; 6.16 GiB free; 2.00 MiB reserved in total by PyTorch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;My question is: why does a zero tensor need that big memory? Or am I doing any mistake?&lt;/p&gt;&#xA;&lt;p&gt;P.S. I was checking with &lt;code&gt;getsizeof&lt;/code&gt; in another system - the size of this tensor is showing as 72bytes only.&lt;/p&gt;&#xA;"" OwnerUserId=""108477"" LastActivityDate=""2021-04-27T10:38:45.567"" Title=""Memory issue when trying to initiate zero tensor with pytorch"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;pytorch&gt;&lt;memory&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""93613"" PostTypeId=""1"" AcceptedAnswerId=""93614"" CreationDate=""2021-04-27T10:07:25.273"" Score=""0"" ViewCount=""780"" Body=""&lt;p&gt;I am facing a memory issue while trying to initialize a torch.zeros -&#xA;&lt;code&gt;torch.zeros((2000,2000,3200), device=device)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Getting the following error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA out of memory. Tried to allocate 47.69 GiB (GPU 0; 8.00 GiB total capacity; 1.50 KiB already allocated; 6.16 GiB free; 2.00 MiB reserved in total by PyTorch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;My question is: why does a zero tensor need that big memory? Or am I doing any mistake?&lt;/p&gt;&#xA;&lt;p&gt;P.S. I was checking with &lt;code&gt;getsizeof&lt;/code&gt; in another system - the size of this tensor is showing as 72bytes only.&lt;/p&gt;&#xA;"" OwnerUserId=""108477"" LastActivityDate=""2021-04-27T10:38:45.567"" Title=""Memory issue when trying to initiate zero tensor with pytorch"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;pytorch&gt;&lt;memory&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""93613"" PostTypeId=""1"" AcceptedAnswerId=""93614"" CreationDate=""2021-04-27T10:07:25.273"" Score=""0"" ViewCount=""780"" Body=""&lt;p&gt;I am facing a memory issue while trying to initialize a torch.zeros -&#xA;&lt;code&gt;torch.zeros((2000,2000,3200), device=device)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Getting the following error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA out of memory. Tried to allocate 47.69 GiB (GPU 0; 8.00 GiB total capacity; 1.50 KiB already allocated; 6.16 GiB free; 2.00 MiB reserved in total by PyTorch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;My question is: why does a zero tensor need that big memory? Or am I doing any mistake?&lt;/p&gt;&#xA;&lt;p&gt;P.S. I was checking with &lt;code&gt;getsizeof&lt;/code&gt; in another system - the size of this tensor is showing as 72bytes only.&lt;/p&gt;&#xA;"" OwnerUserId=""108477"" LastActivityDate=""2021-04-27T10:38:45.567"" Title=""Memory issue when trying to initiate zero tensor with pytorch"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;pytorch&gt;&lt;memory&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""93613"" PostTypeId=""1"" AcceptedAnswerId=""93614"" CreationDate=""2021-04-27T10:07:25.273"" Score=""0"" ViewCount=""780"" Body=""&lt;p&gt;I am facing a memory issue while trying to initialize a torch.zeros -&#xA;&lt;code&gt;torch.zeros((2000,2000,3200), device=device)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;Getting the following error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA out of memory. Tried to allocate 47.69 GiB (GPU 0; 8.00 GiB total capacity; 1.50 KiB already allocated; 6.16 GiB free; 2.00 MiB reserved in total by PyTorch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;My question is: why does a zero tensor need that big memory? Or am I doing any mistake?&lt;/p&gt;&#xA;&lt;p&gt;P.S. I was checking with &lt;code&gt;getsizeof&lt;/code&gt; in another system - the size of this tensor is showing as 72bytes only.&lt;/p&gt;&#xA;"" OwnerUserId=""108477"" LastActivityDate=""2021-04-27T10:38:45.567"" Title=""Memory issue when trying to initiate zero tensor with pytorch"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;tensorflow&gt;&lt;pytorch&gt;&lt;memory&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""111212"" PostTypeId=""1"" CreationDate=""2022-05-22T19:21:49.303"" Score=""0"" ViewCount=""971"" Body=""&lt;p&gt;I am trying to use Keras to do some image analysis using the Xception model. When I run model.fit, GPU memory usage increases rapidly to 16 GB, despite my training data only being (7546, 299, 299, 3) in size, so about 1.9 GB.&lt;/p&gt;&#xA;&lt;p&gt;I have tried using K.clear_session() between epochs, which helps a bit, but GPU memory usage still quickly reaches the maximum of 16 GB and stays there.&lt;/p&gt;&#xA;&lt;p&gt;I am running this on a Jupyter notebook on Google Colab Pro+ using TensorFlow and Keras 2.8.0. Here is the code I use to create and run the network:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class EvalCallback(Callback):&#xA;    def on_epoch_end(self, epoch, logs=None):&#xA;        gc.collect()&#xA;        K.clear_session()&#xA;&#xA;Xception_initial=Xception(include_top=False,&#xA;                 weights='imagenet',&#xA;                 input_shape=(299,299,3),pooling ='avg',&#xA;                 )&#xA;&#xA;for layer in Xception_initial.layers:&#xA;    layer.trainable = True&#xA;&#xA;x = Xception_initial.output&#xA;eval_callback = EvalCallback()&#xA;predicted = Dense(2,activation ='softmax')(x)&#xA;model_pretrain = Model(inputs = Xception_initial.input, outputs = predicted)&#xA;model_pretrain.compile(loss=keras.losses.categorical_crossentropy,&#xA;          optimizer=tf.keras.optimizers.Adam(lr = 0.0002),&#xA;          metrics=['accuracy'])&#xA;pretraining_Xception =model_pretrain.fit(x_train, Y_train,&#xA;                                              verbose=1,&#xA;                                              batch_size=16,  &#xA;                                              epochs=3,&#xA;                                              callbacks=[eval_callback])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Running this code does complete, but GPU memory stays maxed out, and I am unable to do any additional work using Keras, getting a &amp;quot;dst tensor not initialized error&amp;quot; which I believe is caused by the GPU being out of memory.&lt;/p&gt;&#xA;&lt;p&gt;What can I do to fix this memory problem?&lt;/p&gt;&#xA;"" OwnerUserId=""136058"" LastActivityDate=""2022-05-22T19:21:49.303"" Title=""Excessive GPU memory usage for Keras model.fit()"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""111212"" PostTypeId=""1"" CreationDate=""2022-05-22T19:21:49.303"" Score=""0"" ViewCount=""971"" Body=""&lt;p&gt;I am trying to use Keras to do some image analysis using the Xception model. When I run model.fit, GPU memory usage increases rapidly to 16 GB, despite my training data only being (7546, 299, 299, 3) in size, so about 1.9 GB.&lt;/p&gt;&#xA;&lt;p&gt;I have tried using K.clear_session() between epochs, which helps a bit, but GPU memory usage still quickly reaches the maximum of 16 GB and stays there.&lt;/p&gt;&#xA;&lt;p&gt;I am running this on a Jupyter notebook on Google Colab Pro+ using TensorFlow and Keras 2.8.0. Here is the code I use to create and run the network:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class EvalCallback(Callback):&#xA;    def on_epoch_end(self, epoch, logs=None):&#xA;        gc.collect()&#xA;        K.clear_session()&#xA;&#xA;Xception_initial=Xception(include_top=False,&#xA;                 weights='imagenet',&#xA;                 input_shape=(299,299,3),pooling ='avg',&#xA;                 )&#xA;&#xA;for layer in Xception_initial.layers:&#xA;    layer.trainable = True&#xA;&#xA;x = Xception_initial.output&#xA;eval_callback = EvalCallback()&#xA;predicted = Dense(2,activation ='softmax')(x)&#xA;model_pretrain = Model(inputs = Xception_initial.input, outputs = predicted)&#xA;model_pretrain.compile(loss=keras.losses.categorical_crossentropy,&#xA;          optimizer=tf.keras.optimizers.Adam(lr = 0.0002),&#xA;          metrics=['accuracy'])&#xA;pretraining_Xception =model_pretrain.fit(x_train, Y_train,&#xA;                                              verbose=1,&#xA;                                              batch_size=16,  &#xA;                                              epochs=3,&#xA;                                              callbacks=[eval_callback])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Running this code does complete, but GPU memory stays maxed out, and I am unable to do any additional work using Keras, getting a &amp;quot;dst tensor not initialized error&amp;quot; which I believe is caused by the GPU being out of memory.&lt;/p&gt;&#xA;&lt;p&gt;What can I do to fix this memory problem?&lt;/p&gt;&#xA;"" OwnerUserId=""136058"" LastActivityDate=""2022-05-22T19:21:49.303"" Title=""Excessive GPU memory usage for Keras model.fit()"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""111212"" PostTypeId=""1"" CreationDate=""2022-05-22T19:21:49.303"" Score=""0"" ViewCount=""971"" Body=""&lt;p&gt;I am trying to use Keras to do some image analysis using the Xception model. When I run model.fit, GPU memory usage increases rapidly to 16 GB, despite my training data only being (7546, 299, 299, 3) in size, so about 1.9 GB.&lt;/p&gt;&#xA;&lt;p&gt;I have tried using K.clear_session() between epochs, which helps a bit, but GPU memory usage still quickly reaches the maximum of 16 GB and stays there.&lt;/p&gt;&#xA;&lt;p&gt;I am running this on a Jupyter notebook on Google Colab Pro+ using TensorFlow and Keras 2.8.0. Here is the code I use to create and run the network:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class EvalCallback(Callback):&#xA;    def on_epoch_end(self, epoch, logs=None):&#xA;        gc.collect()&#xA;        K.clear_session()&#xA;&#xA;Xception_initial=Xception(include_top=False,&#xA;                 weights='imagenet',&#xA;                 input_shape=(299,299,3),pooling ='avg',&#xA;                 )&#xA;&#xA;for layer in Xception_initial.layers:&#xA;    layer.trainable = True&#xA;&#xA;x = Xception_initial.output&#xA;eval_callback = EvalCallback()&#xA;predicted = Dense(2,activation ='softmax')(x)&#xA;model_pretrain = Model(inputs = Xception_initial.input, outputs = predicted)&#xA;model_pretrain.compile(loss=keras.losses.categorical_crossentropy,&#xA;          optimizer=tf.keras.optimizers.Adam(lr = 0.0002),&#xA;          metrics=['accuracy'])&#xA;pretraining_Xception =model_pretrain.fit(x_train, Y_train,&#xA;                                              verbose=1,&#xA;                                              batch_size=16,  &#xA;                                              epochs=3,&#xA;                                              callbacks=[eval_callback])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Running this code does complete, but GPU memory stays maxed out, and I am unable to do any additional work using Keras, getting a &amp;quot;dst tensor not initialized error&amp;quot; which I believe is caused by the GPU being out of memory.&lt;/p&gt;&#xA;&lt;p&gt;What can I do to fix this memory problem?&lt;/p&gt;&#xA;"" OwnerUserId=""136058"" LastActivityDate=""2022-05-22T19:21:49.303"" Title=""Excessive GPU memory usage for Keras model.fit()"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""111212"" PostTypeId=""1"" CreationDate=""2022-05-22T19:21:49.303"" Score=""0"" ViewCount=""971"" Body=""&lt;p&gt;I am trying to use Keras to do some image analysis using the Xception model. When I run model.fit, GPU memory usage increases rapidly to 16 GB, despite my training data only being (7546, 299, 299, 3) in size, so about 1.9 GB.&lt;/p&gt;&#xA;&lt;p&gt;I have tried using K.clear_session() between epochs, which helps a bit, but GPU memory usage still quickly reaches the maximum of 16 GB and stays there.&lt;/p&gt;&#xA;&lt;p&gt;I am running this on a Jupyter notebook on Google Colab Pro+ using TensorFlow and Keras 2.8.0. Here is the code I use to create and run the network:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class EvalCallback(Callback):&#xA;    def on_epoch_end(self, epoch, logs=None):&#xA;        gc.collect()&#xA;        K.clear_session()&#xA;&#xA;Xception_initial=Xception(include_top=False,&#xA;                 weights='imagenet',&#xA;                 input_shape=(299,299,3),pooling ='avg',&#xA;                 )&#xA;&#xA;for layer in Xception_initial.layers:&#xA;    layer.trainable = True&#xA;&#xA;x = Xception_initial.output&#xA;eval_callback = EvalCallback()&#xA;predicted = Dense(2,activation ='softmax')(x)&#xA;model_pretrain = Model(inputs = Xception_initial.input, outputs = predicted)&#xA;model_pretrain.compile(loss=keras.losses.categorical_crossentropy,&#xA;          optimizer=tf.keras.optimizers.Adam(lr = 0.0002),&#xA;          metrics=['accuracy'])&#xA;pretraining_Xception =model_pretrain.fit(x_train, Y_train,&#xA;                                              verbose=1,&#xA;                                              batch_size=16,  &#xA;                                              epochs=3,&#xA;                                              callbacks=[eval_callback])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Running this code does complete, but GPU memory stays maxed out, and I am unable to do any additional work using Keras, getting a &amp;quot;dst tensor not initialized error&amp;quot; which I believe is caused by the GPU being out of memory.&lt;/p&gt;&#xA;&lt;p&gt;What can I do to fix this memory problem?&lt;/p&gt;&#xA;"" OwnerUserId=""136058"" LastActivityDate=""2022-05-22T19:21:49.303"" Title=""Excessive GPU memory usage for Keras model.fit()"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""122158"" PostTypeId=""1"" CreationDate=""2023-06-14T13:27:21.763"" Score=""0"" ViewCount=""396"" Body=""&lt;p&gt;I'm running some ML python code on Amazon's EC2 - the machine has a GPU: NVIDIA A10G. The ML code is written using PyTorch.&lt;/p&gt;&#xA;&lt;p&gt;When I run the code I get error:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;CUDA error: no kernel image is available for execution on the device&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I suppose this is about PyTorch version? Used version is &lt;code&gt;torch==1.10.2&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;How do I know which version of PyTorch the code MUST use? What's the error about?&lt;/p&gt;&#xA;&lt;p&gt;Are there certain requirement for CUDA by this GPU (A10G)?&lt;/p&gt;&#xA;"" OwnerUserId=""69843"" LastActivityDate=""2023-06-14T13:27:21.763"" Title=""CUDA error: &quot;no kernel image is available for execution on the device&quot; - which PyTorch version to use?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;pytorch&gt;&lt;cuda&gt;&lt;amazon-ml&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""122158"" PostTypeId=""1"" CreationDate=""2023-06-14T13:27:21.763"" Score=""0"" ViewCount=""396"" Body=""&lt;p&gt;I'm running some ML python code on Amazon's EC2 - the machine has a GPU: NVIDIA A10G. The ML code is written using PyTorch.&lt;/p&gt;&#xA;&lt;p&gt;When I run the code I get error:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;CUDA error: no kernel image is available for execution on the device&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I suppose this is about PyTorch version? Used version is &lt;code&gt;torch==1.10.2&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;How do I know which version of PyTorch the code MUST use? What's the error about?&lt;/p&gt;&#xA;&lt;p&gt;Are there certain requirement for CUDA by this GPU (A10G)?&lt;/p&gt;&#xA;"" OwnerUserId=""69843"" LastActivityDate=""2023-06-14T13:27:21.763"" Title=""CUDA error: &quot;no kernel image is available for execution on the device&quot; - which PyTorch version to use?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;pytorch&gt;&lt;cuda&gt;&lt;amazon-ml&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""122158"" PostTypeId=""1"" CreationDate=""2023-06-14T13:27:21.763"" Score=""0"" ViewCount=""396"" Body=""&lt;p&gt;I'm running some ML python code on Amazon's EC2 - the machine has a GPU: NVIDIA A10G. The ML code is written using PyTorch.&lt;/p&gt;&#xA;&lt;p&gt;When I run the code I get error:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;CUDA error: no kernel image is available for execution on the device&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I suppose this is about PyTorch version? Used version is &lt;code&gt;torch==1.10.2&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;How do I know which version of PyTorch the code MUST use? What's the error about?&lt;/p&gt;&#xA;&lt;p&gt;Are there certain requirement for CUDA by this GPU (A10G)?&lt;/p&gt;&#xA;"" OwnerUserId=""69843"" LastActivityDate=""2023-06-14T13:27:21.763"" Title=""CUDA error: &quot;no kernel image is available for execution on the device&quot; - which PyTorch version to use?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;pytorch&gt;&lt;cuda&gt;&lt;amazon-ml&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\datascience.stackexchange.com,"  <row Id=""122158"" PostTypeId=""1"" CreationDate=""2023-06-14T13:27:21.763"" Score=""0"" ViewCount=""396"" Body=""&lt;p&gt;I'm running some ML python code on Amazon's EC2 - the machine has a GPU: NVIDIA A10G. The ML code is written using PyTorch.&lt;/p&gt;&#xA;&lt;p&gt;When I run the code I get error:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;CUDA error: no kernel image is available for execution on the device&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;I suppose this is about PyTorch version? Used version is &lt;code&gt;torch==1.10.2&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;How do I know which version of PyTorch the code MUST use? What's the error about?&lt;/p&gt;&#xA;&lt;p&gt;Are there certain requirement for CUDA by this GPU (A10G)?&lt;/p&gt;&#xA;"" OwnerUserId=""69843"" LastActivityDate=""2023-06-14T13:27:21.763"" Title=""CUDA error: &quot;no kernel image is available for execution on the device&quot; - which PyTorch version to use?"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;pytorch&gt;&lt;cuda&gt;&lt;amazon-ml&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ja.stackoverflow.com,"  <row Id=""62172"" PostTypeId=""1"" CreationDate=""2020-01-11T16:11:34.957"" Score=""0"" ViewCount=""2940"" Body=""&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;br&gt;&#xA;OS:windows10&lt;br&gt;&#xA;python:3.7.4&lt;br&gt;&#xA;numpy:1.16.5&lt;br&gt;&#xA;pytorch:1.3.1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;gpu:GeForce GTX 1060&lt;br&gt;&#xA;Nvidia driver:441.87&lt;br&gt;&#xA;Cuda:10.1&lt;br&gt;&#xA;Cudnn:7.6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bidirectional LSTM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;D:\new\mcep_generator\no_adversarial_module\train_sample_BLSTM.py&quot;, line 65, in forward&#xA;out, hidden = self.l1(input)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;return self.forward_tensor(input, hx)&#xA; File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 526, in forward_impl&#xA;self.dropout, self.training, self.bidirectional, self.batch_first)&#xA;RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cuda(pytorchcudnn)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;torch.backends.cudnn.enabled = False &lt;/p&gt;&#xA;"" OwnerUserId=""37372"" LastActivityDate=""2022-04-02T16:04:55.497"" Title=""PyTorchBidirectional LSTMcudnnRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ja.stackoverflow.com,"  <row Id=""62172"" PostTypeId=""1"" CreationDate=""2020-01-11T16:11:34.957"" Score=""0"" ViewCount=""2940"" Body=""&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;br&gt;&#xA;OS:windows10&lt;br&gt;&#xA;python:3.7.4&lt;br&gt;&#xA;numpy:1.16.5&lt;br&gt;&#xA;pytorch:1.3.1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;gpu:GeForce GTX 1060&lt;br&gt;&#xA;Nvidia driver:441.87&lt;br&gt;&#xA;Cuda:10.1&lt;br&gt;&#xA;Cudnn:7.6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bidirectional LSTM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;D:\new\mcep_generator\no_adversarial_module\train_sample_BLSTM.py&quot;, line 65, in forward&#xA;out, hidden = self.l1(input)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;return self.forward_tensor(input, hx)&#xA; File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 526, in forward_impl&#xA;self.dropout, self.training, self.bidirectional, self.batch_first)&#xA;RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cuda(pytorchcudnn)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;torch.backends.cudnn.enabled = False &lt;/p&gt;&#xA;"" OwnerUserId=""37372"" LastActivityDate=""2022-04-02T16:04:55.497"" Title=""PyTorchBidirectional LSTMcudnnRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ja.stackoverflow.com,"  <row Id=""62172"" PostTypeId=""1"" CreationDate=""2020-01-11T16:11:34.957"" Score=""0"" ViewCount=""2940"" Body=""&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;br&gt;&#xA;OS:windows10&lt;br&gt;&#xA;python:3.7.4&lt;br&gt;&#xA;numpy:1.16.5&lt;br&gt;&#xA;pytorch:1.3.1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;gpu:GeForce GTX 1060&lt;br&gt;&#xA;Nvidia driver:441.87&lt;br&gt;&#xA;Cuda:10.1&lt;br&gt;&#xA;Cudnn:7.6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bidirectional LSTM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;D:\new\mcep_generator\no_adversarial_module\train_sample_BLSTM.py&quot;, line 65, in forward&#xA;out, hidden = self.l1(input)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;return self.forward_tensor(input, hx)&#xA; File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 526, in forward_impl&#xA;self.dropout, self.training, self.bidirectional, self.batch_first)&#xA;RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cuda(pytorchcudnn)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;torch.backends.cudnn.enabled = False &lt;/p&gt;&#xA;"" OwnerUserId=""37372"" LastActivityDate=""2022-04-02T16:04:55.497"" Title=""PyTorchBidirectional LSTMcudnnRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ja.stackoverflow.com,"  <row Id=""62172"" PostTypeId=""1"" CreationDate=""2020-01-11T16:11:34.957"" Score=""0"" ViewCount=""2940"" Body=""&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;br&gt;&#xA;OS:windows10&lt;br&gt;&#xA;python:3.7.4&lt;br&gt;&#xA;numpy:1.16.5&lt;br&gt;&#xA;pytorch:1.3.1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;gpu:GeForce GTX 1060&lt;br&gt;&#xA;Nvidia driver:441.87&lt;br&gt;&#xA;Cuda:10.1&lt;br&gt;&#xA;Cudnn:7.6.5&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bidirectional LSTM&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;D:\new\mcep_generator\no_adversarial_module\train_sample_BLSTM.py&quot;, line 65, in forward&#xA;out, hidden = self.l1(input)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 541, in __call__&#xA;result = self.forward(*input, **kwargs)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;return self.forward_tensor(input, hx)&#xA; File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 543, in forward_tensor&#xA;output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)&#xA;File &quot;C:\Users\\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\rnn.py&quot;, line 526, in forward_impl&#xA;self.dropout, self.training, self.bidirectional, self.batch_first)&#xA;RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cuda(pytorchcudnn)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;torch.backends.cudnn.enabled = False &lt;/p&gt;&#xA;"" OwnerUserId=""37372"" LastActivityDate=""2022-04-02T16:04:55.497"" Title=""PyTorchBidirectional LSTMcudnnRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""955721"" PostTypeId=""1"" CreationDate=""2019-03-12T21:50:49.737"" Score=""1"" ViewCount=""2364"" Body=""&lt;p&gt;  16.04.  pytorch  'conda'.&#xA;  CUDA 9.0  : &lt;a href=&quot;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&lt;/a&gt;.&#xA; &lt;code&gt;import torch; torch.cuda.is_available()&lt;/code&gt;  &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt; : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;torch.cuda.current_device()&#xA;AssertionError: &#xA;Found no NVIDIA driver on your system. Please check that you&#xA;have an NVIDIA GPU and installed a driver from ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;nvcc --version&#xA;  nvcc: NVIDIA (R) Cuda compiler driver&#xA;  Copyright (c) 2005-2017 NVIDIA Corporation&#xA;  Built on Fri_Sep__1_21:08:03_CDT_2017&#xA;  Cuda compilation tools, release 9.0, V9.0.176&#xA;  nvidia-detector &#xA;  none&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;      cuda   nvidia,      .&lt;/p&gt;&#xA;"" OwnerUserId=""330114"" LastEditorUserId=""260769"" LastEditDate=""2023-04-25T04:44:47.973"" LastActivityDate=""2023-09-28T06:00:50.110"" Title=""torch.cuda.is_available() == False"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;&lt;nvidia&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""955721"" PostTypeId=""1"" CreationDate=""2019-03-12T21:50:49.737"" Score=""1"" ViewCount=""2364"" Body=""&lt;p&gt;  16.04.  pytorch  'conda'.&#xA;  CUDA 9.0  : &lt;a href=&quot;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&lt;/a&gt;.&#xA; &lt;code&gt;import torch; torch.cuda.is_available()&lt;/code&gt;  &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt; : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;torch.cuda.current_device()&#xA;AssertionError: &#xA;Found no NVIDIA driver on your system. Please check that you&#xA;have an NVIDIA GPU and installed a driver from ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;nvcc --version&#xA;  nvcc: NVIDIA (R) Cuda compiler driver&#xA;  Copyright (c) 2005-2017 NVIDIA Corporation&#xA;  Built on Fri_Sep__1_21:08:03_CDT_2017&#xA;  Cuda compilation tools, release 9.0, V9.0.176&#xA;  nvidia-detector &#xA;  none&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;      cuda   nvidia,      .&lt;/p&gt;&#xA;"" OwnerUserId=""330114"" LastEditorUserId=""260769"" LastEditDate=""2023-04-25T04:44:47.973"" LastActivityDate=""2023-09-28T06:00:50.110"" Title=""torch.cuda.is_available() == False"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;&lt;nvidia&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""955721"" PostTypeId=""1"" CreationDate=""2019-03-12T21:50:49.737"" Score=""1"" ViewCount=""2364"" Body=""&lt;p&gt;  16.04.  pytorch  'conda'.&#xA;  CUDA 9.0  : &lt;a href=&quot;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://gist.github.com/zhanwenchen/e520767a409325d9961072f666815bb8&lt;/a&gt;.&#xA; &lt;code&gt;import torch; torch.cuda.is_available()&lt;/code&gt;  &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt; : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;torch.cuda.current_device()&#xA;AssertionError: &#xA;Found no NVIDIA driver on your system. Please check that you&#xA;have an NVIDIA GPU and installed a driver from ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;nvcc --version&#xA;  nvcc: NVIDIA (R) Cuda compiler driver&#xA;  Copyright (c) 2005-2017 NVIDIA Corporation&#xA;  Built on Fri_Sep__1_21:08:03_CDT_2017&#xA;  Cuda compilation tools, release 9.0, V9.0.176&#xA;  nvidia-detector &#xA;  none&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;      cuda   nvidia,      .&lt;/p&gt;&#xA;"" OwnerUserId=""330114"" LastEditorUserId=""260769"" LastEditDate=""2023-04-25T04:44:47.973"" LastActivityDate=""2023-09-28T06:00:50.110"" Title=""torch.cuda.is_available() == False"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;&lt;nvidia&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1168634"" PostTypeId=""1"" AcceptedAnswerId=""1168657"" CreationDate=""2020-08-21T08:45:19.413"" Score=""1"" ViewCount=""73"" Body=""&lt;p&gt;   GAN-  pytorch.      &lt;code&gt;CUDA error: an illegal memory access was encountered&lt;/code&gt;.  ,   ,         ,   google collab.  ,   -   .&#xA;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import os&#xA;import torch&#xA;import torchvision&#xA;from torch import nn&#xA;from tqdm import tqdm&#xA;import torch.nn.functional as f&#xA;import matplotlib.pyplot as plt&#xA;import torchvision.transforms as transforms&#xA;&#xA;torch.manual_seed(42)&#xA;&#xA;&#xA;class Discriminator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Discriminator, self).__init__()&#xA;&#xA;        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6)&#xA;&#xA;        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)&#xA;&#xA;        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5)&#xA;        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)&#xA;&#xA;        self.fc1 = nn.Linear(5*5*32, 256)&#xA;        self.fc2 = nn.Linear(256, 128)&#xA;        self.fc3 = nn.Linear(128, 1)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.conv1(x))&#xA;        x = self.max_pool(x)&#xA;        x = f.relu(self.conv2(x))&#xA;        x = f.relu(self.conv3(x))&#xA;&#xA;        x = x.view(-1, 5*5*32)&#xA;&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = self.fc3(x)&#xA;&#xA;        x = f.softmax(x, dim=-1)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class Generator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Generator, self).__init__()&#xA;&#xA;        self.fc1 = nn.Linear(100, 256)&#xA;        self.fc2 = nn.Linear(256, 512)&#xA;        self.fc3 = nn.Linear(512, 1024)&#xA;        self.fc4 = nn.Linear(1024, 784)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = f.relu(self.fc3(x))&#xA;        x = f.tanh(self.fc4(x))&#xA;&#xA;        x = x.view(x.size(0), 1, 28, 28)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class NetworkStuff:&#xA;    def __init__(self):&#xA;&#xA;        self.batch_size = 16&#xA;        self.lr = 0.0001&#xA;        self.num_epoch = 50&#xA;&#xA;        self.discriminator = Discriminator()&#xA;        self.generator = Generator()&#xA;&#xA;        self.criterion = nn.BCELoss()&#xA;&#xA;        self.optimizer_discriminator = torch.optim.Adam(&#xA;            self.discriminator.parameters(),&#xA;            self.lr&#xA;        )&#xA;        self.optimizer_generator = torch.optim.Adam(&#xA;            self.generator.parameters(),&#xA;            self.lr&#xA;        )&#xA;&#xA;        if torch.cuda.is_available():&#xA;            self.device = torch.device(&amp;quot;cuda&amp;quot;)&#xA;        else:&#xA;            self.device = torch.device(&amp;quot;cpu&amp;quot;)&#xA;&#xA;        self.transform = transforms.Compose([&#xA;            transforms.ToTensor(),&#xA;            transforms.Normalize((0.5,), (0.5,))&#xA;        ])&#xA;&#xA;        self.train_set = torchvision.datasets.MNIST(&#xA;            root=os.path.abspath(&amp;quot;data&amp;quot;), train=True, download=True,&#xA;            transform=self.transform&#xA;        )&#xA;&#xA;        self.train_loader = torch.utils.data.DataLoader(&#xA;            self.train_set, batch_size=self.batch_size,&#xA;            shuffle=True&#xA;        )&#xA;&#xA;    def show_samples(self, type_='generated'):&#xA;&#xA;        if type_ == &amp;quot;dataset&amp;quot;:&#xA;&#xA;            real_samples, mnist_labels = next(iter(self.train_loader))&#xA;            plt.suptitle('dataset')&#xA;&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(real_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        elif type_ == &amp;quot;generated&amp;quot;:&#xA;&#xA;            latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;            generated_samples = self.generator(latent_samples).detach()&#xA;&#xA;            plt.suptitle('generated')&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(generated_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        plt.show()&#xA;&#xA;    def train(self):&#xA;&#xA;        for epoch in tqdm(range(self.num_epoch)):&#xA;&#xA;            for n, (real_samples, mnist_labels) in tqdm(enumerate(self.train_loader)):&#xA;&#xA;                real_samples = real_samples.to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;&#xA;                generated_samples = self.generator(latent_samples)&#xA;                generated_samples_labels = torch.zeros((self.batch_size, 1)).to(self.device)&#xA;&#xA;                all_samples = torch.cat((real_samples, generated_samples))&#xA;                all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))&#xA;&#xA;                #  &#xA;                self.discriminator.zero_grad()&#xA;                output_discriminator = self.discriminator(all_samples)&#xA;                loss_discriminator = self.criterion(output_discriminator, all_samples_labels)&#xA;                loss_discriminator.backward()&#xA;                self.optimizer_discriminator.step()&#xA;&#xA;                #  &#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                self.generator.zero_grad()&#xA;                generated_samples = self.generator(latent_samples)&#xA;&#xA;                output_discriminator_generated = self.discriminator(generated_samples)&#xA;&#xA;                generator_loss = self.criterion(output_discriminator_generated, real_samples_labels)&#xA;                generator_loss.backward()&#xA;                self.optimizer_generator.step()&#xA;&#xA;                if n == self.batch_size - 1:&#xA;                    tqdm.write(f&amp;quot;Epoch: {epoch} Loss D.: {loss_discriminator}\n&amp;quot; +&#xA;                               f&amp;quot;Epoch: {epoch} Loss G.: {generator_loss}&amp;quot;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""382496"" LastEditorUserId=""211923"" LastEditDate=""2020-08-21T09:50:27.383"" LastActivityDate=""2020-08-21T09:50:27.383"" Title="" , "" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1168634"" PostTypeId=""1"" AcceptedAnswerId=""1168657"" CreationDate=""2020-08-21T08:45:19.413"" Score=""1"" ViewCount=""73"" Body=""&lt;p&gt;   GAN-  pytorch.      &lt;code&gt;CUDA error: an illegal memory access was encountered&lt;/code&gt;.  ,   ,         ,   google collab.  ,   -   .&#xA;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import os&#xA;import torch&#xA;import torchvision&#xA;from torch import nn&#xA;from tqdm import tqdm&#xA;import torch.nn.functional as f&#xA;import matplotlib.pyplot as plt&#xA;import torchvision.transforms as transforms&#xA;&#xA;torch.manual_seed(42)&#xA;&#xA;&#xA;class Discriminator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Discriminator, self).__init__()&#xA;&#xA;        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6)&#xA;&#xA;        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)&#xA;&#xA;        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5)&#xA;        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)&#xA;&#xA;        self.fc1 = nn.Linear(5*5*32, 256)&#xA;        self.fc2 = nn.Linear(256, 128)&#xA;        self.fc3 = nn.Linear(128, 1)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.conv1(x))&#xA;        x = self.max_pool(x)&#xA;        x = f.relu(self.conv2(x))&#xA;        x = f.relu(self.conv3(x))&#xA;&#xA;        x = x.view(-1, 5*5*32)&#xA;&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = self.fc3(x)&#xA;&#xA;        x = f.softmax(x, dim=-1)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class Generator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Generator, self).__init__()&#xA;&#xA;        self.fc1 = nn.Linear(100, 256)&#xA;        self.fc2 = nn.Linear(256, 512)&#xA;        self.fc3 = nn.Linear(512, 1024)&#xA;        self.fc4 = nn.Linear(1024, 784)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = f.relu(self.fc3(x))&#xA;        x = f.tanh(self.fc4(x))&#xA;&#xA;        x = x.view(x.size(0), 1, 28, 28)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class NetworkStuff:&#xA;    def __init__(self):&#xA;&#xA;        self.batch_size = 16&#xA;        self.lr = 0.0001&#xA;        self.num_epoch = 50&#xA;&#xA;        self.discriminator = Discriminator()&#xA;        self.generator = Generator()&#xA;&#xA;        self.criterion = nn.BCELoss()&#xA;&#xA;        self.optimizer_discriminator = torch.optim.Adam(&#xA;            self.discriminator.parameters(),&#xA;            self.lr&#xA;        )&#xA;        self.optimizer_generator = torch.optim.Adam(&#xA;            self.generator.parameters(),&#xA;            self.lr&#xA;        )&#xA;&#xA;        if torch.cuda.is_available():&#xA;            self.device = torch.device(&amp;quot;cuda&amp;quot;)&#xA;        else:&#xA;            self.device = torch.device(&amp;quot;cpu&amp;quot;)&#xA;&#xA;        self.transform = transforms.Compose([&#xA;            transforms.ToTensor(),&#xA;            transforms.Normalize((0.5,), (0.5,))&#xA;        ])&#xA;&#xA;        self.train_set = torchvision.datasets.MNIST(&#xA;            root=os.path.abspath(&amp;quot;data&amp;quot;), train=True, download=True,&#xA;            transform=self.transform&#xA;        )&#xA;&#xA;        self.train_loader = torch.utils.data.DataLoader(&#xA;            self.train_set, batch_size=self.batch_size,&#xA;            shuffle=True&#xA;        )&#xA;&#xA;    def show_samples(self, type_='generated'):&#xA;&#xA;        if type_ == &amp;quot;dataset&amp;quot;:&#xA;&#xA;            real_samples, mnist_labels = next(iter(self.train_loader))&#xA;            plt.suptitle('dataset')&#xA;&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(real_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        elif type_ == &amp;quot;generated&amp;quot;:&#xA;&#xA;            latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;            generated_samples = self.generator(latent_samples).detach()&#xA;&#xA;            plt.suptitle('generated')&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(generated_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        plt.show()&#xA;&#xA;    def train(self):&#xA;&#xA;        for epoch in tqdm(range(self.num_epoch)):&#xA;&#xA;            for n, (real_samples, mnist_labels) in tqdm(enumerate(self.train_loader)):&#xA;&#xA;                real_samples = real_samples.to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;&#xA;                generated_samples = self.generator(latent_samples)&#xA;                generated_samples_labels = torch.zeros((self.batch_size, 1)).to(self.device)&#xA;&#xA;                all_samples = torch.cat((real_samples, generated_samples))&#xA;                all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))&#xA;&#xA;                #  &#xA;                self.discriminator.zero_grad()&#xA;                output_discriminator = self.discriminator(all_samples)&#xA;                loss_discriminator = self.criterion(output_discriminator, all_samples_labels)&#xA;                loss_discriminator.backward()&#xA;                self.optimizer_discriminator.step()&#xA;&#xA;                #  &#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                self.generator.zero_grad()&#xA;                generated_samples = self.generator(latent_samples)&#xA;&#xA;                output_discriminator_generated = self.discriminator(generated_samples)&#xA;&#xA;                generator_loss = self.criterion(output_discriminator_generated, real_samples_labels)&#xA;                generator_loss.backward()&#xA;                self.optimizer_generator.step()&#xA;&#xA;                if n == self.batch_size - 1:&#xA;                    tqdm.write(f&amp;quot;Epoch: {epoch} Loss D.: {loss_discriminator}\n&amp;quot; +&#xA;                               f&amp;quot;Epoch: {epoch} Loss G.: {generator_loss}&amp;quot;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""382496"" LastEditorUserId=""211923"" LastEditDate=""2020-08-21T09:50:27.383"" LastActivityDate=""2020-08-21T09:50:27.383"" Title="" , "" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1168634"" PostTypeId=""1"" AcceptedAnswerId=""1168657"" CreationDate=""2020-08-21T08:45:19.413"" Score=""1"" ViewCount=""73"" Body=""&lt;p&gt;   GAN-  pytorch.      &lt;code&gt;CUDA error: an illegal memory access was encountered&lt;/code&gt;.  ,   ,         ,   google collab.  ,   -   .&#xA;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import os&#xA;import torch&#xA;import torchvision&#xA;from torch import nn&#xA;from tqdm import tqdm&#xA;import torch.nn.functional as f&#xA;import matplotlib.pyplot as plt&#xA;import torchvision.transforms as transforms&#xA;&#xA;torch.manual_seed(42)&#xA;&#xA;&#xA;class Discriminator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Discriminator, self).__init__()&#xA;&#xA;        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6)&#xA;&#xA;        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)&#xA;&#xA;        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5)&#xA;        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)&#xA;&#xA;        self.fc1 = nn.Linear(5*5*32, 256)&#xA;        self.fc2 = nn.Linear(256, 128)&#xA;        self.fc3 = nn.Linear(128, 1)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.conv1(x))&#xA;        x = self.max_pool(x)&#xA;        x = f.relu(self.conv2(x))&#xA;        x = f.relu(self.conv3(x))&#xA;&#xA;        x = x.view(-1, 5*5*32)&#xA;&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = self.fc3(x)&#xA;&#xA;        x = f.softmax(x, dim=-1)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class Generator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Generator, self).__init__()&#xA;&#xA;        self.fc1 = nn.Linear(100, 256)&#xA;        self.fc2 = nn.Linear(256, 512)&#xA;        self.fc3 = nn.Linear(512, 1024)&#xA;        self.fc4 = nn.Linear(1024, 784)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = f.relu(self.fc3(x))&#xA;        x = f.tanh(self.fc4(x))&#xA;&#xA;        x = x.view(x.size(0), 1, 28, 28)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class NetworkStuff:&#xA;    def __init__(self):&#xA;&#xA;        self.batch_size = 16&#xA;        self.lr = 0.0001&#xA;        self.num_epoch = 50&#xA;&#xA;        self.discriminator = Discriminator()&#xA;        self.generator = Generator()&#xA;&#xA;        self.criterion = nn.BCELoss()&#xA;&#xA;        self.optimizer_discriminator = torch.optim.Adam(&#xA;            self.discriminator.parameters(),&#xA;            self.lr&#xA;        )&#xA;        self.optimizer_generator = torch.optim.Adam(&#xA;            self.generator.parameters(),&#xA;            self.lr&#xA;        )&#xA;&#xA;        if torch.cuda.is_available():&#xA;            self.device = torch.device(&amp;quot;cuda&amp;quot;)&#xA;        else:&#xA;            self.device = torch.device(&amp;quot;cpu&amp;quot;)&#xA;&#xA;        self.transform = transforms.Compose([&#xA;            transforms.ToTensor(),&#xA;            transforms.Normalize((0.5,), (0.5,))&#xA;        ])&#xA;&#xA;        self.train_set = torchvision.datasets.MNIST(&#xA;            root=os.path.abspath(&amp;quot;data&amp;quot;), train=True, download=True,&#xA;            transform=self.transform&#xA;        )&#xA;&#xA;        self.train_loader = torch.utils.data.DataLoader(&#xA;            self.train_set, batch_size=self.batch_size,&#xA;            shuffle=True&#xA;        )&#xA;&#xA;    def show_samples(self, type_='generated'):&#xA;&#xA;        if type_ == &amp;quot;dataset&amp;quot;:&#xA;&#xA;            real_samples, mnist_labels = next(iter(self.train_loader))&#xA;            plt.suptitle('dataset')&#xA;&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(real_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        elif type_ == &amp;quot;generated&amp;quot;:&#xA;&#xA;            latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;            generated_samples = self.generator(latent_samples).detach()&#xA;&#xA;            plt.suptitle('generated')&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(generated_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        plt.show()&#xA;&#xA;    def train(self):&#xA;&#xA;        for epoch in tqdm(range(self.num_epoch)):&#xA;&#xA;            for n, (real_samples, mnist_labels) in tqdm(enumerate(self.train_loader)):&#xA;&#xA;                real_samples = real_samples.to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;&#xA;                generated_samples = self.generator(latent_samples)&#xA;                generated_samples_labels = torch.zeros((self.batch_size, 1)).to(self.device)&#xA;&#xA;                all_samples = torch.cat((real_samples, generated_samples))&#xA;                all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))&#xA;&#xA;                #  &#xA;                self.discriminator.zero_grad()&#xA;                output_discriminator = self.discriminator(all_samples)&#xA;                loss_discriminator = self.criterion(output_discriminator, all_samples_labels)&#xA;                loss_discriminator.backward()&#xA;                self.optimizer_discriminator.step()&#xA;&#xA;                #  &#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                self.generator.zero_grad()&#xA;                generated_samples = self.generator(latent_samples)&#xA;&#xA;                output_discriminator_generated = self.discriminator(generated_samples)&#xA;&#xA;                generator_loss = self.criterion(output_discriminator_generated, real_samples_labels)&#xA;                generator_loss.backward()&#xA;                self.optimizer_generator.step()&#xA;&#xA;                if n == self.batch_size - 1:&#xA;                    tqdm.write(f&amp;quot;Epoch: {epoch} Loss D.: {loss_discriminator}\n&amp;quot; +&#xA;                               f&amp;quot;Epoch: {epoch} Loss G.: {generator_loss}&amp;quot;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""382496"" LastEditorUserId=""211923"" LastEditDate=""2020-08-21T09:50:27.383"" LastActivityDate=""2020-08-21T09:50:27.383"" Title="" , "" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1168634"" PostTypeId=""1"" AcceptedAnswerId=""1168657"" CreationDate=""2020-08-21T08:45:19.413"" Score=""1"" ViewCount=""73"" Body=""&lt;p&gt;   GAN-  pytorch.      &lt;code&gt;CUDA error: an illegal memory access was encountered&lt;/code&gt;.  ,   ,         ,   google collab.  ,   -   .&#xA;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import os&#xA;import torch&#xA;import torchvision&#xA;from torch import nn&#xA;from tqdm import tqdm&#xA;import torch.nn.functional as f&#xA;import matplotlib.pyplot as plt&#xA;import torchvision.transforms as transforms&#xA;&#xA;torch.manual_seed(42)&#xA;&#xA;&#xA;class Discriminator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Discriminator, self).__init__()&#xA;&#xA;        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=6)&#xA;&#xA;        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)&#xA;&#xA;        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5)&#xA;        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)&#xA;&#xA;        self.fc1 = nn.Linear(5*5*32, 256)&#xA;        self.fc2 = nn.Linear(256, 128)&#xA;        self.fc3 = nn.Linear(128, 1)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.conv1(x))&#xA;        x = self.max_pool(x)&#xA;        x = f.relu(self.conv2(x))&#xA;        x = f.relu(self.conv3(x))&#xA;&#xA;        x = x.view(-1, 5*5*32)&#xA;&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = self.fc3(x)&#xA;&#xA;        x = f.softmax(x, dim=-1)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class Generator(nn.Module):&#xA;    def __init__(self):&#xA;        super(Generator, self).__init__()&#xA;&#xA;        self.fc1 = nn.Linear(100, 256)&#xA;        self.fc2 = nn.Linear(256, 512)&#xA;        self.fc3 = nn.Linear(512, 1024)&#xA;        self.fc4 = nn.Linear(1024, 784)&#xA;&#xA;    def forward(self, x):&#xA;        x = f.relu(self.fc1(x))&#xA;        x = f.relu(self.fc2(x))&#xA;        x = f.relu(self.fc3(x))&#xA;        x = f.tanh(self.fc4(x))&#xA;&#xA;        x = x.view(x.size(0), 1, 28, 28)&#xA;&#xA;        return x&#xA;&#xA;&#xA;class NetworkStuff:&#xA;    def __init__(self):&#xA;&#xA;        self.batch_size = 16&#xA;        self.lr = 0.0001&#xA;        self.num_epoch = 50&#xA;&#xA;        self.discriminator = Discriminator()&#xA;        self.generator = Generator()&#xA;&#xA;        self.criterion = nn.BCELoss()&#xA;&#xA;        self.optimizer_discriminator = torch.optim.Adam(&#xA;            self.discriminator.parameters(),&#xA;            self.lr&#xA;        )&#xA;        self.optimizer_generator = torch.optim.Adam(&#xA;            self.generator.parameters(),&#xA;            self.lr&#xA;        )&#xA;&#xA;        if torch.cuda.is_available():&#xA;            self.device = torch.device(&amp;quot;cuda&amp;quot;)&#xA;        else:&#xA;            self.device = torch.device(&amp;quot;cpu&amp;quot;)&#xA;&#xA;        self.transform = transforms.Compose([&#xA;            transforms.ToTensor(),&#xA;            transforms.Normalize((0.5,), (0.5,))&#xA;        ])&#xA;&#xA;        self.train_set = torchvision.datasets.MNIST(&#xA;            root=os.path.abspath(&amp;quot;data&amp;quot;), train=True, download=True,&#xA;            transform=self.transform&#xA;        )&#xA;&#xA;        self.train_loader = torch.utils.data.DataLoader(&#xA;            self.train_set, batch_size=self.batch_size,&#xA;            shuffle=True&#xA;        )&#xA;&#xA;    def show_samples(self, type_='generated'):&#xA;&#xA;        if type_ == &amp;quot;dataset&amp;quot;:&#xA;&#xA;            real_samples, mnist_labels = next(iter(self.train_loader))&#xA;            plt.suptitle('dataset')&#xA;&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(real_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        elif type_ == &amp;quot;generated&amp;quot;:&#xA;&#xA;            latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;            generated_samples = self.generator(latent_samples).detach()&#xA;&#xA;            plt.suptitle('generated')&#xA;            for i in range(16):&#xA;                plt.subplot(4, 4, i + 1)&#xA;                plt.imshow(generated_samples[i].reshape(28, 28), cmap=&amp;quot;gray_r&amp;quot;)&#xA;                plt.xticks([])&#xA;                plt.yticks([])&#xA;&#xA;        plt.show()&#xA;&#xA;    def train(self):&#xA;&#xA;        for epoch in tqdm(range(self.num_epoch)):&#xA;&#xA;            for n, (real_samples, mnist_labels) in tqdm(enumerate(self.train_loader)):&#xA;&#xA;                real_samples = real_samples.to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;&#xA;                generated_samples = self.generator(latent_samples)&#xA;                generated_samples_labels = torch.zeros((self.batch_size, 1)).to(self.device)&#xA;&#xA;                all_samples = torch.cat((real_samples, generated_samples))&#xA;                all_samples_labels = torch.cat((real_samples_labels, generated_samples_labels))&#xA;&#xA;                #  &#xA;                self.discriminator.zero_grad()&#xA;                output_discriminator = self.discriminator(all_samples)&#xA;                loss_discriminator = self.criterion(output_discriminator, all_samples_labels)&#xA;                loss_discriminator.backward()&#xA;                self.optimizer_discriminator.step()&#xA;&#xA;                #  &#xA;                latent_samples = torch.rand((self.batch_size, 100)).to(self.device)&#xA;                real_samples_labels = torch.ones((self.batch_size, 1)).to(self.device)&#xA;&#xA;                self.generator.zero_grad()&#xA;                generated_samples = self.generator(latent_samples)&#xA;&#xA;                output_discriminator_generated = self.discriminator(generated_samples)&#xA;&#xA;                generator_loss = self.criterion(output_discriminator_generated, real_samples_labels)&#xA;                generator_loss.backward()&#xA;                self.optimizer_generator.step()&#xA;&#xA;                if n == self.batch_size - 1:&#xA;                    tqdm.write(f&amp;quot;Epoch: {epoch} Loss D.: {loss_discriminator}\n&amp;quot; +&#xA;                               f&amp;quot;Epoch: {epoch} Loss G.: {generator_loss}&amp;quot;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""382496"" LastEditorUserId=""211923"" LastEditDate=""2020-08-21T09:50:27.383"" LastActivityDate=""2020-08-21T09:50:27.383"" Title="" , "" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1277582"" PostTypeId=""1"" CreationDate=""2021-05-03T09:04:13.433"" Score=""1"" ViewCount=""9"" Body=""&lt;p&gt;    Google Collabotary   . ,        .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time, datetime&#xA;&#xA;criterion = nn.CrossEntropyLoss()&#xA;model.train()&#xA;model.to(device)&#xA;&#xA;#Set one random state&#xA;seed_val = 42&#xA;&#xA;torch.manual_seed(seed_val)&#xA;torch.cuda.manual_seed_all(seed_val)&#xA;&#xA;# We'll store a number of quantities such as training and validation loss, &#xA;# validation accuracy, and timings.&#xA;training_stats = []&#xA;&#xA;# Measure the total training time for the whole run.&#xA;total_t0 = time.time()&#xA;for epoch in range(1):&#xA;  for step, batch in enumerate(train_dataloader):&#xA;    if step % 30 == 0 and step != 0:&#xA;      print('{}/{}. Time elapsed: {}'.format(step+1, len(train_dataloader), format_time(time.time()-total_t0)))&#xA;    ids = batch[0].to(device)&#xA;    mask = batch[1].to(device)&#xA;    label = batch[2].to(device)&#xA;&#xA;    model.zero_grad()&#xA;    output = model.forward(ids, mask)&#xA;    loos = criterion(output, label)&#xA;&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch import nn&#xA;from transformers import BertForSequenceClassification, AdamW, BertConfig&#xA;&#xA;class MyBERT(nn.Module):&#xA;  def __init__(self):&#xA;    super(MyBERT, self).__init__()&#xA;    self.bert = torch.load('/content/drive/MyDrive/BERT').to(device)&#xA;    self.linear1 = nn.Linear(768, 256)&#xA;    self.linear2 = nn.Linear(256, 2)&#xA;    self.relu = nn.ReLU()&#xA;    self.softmax = nn.Softmax()&#xA;  def forward(self, ids, masks):&#xA;    output = self.bert(&#xA;        ids,&#xA;        token_type_ids=None, &#xA;        attention_mask=masks,&#xA;        output_hidden_states=True&#xA;    )&#xA;    result = 0&#xA;    print(output.hidden_states[0].shape)&#xA;    result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;    print(result.shape)&#xA;    result = self.relu(self.linear1(result))&#xA;    print(result.shape)&#xA;    result = self.softmax(self.linear2(result))&#xA;    print(result.shape)&#xA;    return result&#xA;  def SetBert(self, status):&#xA;    for param in self.bert.parameters():&#xA;      param.requires_grad = status&#xA;&#xA;model = MyBERT()&#xA;model.SetBert(False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-18-c10dad19242f&amp;gt; in &amp;lt;module&amp;gt;()&#xA;     27 &#xA;     28     model.zero_grad()&#xA;---&amp;gt; 29     output = model.forward(ids, mask)&#xA;     30     loos = criterion(output, label)&#xA;     31 &#xA;&#xA;&amp;lt;ipython-input-17-7b8d7c292bfb&amp;gt; in forward(self, ids, masks)&#xA;     19     result = 0&#xA;     20     print(output.hidden_states[0].shape)&#xA;---&amp;gt; 21     result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;     22     print(result.shape)&#xA;     23     result = self.relu(self.linear1(result))&#xA;&#xA;RuntimeError: CUDA error: device-side assert triggered&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""360793"" LastActivityDate=""2021-05-03T09:04:13.433"" Title=""Pytorch CUDA     index_select"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1277582"" PostTypeId=""1"" CreationDate=""2021-05-03T09:04:13.433"" Score=""1"" ViewCount=""9"" Body=""&lt;p&gt;    Google Collabotary   . ,        .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time, datetime&#xA;&#xA;criterion = nn.CrossEntropyLoss()&#xA;model.train()&#xA;model.to(device)&#xA;&#xA;#Set one random state&#xA;seed_val = 42&#xA;&#xA;torch.manual_seed(seed_val)&#xA;torch.cuda.manual_seed_all(seed_val)&#xA;&#xA;# We'll store a number of quantities such as training and validation loss, &#xA;# validation accuracy, and timings.&#xA;training_stats = []&#xA;&#xA;# Measure the total training time for the whole run.&#xA;total_t0 = time.time()&#xA;for epoch in range(1):&#xA;  for step, batch in enumerate(train_dataloader):&#xA;    if step % 30 == 0 and step != 0:&#xA;      print('{}/{}. Time elapsed: {}'.format(step+1, len(train_dataloader), format_time(time.time()-total_t0)))&#xA;    ids = batch[0].to(device)&#xA;    mask = batch[1].to(device)&#xA;    label = batch[2].to(device)&#xA;&#xA;    model.zero_grad()&#xA;    output = model.forward(ids, mask)&#xA;    loos = criterion(output, label)&#xA;&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch import nn&#xA;from transformers import BertForSequenceClassification, AdamW, BertConfig&#xA;&#xA;class MyBERT(nn.Module):&#xA;  def __init__(self):&#xA;    super(MyBERT, self).__init__()&#xA;    self.bert = torch.load('/content/drive/MyDrive/BERT').to(device)&#xA;    self.linear1 = nn.Linear(768, 256)&#xA;    self.linear2 = nn.Linear(256, 2)&#xA;    self.relu = nn.ReLU()&#xA;    self.softmax = nn.Softmax()&#xA;  def forward(self, ids, masks):&#xA;    output = self.bert(&#xA;        ids,&#xA;        token_type_ids=None, &#xA;        attention_mask=masks,&#xA;        output_hidden_states=True&#xA;    )&#xA;    result = 0&#xA;    print(output.hidden_states[0].shape)&#xA;    result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;    print(result.shape)&#xA;    result = self.relu(self.linear1(result))&#xA;    print(result.shape)&#xA;    result = self.softmax(self.linear2(result))&#xA;    print(result.shape)&#xA;    return result&#xA;  def SetBert(self, status):&#xA;    for param in self.bert.parameters():&#xA;      param.requires_grad = status&#xA;&#xA;model = MyBERT()&#xA;model.SetBert(False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-18-c10dad19242f&amp;gt; in &amp;lt;module&amp;gt;()&#xA;     27 &#xA;     28     model.zero_grad()&#xA;---&amp;gt; 29     output = model.forward(ids, mask)&#xA;     30     loos = criterion(output, label)&#xA;     31 &#xA;&#xA;&amp;lt;ipython-input-17-7b8d7c292bfb&amp;gt; in forward(self, ids, masks)&#xA;     19     result = 0&#xA;     20     print(output.hidden_states[0].shape)&#xA;---&amp;gt; 21     result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;     22     print(result.shape)&#xA;     23     result = self.relu(self.linear1(result))&#xA;&#xA;RuntimeError: CUDA error: device-side assert triggered&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""360793"" LastActivityDate=""2021-05-03T09:04:13.433"" Title=""Pytorch CUDA     index_select"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1277582"" PostTypeId=""1"" CreationDate=""2021-05-03T09:04:13.433"" Score=""1"" ViewCount=""9"" Body=""&lt;p&gt;    Google Collabotary   . ,        .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time, datetime&#xA;&#xA;criterion = nn.CrossEntropyLoss()&#xA;model.train()&#xA;model.to(device)&#xA;&#xA;#Set one random state&#xA;seed_val = 42&#xA;&#xA;torch.manual_seed(seed_val)&#xA;torch.cuda.manual_seed_all(seed_val)&#xA;&#xA;# We'll store a number of quantities such as training and validation loss, &#xA;# validation accuracy, and timings.&#xA;training_stats = []&#xA;&#xA;# Measure the total training time for the whole run.&#xA;total_t0 = time.time()&#xA;for epoch in range(1):&#xA;  for step, batch in enumerate(train_dataloader):&#xA;    if step % 30 == 0 and step != 0:&#xA;      print('{}/{}. Time elapsed: {}'.format(step+1, len(train_dataloader), format_time(time.time()-total_t0)))&#xA;    ids = batch[0].to(device)&#xA;    mask = batch[1].to(device)&#xA;    label = batch[2].to(device)&#xA;&#xA;    model.zero_grad()&#xA;    output = model.forward(ids, mask)&#xA;    loos = criterion(output, label)&#xA;&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch import nn&#xA;from transformers import BertForSequenceClassification, AdamW, BertConfig&#xA;&#xA;class MyBERT(nn.Module):&#xA;  def __init__(self):&#xA;    super(MyBERT, self).__init__()&#xA;    self.bert = torch.load('/content/drive/MyDrive/BERT').to(device)&#xA;    self.linear1 = nn.Linear(768, 256)&#xA;    self.linear2 = nn.Linear(256, 2)&#xA;    self.relu = nn.ReLU()&#xA;    self.softmax = nn.Softmax()&#xA;  def forward(self, ids, masks):&#xA;    output = self.bert(&#xA;        ids,&#xA;        token_type_ids=None, &#xA;        attention_mask=masks,&#xA;        output_hidden_states=True&#xA;    )&#xA;    result = 0&#xA;    print(output.hidden_states[0].shape)&#xA;    result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;    print(result.shape)&#xA;    result = self.relu(self.linear1(result))&#xA;    print(result.shape)&#xA;    result = self.softmax(self.linear2(result))&#xA;    print(result.shape)&#xA;    return result&#xA;  def SetBert(self, status):&#xA;    for param in self.bert.parameters():&#xA;      param.requires_grad = status&#xA;&#xA;model = MyBERT()&#xA;model.SetBert(False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-18-c10dad19242f&amp;gt; in &amp;lt;module&amp;gt;()&#xA;     27 &#xA;     28     model.zero_grad()&#xA;---&amp;gt; 29     output = model.forward(ids, mask)&#xA;     30     loos = criterion(output, label)&#xA;     31 &#xA;&#xA;&amp;lt;ipython-input-17-7b8d7c292bfb&amp;gt; in forward(self, ids, masks)&#xA;     19     result = 0&#xA;     20     print(output.hidden_states[0].shape)&#xA;---&amp;gt; 21     result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;     22     print(result.shape)&#xA;     23     result = self.relu(self.linear1(result))&#xA;&#xA;RuntimeError: CUDA error: device-side assert triggered&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""360793"" LastActivityDate=""2021-05-03T09:04:13.433"" Title=""Pytorch CUDA     index_select"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1277582"" PostTypeId=""1"" CreationDate=""2021-05-03T09:04:13.433"" Score=""1"" ViewCount=""9"" Body=""&lt;p&gt;    Google Collabotary   . ,        .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time, datetime&#xA;&#xA;criterion = nn.CrossEntropyLoss()&#xA;model.train()&#xA;model.to(device)&#xA;&#xA;#Set one random state&#xA;seed_val = 42&#xA;&#xA;torch.manual_seed(seed_val)&#xA;torch.cuda.manual_seed_all(seed_val)&#xA;&#xA;# We'll store a number of quantities such as training and validation loss, &#xA;# validation accuracy, and timings.&#xA;training_stats = []&#xA;&#xA;# Measure the total training time for the whole run.&#xA;total_t0 = time.time()&#xA;for epoch in range(1):&#xA;  for step, batch in enumerate(train_dataloader):&#xA;    if step % 30 == 0 and step != 0:&#xA;      print('{}/{}. Time elapsed: {}'.format(step+1, len(train_dataloader), format_time(time.time()-total_t0)))&#xA;    ids = batch[0].to(device)&#xA;    mask = batch[1].to(device)&#xA;    label = batch[2].to(device)&#xA;&#xA;    model.zero_grad()&#xA;    output = model.forward(ids, mask)&#xA;    loos = criterion(output, label)&#xA;&#xA;model.eval()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch import nn&#xA;from transformers import BertForSequenceClassification, AdamW, BertConfig&#xA;&#xA;class MyBERT(nn.Module):&#xA;  def __init__(self):&#xA;    super(MyBERT, self).__init__()&#xA;    self.bert = torch.load('/content/drive/MyDrive/BERT').to(device)&#xA;    self.linear1 = nn.Linear(768, 256)&#xA;    self.linear2 = nn.Linear(256, 2)&#xA;    self.relu = nn.ReLU()&#xA;    self.softmax = nn.Softmax()&#xA;  def forward(self, ids, masks):&#xA;    output = self.bert(&#xA;        ids,&#xA;        token_type_ids=None, &#xA;        attention_mask=masks,&#xA;        output_hidden_states=True&#xA;    )&#xA;    result = 0&#xA;    print(output.hidden_states[0].shape)&#xA;    result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;    print(result.shape)&#xA;    result = self.relu(self.linear1(result))&#xA;    print(result.shape)&#xA;    result = self.softmax(self.linear2(result))&#xA;    print(result.shape)&#xA;    return result&#xA;  def SetBert(self, status):&#xA;    for param in self.bert.parameters():&#xA;      param.requires_grad = status&#xA;&#xA;model = MyBERT()&#xA;model.SetBert(False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-18-c10dad19242f&amp;gt; in &amp;lt;module&amp;gt;()&#xA;     27 &#xA;     28     model.zero_grad()&#xA;---&amp;gt; 29     output = model.forward(ids, mask)&#xA;     30     loos = criterion(output, label)&#xA;     31 &#xA;&#xA;&amp;lt;ipython-input-17-7b8d7c292bfb&amp;gt; in forward(self, ids, masks)&#xA;     19     result = 0&#xA;     20     print(output.hidden_states[0].shape)&#xA;---&amp;gt; 21     result = torch.index_select(output.hidden_states[0].to(device), 1, torch.tensor([-1]).to(device))&#xA;     22     print(result.shape)&#xA;     23     result = self.relu(self.linear1(result))&#xA;&#xA;RuntimeError: CUDA error: device-side assert triggered&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""360793"" LastActivityDate=""2021-05-03T09:04:13.433"" Title=""Pytorch CUDA     index_select"" Tags=""&lt;python&gt;&lt;cuda&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1314420"" PostTypeId=""1"" CreationDate=""2021-08-05T12:27:26.757"" Score=""2"" ViewCount=""257"" Body=""&lt;p&gt;   -       :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeError: CUDA out of memory. Tried to allocate 9.88 GiB (GPU 0; 10.91 GiB total capacity; 1.89 GiB already allocated; 8.05 GiB free; 1.91 GiB reserved in total by PyTorch)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;     .&lt;/p&gt;&#xA;&lt;p&gt; nvidia-smi:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/DUrNg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DUrNg.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   2 x NVIDIA GeForce GTX 1080 Ti&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&#xA;from torchvision import transforms, models&#xA;import torch.nn as nn&#xA;from torch.autograd import Variable&#xA;from PIL import Image&#xA;&#xA;&#xA;torch.cuda.empty_cache()&#xA;device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)&#xA;print(torch.cuda.device_count())&#xA;&#xA;model_conv = models.resnet18(pretrained=True)&#xA;model_conv.to(device)&#xA;num_ftrs = model_conv.fc.in_features&#xA;model_conv.fc = nn.Linear(num_ftrs, 2)&#xA;model_conv.load_state_dict(torch.load('model2'), strict=False)&#xA;&#xA;test_transforms = transforms.Compose([transforms.Resize(79*128),&#xA;                                      transforms.ToTensor(),&#xA;                                     ])&#xA;&#xA;def predict_image(image):&#xA;    image_tensor = test_transforms(image).float()&#xA;    image_tensor = image_tensor.unsqueeze_(0)&#xA;    input = Variable(image_tensor)&#xA;    input = input.to(device)&#xA;    with torch.no_grad():&#xA;        output = model_conv(input)&#xA;    index = output.data.gpu().numpy().argmax()&#xA;    return index&#xA;&#xA;img = Image.open('/home/georgy/dataset/123/bad/46.jpeg')&#xA;&#xA;print(predict_image(img))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""380293"" LastEditorUserId=""380293"" LastEditDate=""2021-08-05T12:36:36.793"" LastActivityDate=""2021-08-05T12:36:36.793"" Title=""Pytorch CUDA out of memory"" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1314420"" PostTypeId=""1"" CreationDate=""2021-08-05T12:27:26.757"" Score=""2"" ViewCount=""257"" Body=""&lt;p&gt;   -       :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeError: CUDA out of memory. Tried to allocate 9.88 GiB (GPU 0; 10.91 GiB total capacity; 1.89 GiB already allocated; 8.05 GiB free; 1.91 GiB reserved in total by PyTorch)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;     .&lt;/p&gt;&#xA;&lt;p&gt; nvidia-smi:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/DUrNg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DUrNg.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   2 x NVIDIA GeForce GTX 1080 Ti&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&#xA;from torchvision import transforms, models&#xA;import torch.nn as nn&#xA;from torch.autograd import Variable&#xA;from PIL import Image&#xA;&#xA;&#xA;torch.cuda.empty_cache()&#xA;device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)&#xA;print(torch.cuda.device_count())&#xA;&#xA;model_conv = models.resnet18(pretrained=True)&#xA;model_conv.to(device)&#xA;num_ftrs = model_conv.fc.in_features&#xA;model_conv.fc = nn.Linear(num_ftrs, 2)&#xA;model_conv.load_state_dict(torch.load('model2'), strict=False)&#xA;&#xA;test_transforms = transforms.Compose([transforms.Resize(79*128),&#xA;                                      transforms.ToTensor(),&#xA;                                     ])&#xA;&#xA;def predict_image(image):&#xA;    image_tensor = test_transforms(image).float()&#xA;    image_tensor = image_tensor.unsqueeze_(0)&#xA;    input = Variable(image_tensor)&#xA;    input = input.to(device)&#xA;    with torch.no_grad():&#xA;        output = model_conv(input)&#xA;    index = output.data.gpu().numpy().argmax()&#xA;    return index&#xA;&#xA;img = Image.open('/home/georgy/dataset/123/bad/46.jpeg')&#xA;&#xA;print(predict_image(img))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""380293"" LastEditorUserId=""380293"" LastEditDate=""2021-08-05T12:36:36.793"" LastActivityDate=""2021-08-05T12:36:36.793"" Title=""Pytorch CUDA out of memory"" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1314420"" PostTypeId=""1"" CreationDate=""2021-08-05T12:27:26.757"" Score=""2"" ViewCount=""257"" Body=""&lt;p&gt;   -       :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeError: CUDA out of memory. Tried to allocate 9.88 GiB (GPU 0; 10.91 GiB total capacity; 1.89 GiB already allocated; 8.05 GiB free; 1.91 GiB reserved in total by PyTorch)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;     .&lt;/p&gt;&#xA;&lt;p&gt; nvidia-smi:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/DUrNg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DUrNg.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   2 x NVIDIA GeForce GTX 1080 Ti&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&#xA;from torchvision import transforms, models&#xA;import torch.nn as nn&#xA;from torch.autograd import Variable&#xA;from PIL import Image&#xA;&#xA;&#xA;torch.cuda.empty_cache()&#xA;device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)&#xA;print(torch.cuda.device_count())&#xA;&#xA;model_conv = models.resnet18(pretrained=True)&#xA;model_conv.to(device)&#xA;num_ftrs = model_conv.fc.in_features&#xA;model_conv.fc = nn.Linear(num_ftrs, 2)&#xA;model_conv.load_state_dict(torch.load('model2'), strict=False)&#xA;&#xA;test_transforms = transforms.Compose([transforms.Resize(79*128),&#xA;                                      transforms.ToTensor(),&#xA;                                     ])&#xA;&#xA;def predict_image(image):&#xA;    image_tensor = test_transforms(image).float()&#xA;    image_tensor = image_tensor.unsqueeze_(0)&#xA;    input = Variable(image_tensor)&#xA;    input = input.to(device)&#xA;    with torch.no_grad():&#xA;        output = model_conv(input)&#xA;    index = output.data.gpu().numpy().argmax()&#xA;    return index&#xA;&#xA;img = Image.open('/home/georgy/dataset/123/bad/46.jpeg')&#xA;&#xA;print(predict_image(img))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""380293"" LastEditorUserId=""380293"" LastEditDate=""2021-08-05T12:36:36.793"" LastActivityDate=""2021-08-05T12:36:36.793"" Title=""Pytorch CUDA out of memory"" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1314420"" PostTypeId=""1"" CreationDate=""2021-08-05T12:27:26.757"" Score=""2"" ViewCount=""257"" Body=""&lt;p&gt;   -       :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeError: CUDA out of memory. Tried to allocate 9.88 GiB (GPU 0; 10.91 GiB total capacity; 1.89 GiB already allocated; 8.05 GiB free; 1.91 GiB reserved in total by PyTorch)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;     .&lt;/p&gt;&#xA;&lt;p&gt; nvidia-smi:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/DUrNg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DUrNg.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   2 x NVIDIA GeForce GTX 1080 Ti&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&#xA;from torchvision import transforms, models&#xA;import torch.nn as nn&#xA;from torch.autograd import Variable&#xA;from PIL import Image&#xA;&#xA;&#xA;torch.cuda.empty_cache()&#xA;device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)&#xA;print(torch.cuda.device_count())&#xA;&#xA;model_conv = models.resnet18(pretrained=True)&#xA;model_conv.to(device)&#xA;num_ftrs = model_conv.fc.in_features&#xA;model_conv.fc = nn.Linear(num_ftrs, 2)&#xA;model_conv.load_state_dict(torch.load('model2'), strict=False)&#xA;&#xA;test_transforms = transforms.Compose([transforms.Resize(79*128),&#xA;                                      transforms.ToTensor(),&#xA;                                     ])&#xA;&#xA;def predict_image(image):&#xA;    image_tensor = test_transforms(image).float()&#xA;    image_tensor = image_tensor.unsqueeze_(0)&#xA;    input = Variable(image_tensor)&#xA;    input = input.to(device)&#xA;    with torch.no_grad():&#xA;        output = model_conv(input)&#xA;    index = output.data.gpu().numpy().argmax()&#xA;    return index&#xA;&#xA;img = Image.open('/home/georgy/dataset/123/bad/46.jpeg')&#xA;&#xA;print(predict_image(img))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""380293"" LastEditorUserId=""380293"" LastEditDate=""2021-08-05T12:36:36.793"" LastActivityDate=""2021-08-05T12:36:36.793"" Title=""Pytorch CUDA out of memory"" Tags=""&lt;python&gt;&lt;-&gt;&lt;-&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1370777"" PostTypeId=""1"" CreationDate=""2022-01-12T11:44:16.177"" Score=""0"" ViewCount=""43"" Body=""&lt;p&gt;   &#xA;    VGG16   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&#xA;VGG-16 Implementation on Cats&amp;amp;Dogs Dataset&#xA;&amp;quot;&amp;quot;&amp;quot;&#xA;&#xA;import keras&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from keras.backend import set_session&#xA;from keras.callbacks import ModelCheckpoint, EarlyStopping&#xA;from keras.layers import Conv2D, Dense, Flatten, MaxPool2D&#xA;from keras.models import Sequential, load_model&#xA;from keras.optimizers import adam_v2&#xA;from keras.preprocessing import image&#xA;from keras.preprocessing.image import ImageDataGenerator&#xA;&#xA;config = tf.compat.v1.ConfigProto()&#xA;config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU&#xA;config.log_device_placement = True  # to log device placement (on which device the operation ran)&#xA;sess = tf.compat.v1.Session(config=config)&#xA;set_session(sess)&#xA;&#xA;# Get the data&#xA;trdata = ImageDataGenerator()&#xA;traindata = trdata.flow_from_directory(directory=&amp;quot;C:/Temp/Datasets/CatsDogs/train&amp;quot;, target_size=(224, 224))&#xA;tsdata = ImageDataGenerator()&#xA;testdata = tsdata.flow_from_directory(directory='C:/Temp/Datasets/CatsDogs/validation', target_size=(224, 224))&#xA;&#xA;# Generate the model&#xA;model = Sequential()&#xA;# Layer 1: Convolutional&#xA;model.add(Conv2D(input_shape=(224, 224, 3), filters=64, kernel_size=(3, 3),&#xA;                 padding='same', activation='relu'))&#xA;# Layer 2: Convolutional&#xA;model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 3: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 4: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 5: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 6: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 7: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 8: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 9: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 10: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 11: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 12: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 13: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 14: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 15: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 16: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 17: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 18: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 19: Flatten&#xA;model.add(Flatten())&#xA;# Layer 20: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 21: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 22: Softmax Layer&#xA;model.add(Dense(units=2, activation='softmax'))&#xA;&#xA;# Add Optimizer and check accuracy metrics&#xA;optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.001)&#xA;model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy,&#xA;              metrics=['accuracy'])&#xA;# Check model summary&#xA;print(model.summary())&#xA;&#xA;checkpoint = ModelCheckpoint(&amp;quot;vgg16_1.h5&amp;quot;, monitor='val_acc', verbose=1, save_best_only=True,&#xA;                             save_weights_only=False, mode='auto', period=1)&#xA;earlystop = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')&#xA;hist = model.fit_generator(steps_per_epoch=100, generator=traindata, validation_data=testdata,&#xA;                           validation_steps=10, epochs=100,&#xA;                           callbacks=[checkpoint, earlystop])&#xA;&#xA;plt.plot(hist.history[&amp;quot;acc&amp;quot;])&#xA;plt.plot(hist.history['val_acc'])&#xA;plt.plot(hist.history['loss'])&#xA;plt.plot(hist.history['val_loss'])&#xA;plt.title(&amp;quot;model accuracy&amp;quot;)&#xA;plt.ylabel(&amp;quot;Accuracy&amp;quot;)&#xA;plt.xlabel(&amp;quot;Epoch&amp;quot;)&#xA;plt.legend([&amp;quot;Accuracy&amp;quot;, &amp;quot;Validation Accuracy&amp;quot;, &amp;quot;loss&amp;quot;, &amp;quot;Validation Loss&amp;quot;])&#xA;plt.show(block=True)&#xA;&#xA;# Try on test data&#xA;img = image.load_img(&amp;quot;C:/Temp/Datasets/CatsDogs/validation/39.jpg&amp;quot;, target_size=(224, 224))&#xA;img = np.asarray(img)&#xA;plt.imshow(img)&#xA;img = np.expand_dims(img, axis=0)&#xA;saved_model = load_model(&amp;quot;vgg16_1.h5&amp;quot;)&#xA;output = saved_model.predict(img)&#xA;if output[0][0] &amp;gt; output[0][1]:&#xA;    print(&amp;quot;cat&amp;quot;)&#xA;else:&#xA;    print('dog')&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      ValueError: Asked to retrieve element 0, but the Sequence has length 0.&#xA;         ? ,   .    .&#xA; tensorflow 2.7.0,  keras 2.7.0.&#xA;      :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):  &#xA;  File &amp;quot;D:/Code/Python/test/test10.py&amp;quot;, line 94, in &amp;lt;module&amp;gt;&#xA;    callbacks=[checkpoint, earlystop])  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-packages\keras\engine\training.py&amp;quot;, line 2030, in fit_generator&#xA;    initial_epoch=initial_epoch)  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler &#xA;    raise e.with_traceback(filtered_tb) from None  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras_preprocessing\image\iterator.py&amp;quot;, line 57, in __getitem__ length=len(self)))  &#xA;ValueError: Asked to retrieve element 0, but the Sequence has length 0  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""394072"" LastEditorUserId=""260769"" LastEditDate=""2022-01-12T14:40:54.727"" LastActivityDate=""2022-01-12T14:40:54.727"" Title="" ValueError    VGG16"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1370777"" PostTypeId=""1"" CreationDate=""2022-01-12T11:44:16.177"" Score=""0"" ViewCount=""43"" Body=""&lt;p&gt;   &#xA;    VGG16   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&#xA;VGG-16 Implementation on Cats&amp;amp;Dogs Dataset&#xA;&amp;quot;&amp;quot;&amp;quot;&#xA;&#xA;import keras&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from keras.backend import set_session&#xA;from keras.callbacks import ModelCheckpoint, EarlyStopping&#xA;from keras.layers import Conv2D, Dense, Flatten, MaxPool2D&#xA;from keras.models import Sequential, load_model&#xA;from keras.optimizers import adam_v2&#xA;from keras.preprocessing import image&#xA;from keras.preprocessing.image import ImageDataGenerator&#xA;&#xA;config = tf.compat.v1.ConfigProto()&#xA;config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU&#xA;config.log_device_placement = True  # to log device placement (on which device the operation ran)&#xA;sess = tf.compat.v1.Session(config=config)&#xA;set_session(sess)&#xA;&#xA;# Get the data&#xA;trdata = ImageDataGenerator()&#xA;traindata = trdata.flow_from_directory(directory=&amp;quot;C:/Temp/Datasets/CatsDogs/train&amp;quot;, target_size=(224, 224))&#xA;tsdata = ImageDataGenerator()&#xA;testdata = tsdata.flow_from_directory(directory='C:/Temp/Datasets/CatsDogs/validation', target_size=(224, 224))&#xA;&#xA;# Generate the model&#xA;model = Sequential()&#xA;# Layer 1: Convolutional&#xA;model.add(Conv2D(input_shape=(224, 224, 3), filters=64, kernel_size=(3, 3),&#xA;                 padding='same', activation='relu'))&#xA;# Layer 2: Convolutional&#xA;model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 3: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 4: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 5: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 6: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 7: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 8: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 9: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 10: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 11: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 12: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 13: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 14: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 15: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 16: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 17: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 18: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 19: Flatten&#xA;model.add(Flatten())&#xA;# Layer 20: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 21: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 22: Softmax Layer&#xA;model.add(Dense(units=2, activation='softmax'))&#xA;&#xA;# Add Optimizer and check accuracy metrics&#xA;optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.001)&#xA;model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy,&#xA;              metrics=['accuracy'])&#xA;# Check model summary&#xA;print(model.summary())&#xA;&#xA;checkpoint = ModelCheckpoint(&amp;quot;vgg16_1.h5&amp;quot;, monitor='val_acc', verbose=1, save_best_only=True,&#xA;                             save_weights_only=False, mode='auto', period=1)&#xA;earlystop = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')&#xA;hist = model.fit_generator(steps_per_epoch=100, generator=traindata, validation_data=testdata,&#xA;                           validation_steps=10, epochs=100,&#xA;                           callbacks=[checkpoint, earlystop])&#xA;&#xA;plt.plot(hist.history[&amp;quot;acc&amp;quot;])&#xA;plt.plot(hist.history['val_acc'])&#xA;plt.plot(hist.history['loss'])&#xA;plt.plot(hist.history['val_loss'])&#xA;plt.title(&amp;quot;model accuracy&amp;quot;)&#xA;plt.ylabel(&amp;quot;Accuracy&amp;quot;)&#xA;plt.xlabel(&amp;quot;Epoch&amp;quot;)&#xA;plt.legend([&amp;quot;Accuracy&amp;quot;, &amp;quot;Validation Accuracy&amp;quot;, &amp;quot;loss&amp;quot;, &amp;quot;Validation Loss&amp;quot;])&#xA;plt.show(block=True)&#xA;&#xA;# Try on test data&#xA;img = image.load_img(&amp;quot;C:/Temp/Datasets/CatsDogs/validation/39.jpg&amp;quot;, target_size=(224, 224))&#xA;img = np.asarray(img)&#xA;plt.imshow(img)&#xA;img = np.expand_dims(img, axis=0)&#xA;saved_model = load_model(&amp;quot;vgg16_1.h5&amp;quot;)&#xA;output = saved_model.predict(img)&#xA;if output[0][0] &amp;gt; output[0][1]:&#xA;    print(&amp;quot;cat&amp;quot;)&#xA;else:&#xA;    print('dog')&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      ValueError: Asked to retrieve element 0, but the Sequence has length 0.&#xA;         ? ,   .    .&#xA; tensorflow 2.7.0,  keras 2.7.0.&#xA;      :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):  &#xA;  File &amp;quot;D:/Code/Python/test/test10.py&amp;quot;, line 94, in &amp;lt;module&amp;gt;&#xA;    callbacks=[checkpoint, earlystop])  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-packages\keras\engine\training.py&amp;quot;, line 2030, in fit_generator&#xA;    initial_epoch=initial_epoch)  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler &#xA;    raise e.with_traceback(filtered_tb) from None  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras_preprocessing\image\iterator.py&amp;quot;, line 57, in __getitem__ length=len(self)))  &#xA;ValueError: Asked to retrieve element 0, but the Sequence has length 0  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""394072"" LastEditorUserId=""260769"" LastEditDate=""2022-01-12T14:40:54.727"" LastActivityDate=""2022-01-12T14:40:54.727"" Title="" ValueError    VGG16"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1370777"" PostTypeId=""1"" CreationDate=""2022-01-12T11:44:16.177"" Score=""0"" ViewCount=""43"" Body=""&lt;p&gt;   &#xA;    VGG16   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&#xA;VGG-16 Implementation on Cats&amp;amp;Dogs Dataset&#xA;&amp;quot;&amp;quot;&amp;quot;&#xA;&#xA;import keras&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from keras.backend import set_session&#xA;from keras.callbacks import ModelCheckpoint, EarlyStopping&#xA;from keras.layers import Conv2D, Dense, Flatten, MaxPool2D&#xA;from keras.models import Sequential, load_model&#xA;from keras.optimizers import adam_v2&#xA;from keras.preprocessing import image&#xA;from keras.preprocessing.image import ImageDataGenerator&#xA;&#xA;config = tf.compat.v1.ConfigProto()&#xA;config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU&#xA;config.log_device_placement = True  # to log device placement (on which device the operation ran)&#xA;sess = tf.compat.v1.Session(config=config)&#xA;set_session(sess)&#xA;&#xA;# Get the data&#xA;trdata = ImageDataGenerator()&#xA;traindata = trdata.flow_from_directory(directory=&amp;quot;C:/Temp/Datasets/CatsDogs/train&amp;quot;, target_size=(224, 224))&#xA;tsdata = ImageDataGenerator()&#xA;testdata = tsdata.flow_from_directory(directory='C:/Temp/Datasets/CatsDogs/validation', target_size=(224, 224))&#xA;&#xA;# Generate the model&#xA;model = Sequential()&#xA;# Layer 1: Convolutional&#xA;model.add(Conv2D(input_shape=(224, 224, 3), filters=64, kernel_size=(3, 3),&#xA;                 padding='same', activation='relu'))&#xA;# Layer 2: Convolutional&#xA;model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 3: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 4: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 5: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 6: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 7: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 8: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 9: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 10: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 11: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 12: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 13: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 14: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 15: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 16: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 17: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 18: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 19: Flatten&#xA;model.add(Flatten())&#xA;# Layer 20: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 21: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 22: Softmax Layer&#xA;model.add(Dense(units=2, activation='softmax'))&#xA;&#xA;# Add Optimizer and check accuracy metrics&#xA;optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.001)&#xA;model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy,&#xA;              metrics=['accuracy'])&#xA;# Check model summary&#xA;print(model.summary())&#xA;&#xA;checkpoint = ModelCheckpoint(&amp;quot;vgg16_1.h5&amp;quot;, monitor='val_acc', verbose=1, save_best_only=True,&#xA;                             save_weights_only=False, mode='auto', period=1)&#xA;earlystop = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')&#xA;hist = model.fit_generator(steps_per_epoch=100, generator=traindata, validation_data=testdata,&#xA;                           validation_steps=10, epochs=100,&#xA;                           callbacks=[checkpoint, earlystop])&#xA;&#xA;plt.plot(hist.history[&amp;quot;acc&amp;quot;])&#xA;plt.plot(hist.history['val_acc'])&#xA;plt.plot(hist.history['loss'])&#xA;plt.plot(hist.history['val_loss'])&#xA;plt.title(&amp;quot;model accuracy&amp;quot;)&#xA;plt.ylabel(&amp;quot;Accuracy&amp;quot;)&#xA;plt.xlabel(&amp;quot;Epoch&amp;quot;)&#xA;plt.legend([&amp;quot;Accuracy&amp;quot;, &amp;quot;Validation Accuracy&amp;quot;, &amp;quot;loss&amp;quot;, &amp;quot;Validation Loss&amp;quot;])&#xA;plt.show(block=True)&#xA;&#xA;# Try on test data&#xA;img = image.load_img(&amp;quot;C:/Temp/Datasets/CatsDogs/validation/39.jpg&amp;quot;, target_size=(224, 224))&#xA;img = np.asarray(img)&#xA;plt.imshow(img)&#xA;img = np.expand_dims(img, axis=0)&#xA;saved_model = load_model(&amp;quot;vgg16_1.h5&amp;quot;)&#xA;output = saved_model.predict(img)&#xA;if output[0][0] &amp;gt; output[0][1]:&#xA;    print(&amp;quot;cat&amp;quot;)&#xA;else:&#xA;    print('dog')&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      ValueError: Asked to retrieve element 0, but the Sequence has length 0.&#xA;         ? ,   .    .&#xA; tensorflow 2.7.0,  keras 2.7.0.&#xA;      :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):  &#xA;  File &amp;quot;D:/Code/Python/test/test10.py&amp;quot;, line 94, in &amp;lt;module&amp;gt;&#xA;    callbacks=[checkpoint, earlystop])  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-packages\keras\engine\training.py&amp;quot;, line 2030, in fit_generator&#xA;    initial_epoch=initial_epoch)  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler &#xA;    raise e.with_traceback(filtered_tb) from None  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras_preprocessing\image\iterator.py&amp;quot;, line 57, in __getitem__ length=len(self)))  &#xA;ValueError: Asked to retrieve element 0, but the Sequence has length 0  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""394072"" LastEditorUserId=""260769"" LastEditDate=""2022-01-12T14:40:54.727"" LastActivityDate=""2022-01-12T14:40:54.727"" Title="" ValueError    VGG16"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1370777"" PostTypeId=""1"" CreationDate=""2022-01-12T11:44:16.177"" Score=""0"" ViewCount=""43"" Body=""&lt;p&gt;   &#xA;    VGG16   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;&#xA;VGG-16 Implementation on Cats&amp;amp;Dogs Dataset&#xA;&amp;quot;&amp;quot;&amp;quot;&#xA;&#xA;import keras&#xA;import matplotlib.pyplot as plt&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from keras.backend import set_session&#xA;from keras.callbacks import ModelCheckpoint, EarlyStopping&#xA;from keras.layers import Conv2D, Dense, Flatten, MaxPool2D&#xA;from keras.models import Sequential, load_model&#xA;from keras.optimizers import adam_v2&#xA;from keras.preprocessing import image&#xA;from keras.preprocessing.image import ImageDataGenerator&#xA;&#xA;config = tf.compat.v1.ConfigProto()&#xA;config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU&#xA;config.log_device_placement = True  # to log device placement (on which device the operation ran)&#xA;sess = tf.compat.v1.Session(config=config)&#xA;set_session(sess)&#xA;&#xA;# Get the data&#xA;trdata = ImageDataGenerator()&#xA;traindata = trdata.flow_from_directory(directory=&amp;quot;C:/Temp/Datasets/CatsDogs/train&amp;quot;, target_size=(224, 224))&#xA;tsdata = ImageDataGenerator()&#xA;testdata = tsdata.flow_from_directory(directory='C:/Temp/Datasets/CatsDogs/validation', target_size=(224, 224))&#xA;&#xA;# Generate the model&#xA;model = Sequential()&#xA;# Layer 1: Convolutional&#xA;model.add(Conv2D(input_shape=(224, 224, 3), filters=64, kernel_size=(3, 3),&#xA;                 padding='same', activation='relu'))&#xA;# Layer 2: Convolutional&#xA;model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 3: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 4: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 5: Convolutional&#xA;model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 6: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 7: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 8: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 9: Convolutional&#xA;model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 10: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 11: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 12: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 13: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 14: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 15: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 16: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 17: Convolutional&#xA;model.add(Conv2D(filters=512, kernel_size=(3, 3), padding='same', activation='relu'))&#xA;# Layer 18: MaxPooling&#xA;model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))&#xA;&#xA;# Layer 19: Flatten&#xA;model.add(Flatten())&#xA;# Layer 20: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 21: Fully Connected Layer&#xA;model.add(Dense(units=4096, activation='relu'))&#xA;# Layer 22: Softmax Layer&#xA;model.add(Dense(units=2, activation='softmax'))&#xA;&#xA;# Add Optimizer and check accuracy metrics&#xA;optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.001)&#xA;model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy,&#xA;              metrics=['accuracy'])&#xA;# Check model summary&#xA;print(model.summary())&#xA;&#xA;checkpoint = ModelCheckpoint(&amp;quot;vgg16_1.h5&amp;quot;, monitor='val_acc', verbose=1, save_best_only=True,&#xA;                             save_weights_only=False, mode='auto', period=1)&#xA;earlystop = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')&#xA;hist = model.fit_generator(steps_per_epoch=100, generator=traindata, validation_data=testdata,&#xA;                           validation_steps=10, epochs=100,&#xA;                           callbacks=[checkpoint, earlystop])&#xA;&#xA;plt.plot(hist.history[&amp;quot;acc&amp;quot;])&#xA;plt.plot(hist.history['val_acc'])&#xA;plt.plot(hist.history['loss'])&#xA;plt.plot(hist.history['val_loss'])&#xA;plt.title(&amp;quot;model accuracy&amp;quot;)&#xA;plt.ylabel(&amp;quot;Accuracy&amp;quot;)&#xA;plt.xlabel(&amp;quot;Epoch&amp;quot;)&#xA;plt.legend([&amp;quot;Accuracy&amp;quot;, &amp;quot;Validation Accuracy&amp;quot;, &amp;quot;loss&amp;quot;, &amp;quot;Validation Loss&amp;quot;])&#xA;plt.show(block=True)&#xA;&#xA;# Try on test data&#xA;img = image.load_img(&amp;quot;C:/Temp/Datasets/CatsDogs/validation/39.jpg&amp;quot;, target_size=(224, 224))&#xA;img = np.asarray(img)&#xA;plt.imshow(img)&#xA;img = np.expand_dims(img, axis=0)&#xA;saved_model = load_model(&amp;quot;vgg16_1.h5&amp;quot;)&#xA;output = saved_model.predict(img)&#xA;if output[0][0] &amp;gt; output[0][1]:&#xA;    print(&amp;quot;cat&amp;quot;)&#xA;else:&#xA;    print('dog')&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      ValueError: Asked to retrieve element 0, but the Sequence has length 0.&#xA;         ? ,   .    .&#xA; tensorflow 2.7.0,  keras 2.7.0.&#xA;      :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):  &#xA;  File &amp;quot;D:/Code/Python/test/test10.py&amp;quot;, line 94, in &amp;lt;module&amp;gt;&#xA;    callbacks=[checkpoint, earlystop])  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-packages\keras\engine\training.py&amp;quot;, line 2030, in fit_generator&#xA;    initial_epoch=initial_epoch)  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler &#xA;    raise e.with_traceback(filtered_tb) from None  &#xA;  File &amp;quot;C:\Python\Python37\lib\site-&#xA; packages\keras_preprocessing\image\iterator.py&amp;quot;, line 57, in __getitem__ length=len(self)))  &#xA;ValueError: Asked to retrieve element 0, but the Sequence has length 0  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""394072"" LastEditorUserId=""260769"" LastEditDate=""2022-01-12T14:40:54.727"" LastActivityDate=""2022-01-12T14:40:54.727"" Title="" ValueError    VGG16"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
F:\stackexchange\extracted\ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
