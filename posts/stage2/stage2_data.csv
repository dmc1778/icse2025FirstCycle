/media/nima/SSD/stackexchange/extracted/bioinformatics.stackexchange.com,"  <row Id=""19778"" PostTypeId=""1"" CreationDate=""2022-10-03T18:43:08.530"" Score=""1"" ViewCount=""396"" Body=""&lt;p&gt;I have come across some related issues pertaining to this tensorflow warning but none related specifically to deepbgc&lt;/p&gt;&#xA;&lt;p&gt;I have created a conda environment and have installed onto it the bioinformatics analysis tool deepbgc. Now the program runs but it does output some warning messages related to the tensorflow package. When I run the &amp;quot;deepbgc info&amp;quot; command through terminal, I get the following output messages related to the tensor flow package:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Could this be related to the version of tensorflow? Currently, I have version 1.15.4 installed onto the conda environment. Are these error messages significant and could they impact the data running through deepbgc? Or can they mostly be ignored? Any guidance appreciated, thanks!&lt;/p&gt;&#xA;"" OwnerUserId=""16173"" LastActivityDate=""2022-10-05T21:14:43.193"" Title=""Warning message related to tensorflow package when trying to run deepbgc bioinformatics tool"" Tags=""&lt;python&gt;&lt;deeptools&gt;&lt;conda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/bioinformatics.stackexchange.com,"  <row Id=""19778"" PostTypeId=""1"" CreationDate=""2022-10-03T18:43:08.530"" Score=""1"" ViewCount=""396"" Body=""&lt;p&gt;I have come across some related issues pertaining to this tensorflow warning but none related specifically to deepbgc&lt;/p&gt;&#xA;&lt;p&gt;I have created a conda environment and have installed onto it the bioinformatics analysis tool deepbgc. Now the program runs but it does output some warning messages related to the tensorflow package. When I run the &amp;quot;deepbgc info&amp;quot; command through terminal, I get the following output messages related to the tensor flow package:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Could this be related to the version of tensorflow? Currently, I have version 1.15.4 installed onto the conda environment. Are these error messages significant and could they impact the data running through deepbgc? Or can they mostly be ignored? Any guidance appreciated, thanks!&lt;/p&gt;&#xA;"" OwnerUserId=""16173"" LastActivityDate=""2022-10-05T21:14:43.193"" Title=""Warning message related to tensorflow package when trying to run deepbgc bioinformatics tool"" Tags=""&lt;python&gt;&lt;deeptools&gt;&lt;conda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/bioinformatics.stackexchange.com,"  <row Id=""19778"" PostTypeId=""1"" CreationDate=""2022-10-03T18:43:08.530"" Score=""1"" ViewCount=""396"" Body=""&lt;p&gt;I have come across some related issues pertaining to this tensorflow warning but none related specifically to deepbgc&lt;/p&gt;&#xA;&lt;p&gt;I have created a conda environment and have installed onto it the bioinformatics analysis tool deepbgc. Now the program runs but it does output some warning messages related to the tensorflow package. When I run the &amp;quot;deepbgc info&amp;quot; command through terminal, I get the following output messages related to the tensor flow package:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Could this be related to the version of tensorflow? Currently, I have version 1.15.4 installed onto the conda environment. Are these error messages significant and could they impact the data running through deepbgc? Or can they mostly be ignored? Any guidance appreciated, thanks!&lt;/p&gt;&#xA;"" OwnerUserId=""16173"" LastActivityDate=""2022-10-05T21:14:43.193"" Title=""Warning message related to tensorflow package when trying to run deepbgc bioinformatics tool"" Tags=""&lt;python&gt;&lt;deeptools&gt;&lt;conda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/bioinformatics.stackexchange.com,"  <row Id=""19778"" PostTypeId=""1"" CreationDate=""2022-10-03T18:43:08.530"" Score=""1"" ViewCount=""396"" Body=""&lt;p&gt;I have come across some related issues pertaining to this tensorflow warning but none related specifically to deepbgc&lt;/p&gt;&#xA;&lt;p&gt;I have created a conda environment and have installed onto it the bioinformatics analysis tool deepbgc. Now the program runs but it does output some warning messages related to the tensorflow package. When I run the &amp;quot;deepbgc info&amp;quot; command through terminal, I get the following output messages related to the tensor flow package:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.&#xA;&#xA;WARNING 03/10 12:55:05   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.&#xA;&#xA;WARNING 03/10 12:55:06   From /EFS/tools/miniconda/envs/deepbgc2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Could this be related to the version of tensorflow? Currently, I have version 1.15.4 installed onto the conda environment. Are these error messages significant and could they impact the data running through deepbgc? Or can they mostly be ignored? Any guidance appreciated, thanks!&lt;/p&gt;&#xA;"" OwnerUserId=""16173"" LastActivityDate=""2022-10-05T21:14:43.193"" Title=""Warning message related to tensorflow package when trying to run deepbgc bioinformatics tool"" Tags=""&lt;python&gt;&lt;deeptools&gt;&lt;conda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/blender.stackexchange.com,"  <row Id=""273796"" PostTypeId=""1"" CreationDate=""2022-09-04T20:20:09.403"" Score=""1"" ViewCount=""331"" Body=""&lt;p&gt;I have this script that works on Blender 3.1, to convert a directory with fbx files to glb.&#xA;I'm trying to using it in Blender 3.2.2, but the scripting is failling with:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;AttributeError: 'NoneType' object has no attribute 'scene'&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This error happens on the line were the glb file is being exported. But I could not understand why, if its a missing parameters, or something that I need to set in the context&lt;/p&gt;&#xA;&lt;p&gt;The script is bellow&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;CONVERT_DIR = &amp;quot;/DIR&amp;quot;&#xA;&#xA;import os&#xA;&#xA;def file_iter(path, ext):&#xA;    for dirpath, dirnames, filenames in os.walk(path):&#xA;        for filename in filenames:&#xA;            ext = os.path.splitext(filename)[1]&#xA;            if ext.lower().endswith(ext):&#xA;                yield os.path.join(dirpath, filename)&#xA;&#xA;import bpy&#xA;&#xA;&#xA;def reset_blend():&#xA;    bpy.ops.wm.read_factory_settings(use_empty=True)&#xA;&#xA;def convert_recursive(base_path):&#xA;    for filepath_src in file_iter(base_path, &amp;quot;.fbx&amp;quot;):&#xA;        filepath_dst = os.path.splitext(filepath_src)[0] + &amp;quot;.glb&amp;quot;&#xA;&#xA;        print(&amp;quot;Converting %r -&amp;gt; %r&amp;quot; % (filepath_src, filepath_dst))&#xA;&#xA;        reset_blend()&#xA;&#xA;        bpy.ops.import_scene.fbx(filepath=filepath_src)&#xA;&#xA;        ctx = bpy.context.copy()&#xA;        &#xA;        ctx['active_object'] = None&#xA;&#xA;        bpy.ops.export_scene.gltf(ctx,export_format=&amp;quot;GLB&amp;quot;,filepath=filepath_dst)&#xA;&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    convert_recursive(CONVERT_DIR)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The stacktrace:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;/Text:32: DeprecationWarning: Passing in context overrides is deprecated in favor of Context.temp_override(..), calling &amp;quot;export_scene.gltf&amp;quot;&#xA;'/app/blender/3.2/python/lib/python3.10/site-packages/libextern_draco.so' exists, draco mesh compression is available&#xA;20:11:52 | INFO: Starting glTF 2.0 export&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gatheError: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/Text&amp;quot;, line 36, in &amp;lt;module&amp;gt;&#xA;  File &amp;quot;/Text&amp;quot;, line 32, in convert_recursive&#xA;  File &amp;quot;/app/blender/3.2/scripts/modules/bpy/ops.py&amp;quot;, line 113, in __call__&#xA;    ret = _op_call(self.idname_py(), C_dict, kw, C_exec, C_undo)&#xA;RuntimeError: Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Location: /app/blender/3.2/scripts/modules/bpy/ops.py:113&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""110202"" LastActivityDate=""2022-09-04T20:20:09.403"" Title=""Script to export gltf fails with &quot;AttributeError: 'NoneType' object has no attribute 'scene'&quot;"" Tags=""&lt;python&gt;&lt;scripting&gt;&lt;gltf&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/blender.stackexchange.com,"  <row Id=""273796"" PostTypeId=""1"" CreationDate=""2022-09-04T20:20:09.403"" Score=""1"" ViewCount=""331"" Body=""&lt;p&gt;I have this script that works on Blender 3.1, to convert a directory with fbx files to glb.&#xA;I'm trying to using it in Blender 3.2.2, but the scripting is failling with:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;AttributeError: 'NoneType' object has no attribute 'scene'&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This error happens on the line were the glb file is being exported. But I could not understand why, if its a missing parameters, or something that I need to set in the context&lt;/p&gt;&#xA;&lt;p&gt;The script is bellow&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;CONVERT_DIR = &amp;quot;/DIR&amp;quot;&#xA;&#xA;import os&#xA;&#xA;def file_iter(path, ext):&#xA;    for dirpath, dirnames, filenames in os.walk(path):&#xA;        for filename in filenames:&#xA;            ext = os.path.splitext(filename)[1]&#xA;            if ext.lower().endswith(ext):&#xA;                yield os.path.join(dirpath, filename)&#xA;&#xA;import bpy&#xA;&#xA;&#xA;def reset_blend():&#xA;    bpy.ops.wm.read_factory_settings(use_empty=True)&#xA;&#xA;def convert_recursive(base_path):&#xA;    for filepath_src in file_iter(base_path, &amp;quot;.fbx&amp;quot;):&#xA;        filepath_dst = os.path.splitext(filepath_src)[0] + &amp;quot;.glb&amp;quot;&#xA;&#xA;        print(&amp;quot;Converting %r -&amp;gt; %r&amp;quot; % (filepath_src, filepath_dst))&#xA;&#xA;        reset_blend()&#xA;&#xA;        bpy.ops.import_scene.fbx(filepath=filepath_src)&#xA;&#xA;        ctx = bpy.context.copy()&#xA;        &#xA;        ctx['active_object'] = None&#xA;&#xA;        bpy.ops.export_scene.gltf(ctx,export_format=&amp;quot;GLB&amp;quot;,filepath=filepath_dst)&#xA;&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    convert_recursive(CONVERT_DIR)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The stacktrace:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;/Text:32: DeprecationWarning: Passing in context overrides is deprecated in favor of Context.temp_override(..), calling &amp;quot;export_scene.gltf&amp;quot;&#xA;'/app/blender/3.2/python/lib/python3.10/site-packages/libextern_draco.so' exists, draco mesh compression is available&#xA;20:11:52 | INFO: Starting glTF 2.0 export&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gatheError: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/Text&amp;quot;, line 36, in &amp;lt;module&amp;gt;&#xA;  File &amp;quot;/Text&amp;quot;, line 32, in convert_recursive&#xA;  File &amp;quot;/app/blender/3.2/scripts/modules/bpy/ops.py&amp;quot;, line 113, in __call__&#xA;    ret = _op_call(self.idname_py(), C_dict, kw, C_exec, C_undo)&#xA;RuntimeError: Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Location: /app/blender/3.2/scripts/modules/bpy/ops.py:113&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""110202"" LastActivityDate=""2022-09-04T20:20:09.403"" Title=""Script to export gltf fails with &quot;AttributeError: 'NoneType' object has no attribute 'scene'&quot;"" Tags=""&lt;python&gt;&lt;scripting&gt;&lt;gltf&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/blender.stackexchange.com,"  <row Id=""273796"" PostTypeId=""1"" CreationDate=""2022-09-04T20:20:09.403"" Score=""1"" ViewCount=""331"" Body=""&lt;p&gt;I have this script that works on Blender 3.1, to convert a directory with fbx files to glb.&#xA;I'm trying to using it in Blender 3.2.2, but the scripting is failling with:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;AttributeError: 'NoneType' object has no attribute 'scene'&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This error happens on the line were the glb file is being exported. But I could not understand why, if its a missing parameters, or something that I need to set in the context&lt;/p&gt;&#xA;&lt;p&gt;The script is bellow&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;CONVERT_DIR = &amp;quot;/DIR&amp;quot;&#xA;&#xA;import os&#xA;&#xA;def file_iter(path, ext):&#xA;    for dirpath, dirnames, filenames in os.walk(path):&#xA;        for filename in filenames:&#xA;            ext = os.path.splitext(filename)[1]&#xA;            if ext.lower().endswith(ext):&#xA;                yield os.path.join(dirpath, filename)&#xA;&#xA;import bpy&#xA;&#xA;&#xA;def reset_blend():&#xA;    bpy.ops.wm.read_factory_settings(use_empty=True)&#xA;&#xA;def convert_recursive(base_path):&#xA;    for filepath_src in file_iter(base_path, &amp;quot;.fbx&amp;quot;):&#xA;        filepath_dst = os.path.splitext(filepath_src)[0] + &amp;quot;.glb&amp;quot;&#xA;&#xA;        print(&amp;quot;Converting %r -&amp;gt; %r&amp;quot; % (filepath_src, filepath_dst))&#xA;&#xA;        reset_blend()&#xA;&#xA;        bpy.ops.import_scene.fbx(filepath=filepath_src)&#xA;&#xA;        ctx = bpy.context.copy()&#xA;        &#xA;        ctx['active_object'] = None&#xA;&#xA;        bpy.ops.export_scene.gltf(ctx,export_format=&amp;quot;GLB&amp;quot;,filepath=filepath_dst)&#xA;&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    convert_recursive(CONVERT_DIR)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The stacktrace:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;/Text:32: DeprecationWarning: Passing in context overrides is deprecated in favor of Context.temp_override(..), calling &amp;quot;export_scene.gltf&amp;quot;&#xA;'/app/blender/3.2/python/lib/python3.10/site-packages/libextern_draco.so' exists, draco mesh compression is available&#xA;20:11:52 | INFO: Starting glTF 2.0 export&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gatheError: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/Text&amp;quot;, line 36, in &amp;lt;module&amp;gt;&#xA;  File &amp;quot;/Text&amp;quot;, line 32, in convert_recursive&#xA;  File &amp;quot;/app/blender/3.2/scripts/modules/bpy/ops.py&amp;quot;, line 113, in __call__&#xA;    ret = _op_call(self.idname_py(), C_dict, kw, C_exec, C_undo)&#xA;RuntimeError: Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Location: /app/blender/3.2/scripts/modules/bpy/ops.py:113&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""110202"" LastActivityDate=""2022-09-04T20:20:09.403"" Title=""Script to export gltf fails with &quot;AttributeError: 'NoneType' object has no attribute 'scene'&quot;"" Tags=""&lt;python&gt;&lt;scripting&gt;&lt;gltf&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/blender.stackexchange.com,"  <row Id=""273796"" PostTypeId=""1"" CreationDate=""2022-09-04T20:20:09.403"" Score=""1"" ViewCount=""331"" Body=""&lt;p&gt;I have this script that works on Blender 3.1, to convert a directory with fbx files to glb.&#xA;I'm trying to using it in Blender 3.2.2, but the scripting is failling with:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;quot;AttributeError: 'NoneType' object has no attribute 'scene'&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This error happens on the line were the glb file is being exported. But I could not understand why, if its a missing parameters, or something that I need to set in the context&lt;/p&gt;&#xA;&lt;p&gt;The script is bellow&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;CONVERT_DIR = &amp;quot;/DIR&amp;quot;&#xA;&#xA;import os&#xA;&#xA;def file_iter(path, ext):&#xA;    for dirpath, dirnames, filenames in os.walk(path):&#xA;        for filename in filenames:&#xA;            ext = os.path.splitext(filename)[1]&#xA;            if ext.lower().endswith(ext):&#xA;                yield os.path.join(dirpath, filename)&#xA;&#xA;import bpy&#xA;&#xA;&#xA;def reset_blend():&#xA;    bpy.ops.wm.read_factory_settings(use_empty=True)&#xA;&#xA;def convert_recursive(base_path):&#xA;    for filepath_src in file_iter(base_path, &amp;quot;.fbx&amp;quot;):&#xA;        filepath_dst = os.path.splitext(filepath_src)[0] + &amp;quot;.glb&amp;quot;&#xA;&#xA;        print(&amp;quot;Converting %r -&amp;gt; %r&amp;quot; % (filepath_src, filepath_dst))&#xA;&#xA;        reset_blend()&#xA;&#xA;        bpy.ops.import_scene.fbx(filepath=filepath_src)&#xA;&#xA;        ctx = bpy.context.copy()&#xA;        &#xA;        ctx['active_object'] = None&#xA;&#xA;        bpy.ops.export_scene.gltf(ctx,export_format=&amp;quot;GLB&amp;quot;,filepath=filepath_dst)&#xA;&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    convert_recursive(CONVERT_DIR)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;The stacktrace:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;/Text:32: DeprecationWarning: Passing in context overrides is deprecated in favor of Context.temp_override(..), calling &amp;quot;export_scene.gltf&amp;quot;&#xA;'/app/blender/3.2/python/lib/python3.10/site-packages/libextern_draco.so' exists, draco mesh compression is available&#xA;20:11:52 | INFO: Starting glTF 2.0 export&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gatheError: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/Text&amp;quot;, line 36, in &amp;lt;module&amp;gt;&#xA;  File &amp;quot;/Text&amp;quot;, line 32, in convert_recursive&#xA;  File &amp;quot;/app/blender/3.2/scripts/modules/bpy/ops.py&amp;quot;, line 113, in __call__&#xA;    ret = _op_call(self.idname_py(), C_dict, kw, C_exec, C_undo)&#xA;RuntimeError: Error: Python: Traceback (most recent call last):&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/__init__.py&amp;quot;, line 628, in execute&#xA;    return gltf2_blender_export.save(context, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 35, in save&#xA;    json, buffer = __export(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 52, in __export&#xA;    __gather_gltf(exporter, export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_export.py&amp;quot;, line 66, in __gather_gltf&#xA;    active_scene_idx, scenes, animations = gltf2_blender_gather.gather_gltf2(export_settings)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 30, in gather_gltf2&#xA;    scenes.append(__gather_scene(blender_scene, export_settings))&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_cache.py&amp;quot;, line 38, in wrapper_cached&#xA;    result = func(*args, **kwargs)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather.py&amp;quot;, line 54, in __gather_scene&#xA;    vtree.construct(blender_scene)&#xA;  File &amp;quot;/app/blender/3.2/scripts/addons/io_scene_gltf2/blender/exp/gltf2_blender_gather_tree.py&amp;quot;, line 95, in construct&#xA;    bpy.context.window.scene = blender_scene&#xA;AttributeError: 'NoneType' object has no attribute 'scene'&#xA;Location: /app/blender/3.2/scripts/modules/bpy/ops.py:113&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""110202"" LastActivityDate=""2022-09-04T20:20:09.403"" Title=""Script to export gltf fails with &quot;AttributeError: 'NoneType' object has no attribute 'scene'&quot;"" Tags=""&lt;python&gt;&lt;scripting&gt;&lt;gltf&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/codereview.stackexchange.com,"  <row Id=""263299"" PostTypeId=""1"" CreationDate=""2021-06-22T00:34:28.010"" Score=""2"" ViewCount=""418"" Body=""&lt;p&gt;I use the following code in order to assess the quality of an audio, which is based on this &lt;a href=&quot;https://github.com/lochenchou/MOSNet&quot; rel=&quot;nofollow noreferrer&quot;&gt;original-project: MOSNet&lt;/a&gt;. I call &lt;code&gt;compute_mosnet_score()&lt;/code&gt; in a loop with a different audio file in each iteration.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;import scipy&#xA;import librosa&#xA;import numpy as np&#xA;import tensorflow as tf&#xA;from tensorflow import keras&#xA;from tensorflow.keras import Model, layers&#xA;from tensorflow.keras.constraints import max_norm&#xA;from tensorflow.keras.layers import Dense, Dropout, Conv2D&#xA;from tensorflow.keras.layers import LSTM, TimeDistributed, Bidirectional&#xA;&#xA;&#xA;class MOSNet():&#xA;    def __init__(self, window, wave_handler, hop=None):&#xA;        # init&#xA;        self.wave_handler = wave_handler&#xA;&#xA;        # constants&#xA;        self.fixed_rate = 16000&#xA;        self.mono       = True&#xA;        self.absolute   = True&#xA;        self.FFT_SIZE   = 512&#xA;        self.SGRAM_DIM  = self.FFT_SIZE // 2 + 1&#xA;        self.HOP_LENGTH = 256&#xA;        self.WIN_LENGTH = 512&#xA;&#xA;        _input = keras.Input(shape=(None, 257))&#xA;        re_input = layers.Reshape((-1, 257, 1), input_shape=(-1, 257))(_input)&#xA;&#xA;        # CNN&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(re_input)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv1 = (Conv2D(16, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv1)&#xA;&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv1)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv2 = (Conv2D(32, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv2)&#xA;&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv2)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv3 = (Conv2D(64, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv3)&#xA;&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv3)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))(conv4)&#xA;        conv4 = (Conv2D(128, (3, 3), strides=(1, 3), activation='relu', padding='same'))(conv4)&#xA;&#xA;        re_shape = layers.Reshape((-1, 4 * 128), input_shape=(-1, 4, 128))(conv4)&#xA;&#xA;        # BLSTM&#xA;        blstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3,&#xA;                                    recurrent_dropout=0.3,&#xA;                                    recurrent_constraint=max_norm(0.00001)),&#xA;                               merge_mode='concat')(re_shape)&#xA;&#xA;        # DNN&#xA;        flatten = TimeDistributed(layers.Flatten())(blstm1)&#xA;        dense1 = TimeDistributed(Dense(128, activation='relu'))(flatten)&#xA;        dense1 = Dropout(0.3)(dense1)&#xA;&#xA;        frame_score   = TimeDistributed(Dense(1), name='frame')(dense1)&#xA;        average_score = layers.GlobalAveragePooling1D(name='avg')(frame_score)&#xA;&#xA;        self.model = Model(outputs=[average_score, frame_score], inputs=_input)&#xA;&#xA;        # weights are in the directory of this file&#xA;        pre_trained_dir = &amp;quot;resources&amp;quot;&#xA;&#xA;        # load pre-trained weights. CNN_BLSTM is reported as best&#xA;        self.model.load_weights(os.path.join(pre_trained_dir, 'cnn_blstm.h5'))&#xA;&#xA;    def compute_mosnet_score(self, wav_file, fs):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Evaluate the audio quality using a MOSnet (neural network).&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # compute mosnet score&#xA;        sig4mosnet, sig4mosnetlen = self.wave_handler.ffmpeg_load_audio(wav_file, fs)&#xA;        mosnet_scores, avg_mos_score = self.predict_mos(sig4mosnet, self.fixed_rate)&#xA;        return avg_mos_score&#xA;&#xA;    def predict_wrapper(self, mag):&#xA;        res = self.model.predict(mag, verbose=0, batch_size=1, steps=1)&#xA;        return res&#xA;&#xA;    def get_mag(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Get signal magnitude.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        # stft. D: (1+n_fft//2, T) -&amp;gt; magnitude spectrogram&#xA;        mag = np.abs(librosa.stft(y=np.asfortranarray(audios),&#xA;                                  n_fft=self.FFT_SIZE,&#xA;                                  hop_length=self.HOP_LENGTH,&#xA;                                  win_length=self.WIN_LENGTH,&#xA;                                  window=scipy.signal.hamming))  # (1+n_fft/2, T)&#xA;&#xA;        # shape in (T, 1+n_fft/2) -&amp;gt; now call the actual MOSnet&#xA;        mag = np.transpose(mag.astype(np.float32))&#xA;        return mag[None, ...]&#xA;&#xA;    def predict_mos(self, audios, rate):&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        Predict the Mean Objective Score of a given frame.&#xA;        &amp;quot;&amp;quot;&amp;quot;&#xA;        mag = self.get_mag(audios, rate)&#xA;        res = self.predict_wrapper(mag)&#xA;        return res[1][0], round(res[0][0][0], 3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Since each iteration, the code computes the score using a different audio, it means the input size changes. This causes the following warning:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARNING:tensorflow:6 out of the last 12 calls to &amp;lt;function _make_execution_function.&amp;lt;locals&amp;gt;.distributed_function at 0x7f999459e170&amp;gt; triggered tf.function retracing. &#xA;Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing.&#xA;Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Unfortunately, this retracing is slowing down the code so is there any alternatives here that will disable retracing or accelerate the code?&lt;/strong&gt;&#xA;Obviously, I can use GPU accelerations, but those are not an option in my case.&#xA;I already tried to frame the audio signal and feed it to the prediction fucntion, which reduces the warnings but doesn't remove them all. However that is not practical since it add extra calls and slows the code down. I also tried &lt;code&gt;@tf.function&lt;/code&gt; and &lt;code&gt;@tf.function( experimental_relax_shapes=True)&lt;/code&gt; for &lt;code&gt;def predict_wrapper(self, mag)&lt;/code&gt; but it resulted in a &lt;code&gt;ValueError: in converted code&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;*I am using :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;tensorflow-base           2.1.0           &#xA;tensorflow-estimator      2.1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""189876"" LastActivityDate=""2021-06-22T00:34:28.010"" Title=""How to avoid retracing in tensorflow when predicting scores for inputs with different sizes"" Tags=""&lt;python&gt;&lt;performance&gt;&lt;tensorflow&gt;&lt;keras&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1179733"" PostTypeId=""1"" AcceptedAnswerId=""1179741"" CreationDate=""2019-10-09T13:29:22.460"" Score=""0"" ViewCount=""172"" Body=""&lt;blockquote&gt;&#xA;  &lt;p&gt;Environment : Ubuntu &lt;code&gt;16.04&lt;/code&gt; / tensorflow &lt;code&gt;1.14.0&lt;/code&gt;/ python &lt;code&gt;3.5.3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I installed TensorFlow using this command.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the result of it.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB)&#xA;Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (0.33.4)&#xA;Requirement already satisfied, skipping upgrade: protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (3.0.0b2)&#xA;Requirement already satisfied, skipping upgrade: six&amp;gt;=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1) (1.10.0)&#xA;Requirement already satisfied, skipping upgrade: numpy&amp;gt;=1.8.2 in /home/hanbit-o/.local/lib/python2.7/site-packages (from tensorflow==0.7.1) (1.16.4)&#xA;Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2-&amp;gt;tensorflow==0.7.1) (20.7.0)&#xA;Installing collected packages: tensorflow&#xA;  Found existing installation: tensorflow 0.7.1&#xA;    Uninstalling tensorflow-0.7.1:&#xA;      Successfully uninstalled tensorflow-0.7.1&#xA;Successfully installed tensorflow-0.7.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in this time there is a warning of python2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, I want to use TensorFlow at the python3&#xA;I think because of installing TensorFlow at python2.&#xA;There are lots of comments on the prompt.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;Python 3.5.3 (default, Aug 28 2019, 20:35:32) &#xA;[GCC 5.4.0 20160609] on linux&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tensorflow&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and i tried to ignore it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant('Hello, TensorFlow!')&#xA;&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()&#xA;2019-10-09 21:40:31.902027: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA&#xA;2019-10-09 21:40:31.926393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398000000 Hz&#xA;2019-10-09 21:40:31.929440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x42f31d0 executing computations on platform Host. Devices:&#xA;2019-10-09 21:40:31.929480: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &amp;lt;undefined&amp;gt;, &amp;lt;undefined&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(hello)&#xA;]b'Hello, TensorFlow!'&#xA;&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)&#xA;&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(a+b)&#xA;2019-10-09 21:41:28.143676: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.&#xA;42&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is works(?), and why there lots of warnning?&lt;/p&gt;&#xA;"" OwnerUserId=""1003835"" LastActivityDate=""2019-10-09T14:06:52.150"" Title=""Installing TensorFlow on ubuntu16.04 and lots of warnnings"" Tags=""&lt;software-installation&gt;&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1179733"" PostTypeId=""1"" AcceptedAnswerId=""1179741"" CreationDate=""2019-10-09T13:29:22.460"" Score=""0"" ViewCount=""172"" Body=""&lt;blockquote&gt;&#xA;  &lt;p&gt;Environment : Ubuntu &lt;code&gt;16.04&lt;/code&gt; / tensorflow &lt;code&gt;1.14.0&lt;/code&gt;/ python &lt;code&gt;3.5.3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I installed TensorFlow using this command.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the result of it.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB)&#xA;Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (0.33.4)&#xA;Requirement already satisfied, skipping upgrade: protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (3.0.0b2)&#xA;Requirement already satisfied, skipping upgrade: six&amp;gt;=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1) (1.10.0)&#xA;Requirement already satisfied, skipping upgrade: numpy&amp;gt;=1.8.2 in /home/hanbit-o/.local/lib/python2.7/site-packages (from tensorflow==0.7.1) (1.16.4)&#xA;Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2-&amp;gt;tensorflow==0.7.1) (20.7.0)&#xA;Installing collected packages: tensorflow&#xA;  Found existing installation: tensorflow 0.7.1&#xA;    Uninstalling tensorflow-0.7.1:&#xA;      Successfully uninstalled tensorflow-0.7.1&#xA;Successfully installed tensorflow-0.7.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in this time there is a warning of python2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, I want to use TensorFlow at the python3&#xA;I think because of installing TensorFlow at python2.&#xA;There are lots of comments on the prompt.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;Python 3.5.3 (default, Aug 28 2019, 20:35:32) &#xA;[GCC 5.4.0 20160609] on linux&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tensorflow&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and i tried to ignore it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant('Hello, TensorFlow!')&#xA;&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()&#xA;2019-10-09 21:40:31.902027: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA&#xA;2019-10-09 21:40:31.926393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398000000 Hz&#xA;2019-10-09 21:40:31.929440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x42f31d0 executing computations on platform Host. Devices:&#xA;2019-10-09 21:40:31.929480: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &amp;lt;undefined&amp;gt;, &amp;lt;undefined&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(hello)&#xA;]b'Hello, TensorFlow!'&#xA;&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)&#xA;&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(a+b)&#xA;2019-10-09 21:41:28.143676: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.&#xA;42&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is works(?), and why there lots of warnning?&lt;/p&gt;&#xA;"" OwnerUserId=""1003835"" LastActivityDate=""2019-10-09T14:06:52.150"" Title=""Installing TensorFlow on ubuntu16.04 and lots of warnnings"" Tags=""&lt;software-installation&gt;&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1179733"" PostTypeId=""1"" AcceptedAnswerId=""1179741"" CreationDate=""2019-10-09T13:29:22.460"" Score=""0"" ViewCount=""172"" Body=""&lt;blockquote&gt;&#xA;  &lt;p&gt;Environment : Ubuntu &lt;code&gt;16.04&lt;/code&gt; / tensorflow &lt;code&gt;1.14.0&lt;/code&gt;/ python &lt;code&gt;3.5.3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I installed TensorFlow using this command.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the result of it.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB)&#xA;Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (0.33.4)&#xA;Requirement already satisfied, skipping upgrade: protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (3.0.0b2)&#xA;Requirement already satisfied, skipping upgrade: six&amp;gt;=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1) (1.10.0)&#xA;Requirement already satisfied, skipping upgrade: numpy&amp;gt;=1.8.2 in /home/hanbit-o/.local/lib/python2.7/site-packages (from tensorflow==0.7.1) (1.16.4)&#xA;Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2-&amp;gt;tensorflow==0.7.1) (20.7.0)&#xA;Installing collected packages: tensorflow&#xA;  Found existing installation: tensorflow 0.7.1&#xA;    Uninstalling tensorflow-0.7.1:&#xA;      Successfully uninstalled tensorflow-0.7.1&#xA;Successfully installed tensorflow-0.7.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in this time there is a warning of python2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, I want to use TensorFlow at the python3&#xA;I think because of installing TensorFlow at python2.&#xA;There are lots of comments on the prompt.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;Python 3.5.3 (default, Aug 28 2019, 20:35:32) &#xA;[GCC 5.4.0 20160609] on linux&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tensorflow&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and i tried to ignore it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant('Hello, TensorFlow!')&#xA;&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()&#xA;2019-10-09 21:40:31.902027: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA&#xA;2019-10-09 21:40:31.926393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398000000 Hz&#xA;2019-10-09 21:40:31.929440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x42f31d0 executing computations on platform Host. Devices:&#xA;2019-10-09 21:40:31.929480: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &amp;lt;undefined&amp;gt;, &amp;lt;undefined&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(hello)&#xA;]b'Hello, TensorFlow!'&#xA;&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)&#xA;&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(a+b)&#xA;2019-10-09 21:41:28.143676: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.&#xA;42&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is works(?), and why there lots of warnning?&lt;/p&gt;&#xA;"" OwnerUserId=""1003835"" LastActivityDate=""2019-10-09T14:06:52.150"" Title=""Installing TensorFlow on ubuntu16.04 and lots of warnnings"" Tags=""&lt;software-installation&gt;&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1179733"" PostTypeId=""1"" AcceptedAnswerId=""1179741"" CreationDate=""2019-10-09T13:29:22.460"" Score=""0"" ViewCount=""172"" Body=""&lt;blockquote&gt;&#xA;  &lt;p&gt;Environment : Ubuntu &lt;code&gt;16.04&lt;/code&gt; / tensorflow &lt;code&gt;1.14.0&lt;/code&gt;/ python &lt;code&gt;3.5.3&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I installed TensorFlow using this command.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the result of it.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;WARNING: The directory '/home/hanbit-o/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.&#xA;Collecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl&#xA;  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB)&#xA;Requirement already satisfied, skipping upgrade: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (0.33.4)&#xA;Requirement already satisfied, skipping upgrade: protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.7.1) (3.0.0b2)&#xA;Requirement already satisfied, skipping upgrade: six&amp;gt;=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1) (1.10.0)&#xA;Requirement already satisfied, skipping upgrade: numpy&amp;gt;=1.8.2 in /home/hanbit-o/.local/lib/python2.7/site-packages (from tensorflow==0.7.1) (1.16.4)&#xA;Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2-&amp;gt;tensorflow==0.7.1) (20.7.0)&#xA;Installing collected packages: tensorflow&#xA;  Found existing installation: tensorflow 0.7.1&#xA;    Uninstalling tensorflow-0.7.1:&#xA;      Successfully uninstalled tensorflow-0.7.1&#xA;Successfully installed tensorflow-0.7.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in this time there is a warning of python2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, I want to use TensorFlow at the python3&#xA;I think because of installing TensorFlow at python2.&#xA;There are lots of comments on the prompt.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;Python 3.5.3 (default, Aug 28 2019, 20:35:32) &#xA;[GCC 5.4.0 20160609] on linux&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tensorflow&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;/home/hanbit-o/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and i tried to ignore it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hello = tf.constant('Hello, TensorFlow!')&#xA;&amp;gt;&amp;gt;&amp;gt; sess = tf.Session()&#xA;2019-10-09 21:40:31.902027: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA&#xA;2019-10-09 21:40:31.926393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3398000000 Hz&#xA;2019-10-09 21:40:31.929440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x42f31d0 executing computations on platform Host. Devices:&#xA;2019-10-09 21:40:31.929480: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &amp;lt;undefined&amp;gt;, &amp;lt;undefined&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(hello)&#xA;]b'Hello, TensorFlow!'&#xA;&amp;gt;&amp;gt;&amp;gt; a = tf.constant(10)&#xA;&amp;gt;&amp;gt;&amp;gt; b = tf.constant(32)&#xA;&amp;gt;&amp;gt;&amp;gt; sess.run(a+b)&#xA;2019-10-09 21:41:28.143676: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.&#xA;42&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is works(?), and why there lots of warnning?&lt;/p&gt;&#xA;"" OwnerUserId=""1003835"" LastActivityDate=""2019-10-09T14:06:52.150"" Title=""Installing TensorFlow on ubuntu16.04 and lots of warnnings"" Tags=""&lt;software-installation&gt;&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/askubuntu.com,"  <row Id=""1307693"" PostTypeId=""1"" AcceptedAnswerId=""1309397"" CreationDate=""2021-01-13T15:48:40.440"" Score=""1"" ViewCount=""1182"" Body=""&lt;p&gt;I'm facing a problem with installing Nvidia. I have tried many solutions but none of them is working. Even, I can not open the &lt;code&gt;Software Updater&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/W7imH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/W7imH.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#Python 3.7.4&#xA;#tensorflow-gpu 2.2.0&#xA;&#xA;import tensorflow as tf&#xA;&#xA;print(&amp;quot;Num GPUs Available: &amp;quot;, len(tf.config.experimental.list_physical_devices('GPU')))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Num GPUs Available:  0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;$lspci | grep -i --color 'vga\|3d\|2d'&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$nvidia-smi&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;NVIDIA: could not open the device file /dev/nvidiactl (No such file or directory).&#xA;NVIDIA-SMI has failed because it couldn't communicate with NVIDIA driver. Make sure that latest NVIDIA driver is installed and running.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is a list of all the problems that I face when I'm trying to install Nvidia and Cuda on Ubuntu 16.04.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo dpkg --configure -a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dpkg: error processing package python3-apt (--configure):  package is&#xA;&amp;gt; in a very bad inconsistent state; you should  reinstall it before&#xA;&amp;gt; attempting configuration Errors were encountered while processing: &#xA;&amp;gt; `python3-apt`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;code&gt;$sudo apt-get remove package*&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'packagekit-dbg' for glob 'package*'&#xA;Note, selecting 'packagekit-gtk3-module' for glob 'package*'&#xA;Note, selecting 'packagekit-offline-update' for glob 'package*'&#xA;Note, selecting 'packagekit-system-interface' for glob 'package*'&#xA;Note, selecting 'packagekit-tools' for glob 'package*'&#xA;Note, selecting 'packagekit-gnome' for glob 'package*'&#xA;Note, selecting 'packagekit-docs' for glob 'package*'&#xA;Note, selecting 'packagesearch' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-aptcc' for glob 'package*'&#xA;Note, selecting 'packagekit' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-smart' for glob 'package*'&#xA;Note, selecting 'packagekit-plugin-click' for glob 'package*'&#xA;Note, selecting 'packagekit-backend-apt' for glob 'package*'&#xA;Package 'packagekit-backend-apt' is not installed, so not removed&#xA;Package 'packagekit-offline-update' is not installed, so not removed&#xA;Package 'packagekit-gnome' is not installed, so not removed&#xA;Package 'packagesearch' is not installed, so not removed&#xA;Package 'packagekit' is not installed, so not removed&#xA;Package 'packagekit-backend-aptcc' is not installed, so not removed&#xA;Package 'packagekit-docs' is not installed, so not removed&#xA;Package 'packagekit-tools' is not installed, so not removed&#xA;Package 'packagekit-backend-smart' is not installed, so not removed&#xA;Package 'packagekit-dbg' is not installed, so not removed&#xA;Package 'packagekit-gtk3-module' is not installed, so not removed&#xA;Package 'packagekit-plugin-click' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-settings (&amp;gt;= 384.81) but 361.42-0ubuntu1 is to be installed&#xA; nvidia-304 : Conflicts: xorg-driver-binary&#xA;              Recommends: libcuda1-304 but it is not going to be installed&#xA;              Recommends: nvidia-opencl-icd-304 but it is not going to be installed&#xA; nvidia-384 : Conflicts: xorg-driver-binary&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;ipc@ipc-System-Product-Name:~$ apt --fix-broken install&#xA;E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)&#xA;E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?&#xA;ipc@ipc-System-Product-Name:~$ sudo apt --fix-broken install&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Correcting dependencies... Done&#xA;The following packages were automatically installed and are no longer required:&#xA;  libxapian-1.3-5 libxapian-dev python3-xapian1.3 xapian-doc xapian-examples&#xA;Use 'sudo apt autoremove' to remove them.&#xA;The following additional packages will be installed:&#xA;  libnvidia-compute-460 python3-apt update-notifier-common&#xA;Suggested packages:&#xA;  python3-apt-dbg python-apt-doc&#xA;The following packages will be REMOVED:&#xA;  cuda-9-0 cuda-demo-suite-9-0 cuda-drivers cuda-runtime-9-0 libcuda1-384&#xA;  nvidia-384 nvidia-384-dev nvidia-opencl-icd-384&#xA;The following NEW packages will be installed:&#xA;  libnvidia-compute-460 update-notifier-common&#xA;The following packages will be upgraded:&#xA;  python3-apt&#xA;1 upgraded, 2 newly installed, 8 to remove and 27 not upgraded.&#xA;3 not fully installed or removed.&#xA;Need to get 0 B/22.1 MB of archives.&#xA;After this operation, 268 MB disk space will be freed.&#xA;Do you want to continue? [Y/n] y&#xA;...&#xA;dpkg: warning: files list file for package 'libxcb-sync-dev:amd64' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'ubuntu-standard' missing; assuming package has no files currently installed&#xA;dpkg: warning: files list file for package 'nvidia-opencl-icd-384' missing; assuming package has no files currently installed&#xA;(Reading database ... 618606 files and directories currently installed.)&#xA;Preparing to unpack .../python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb ...&#xA;/var/lib/dpkg/info/python3-apt.prerm: 6: /var/lib/dpkg/info/python3-apt.prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: warning: subprocess old pre-removal script returned error exit status 2&#xA;dpkg: trying script from the new package instead ...&#xA;/var/lib/dpkg/tmp.ci/prerm: 6: /var/lib/dpkg/tmp.ci/prerm: py3clean: Too many levels of symbolic links&#xA;dpkg: error processing archive /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb (--unpack):&#xA; subprocess new pre-removal script returned error exit status 2&#xA;/var/lib/dpkg/info/python3-apt.postinst: 6: /var/lib/dpkg/info/python3-apt.postinst: py3compile: Too many levels of symbolic links&#xA;dpkg: error while cleaning up:&#xA; subprocess installed post-installation script returned error exit status 2&#xA;Errors were encountered while processing:&#xA; /var/cache/apt/archives/python3-apt_1.1.0~beta1ubuntu0.16.04.11_amd64.deb&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ ^C&#xA;ipc@ipc-System-Product-Name:~$ clear&#xA;[3;J&#xA;ipc@ipc-System-Product-Name:~$ sudo dpkg --configure -a&#xA;dpkg: error processing package python3-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;Errors were encountered while processing:&#xA; python3-apt&#xA;ipc@ipc-System-Product-Name:~$ sudo apt-get purge nvidia*&#xA;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;Note, selecting 'nvidia-325-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-glx' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-common-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-modprobe' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-texture-tools' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-diagnostic' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-legacy-340xx-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-686-pae' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-vdpau-driver' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-smi' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-prime' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-dkms' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-nsight' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-common' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-amd64' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-no-dkms-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-compute-utils-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-toolkit' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-fabricmanager-dev-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-visual-profiler' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-persistenced' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-current-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings-binary' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-486' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-uvm' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-304-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-headless-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-driver-local-repo-ubuntu1604-440.33.01' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cg-doc' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-340-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-libopencl1-361-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-cuda-gdb' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-experimental-304-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343-updates' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-304' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-310' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-313' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-319' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-325' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-331' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-334' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-337' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-340' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-343' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-349' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-352' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-355' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-361' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-367' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-375' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-384' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-dkms-kernel' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-390' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-410' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-418' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-346-updates-dev' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-430' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-450' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-455' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-kernel-source-460' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-440' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-opencl-icd' for glob 'nvidia*'&#xA;Note, selecting 'nvidia-settings' instead of 'nvidia-settings-binary'&#xA;Package 'nvidia-libopencl1-dev' is not installed, so not removed&#xA;Package 'nvidia-libopencl1' is not installed, so not removed&#xA;Package 'nvidia-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-legacy-340xx-vdpau-driver' is not installed, so not removed&#xA;Package 'nvidia-driver' is not installed, so not removed&#xA;Package 'nvidia-glx' is not installed, so not removed&#xA;&#xA;Package 'nvidia-334' is not installed, so not removed&#xA;Package 'nvidia-334-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-334' is not installed, so not removed&#xA;Package 'nvidia-337' is not installed, so not removed&#xA;Package 'nvidia-337-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-337' is not installed, so not removed&#xA;Package 'nvidia-experimental-340' is not installed, so not removed&#xA;Package 'nvidia-343' is not installed, so not removed&#xA;Package 'nvidia-343-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-343' is not installed, so not removed&#xA;Package 'nvidia-experimental-346' is not installed, so not removed&#xA;Package 'nvidia-349' is not installed, so not removed&#xA;Package 'nvidia-349-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-349' is not installed, so not removed&#xA;Package 'nvidia-experimental-352' is not installed, so not removed&#xA;Package 'nvidia-355' is not installed, so not removed&#xA;Package 'nvidia-355-updates' is not installed, so not removed&#xA;Package 'nvidia-experimental-355' is not installed, so not removed&#xA;Note, selecting 'libnvtt-bin' instead of 'nvidia-texture-tools'&#xA;Package 'nvidia-390' is not installed, so not removed&#xA;Package 'nvidia-340-updates-uvm' is not installed, so not removed&#xA;Package 'nvidia-346' is not installed, so not removed&#xA;...&#xA;Package 'nvidia-opencl-icd-375' is not installed, so not removed&#xA;You might want to run 'apt-get -f install' to correct these:&#xA;The following packages have unmet dependencies:&#xA; cuda-drivers : Depends: nvidia-384 (&amp;gt;= 384.81)&#xA;                Depends: nvidia-384-dev (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-modprobe (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-settings (&amp;gt;= 384.81) but it is not going to be installed&#xA;                Depends: nvidia-opencl-icd-384 (&amp;gt;= 384.81) but it is not going to be installed&#xA; libcuda1-384 : Depends: nvidia-384 (&amp;gt;= 384.130)&#xA; update-notifier : Depends: update-notifier-common (= 3.168.13) but it is not going to be installed&#xA;E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I have tried many answers&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo dpkg --configure -a&#xA;sudo apt install -f&#xA;sudo apt dist-upgrade&#xA;sudo apt autoremove --purge&#xA;sudo dpkg -i /var/cache/apt/archives/*.deb&#xA;Sudo apt install --reinstall /var/cache/apt/archives/*.deb&#xA;sudo apt install pop-desktop&#xA;sudo apt-get install update-manager-core&#xA;sudo do-release-upgrade -d&#xA;sudo reboot&#xA;EOF&#xA;&#xA;$sudo dpkg -i --force-overwrite /var/cache/apt/archives/*.deb&#xA;&#xA;$sudo -s -- &amp;lt;&amp;lt;EOF&#xA;sudo apt-get autoclean&#xA;sudo apt-get update&#xA;sudo apt-get upgrade&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;But I got a different error and I also changed &lt;code&gt;source.list&lt;/code&gt; file. In addition, I have removed the package from the &lt;code&gt;state&lt;/code&gt; and I have tried &lt;code&gt;synaptic&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Reading package lists... Done&#xA;Building dependency tree       &#xA;Reading state information... Done&#xA;0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.&#xA;97 not fully installed or removed.&#xA;Need to get 0 B/5699 kB of archives.&#xA;After this operation, 0 B of additional disk space will be used.&#xA;dpkg: error processing package python-apt (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-attr (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-blinker (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-bs4 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-idna (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-ipaddress (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package python-pyasn1 (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: error processing package No apport report written because MaxReports is reached already&#xA;             No apport report written because MaxReports is reached already&#xA;                                                                           No apport report written because MaxReports is reached already&#xA;                                                         No apport report written because MaxReports is reached already&#xA;                                       No apport report written because MaxReports is reached already&#xA;                     No apport report written because MaxReports is reached already&#xA;   python-six (--configure):&#xA; package is in a very bad inconsistent state; you should&#xA; reinstall it before attempting configuration&#xA;dpkg: dependency problems prevent configuration of python-cryptography:&#xA; python-cryptography depends on python-idna; however:&#xA;  Package python-idna is not configured yet.&#xA; python-cryptography depends on python-ipaddress; however:&#xA;  Package python-ipaddress is not configured yet.&#xA; python-cryptography depends on python-pyasn1 (&amp;gt;= 0.1.8); however:&#xA;  Package python-pyasn1 is not configured yet.&#xA; python-cryptography depends on python-six (&amp;gt;= 1.4.1); however:&#xA;  Package python-six is not configured yet.&#xA;...&#xA;&#xA;dpkg: error processing package apt-xapian-index (--configure):&#xA; dependency problems - leaving unconfigured&#xA;dpkg: too many errors, stopping&#xA;No apport report written because MaxReports is reached already&#xA;                                                              Errors were encountered while processing:&#xA; python-apt&#xA; python-attr&#xA; python-blinker&#xA; python-bs4&#xA; python-idna&#xA; python-ipaddress&#xA; python-pyasn1&#xA; python-six&#xA; python-cryptography&#xA; python-dbus&#xA; python-debian&#xA; python-debtagshw&#xA; python-html5lib&#xA; python-httplib2&#xA; python-jwt&#xA; python-lxml&#xA; python-oauthlib&#xA; python-openssl&#xA; python-pyasn1-modules&#xA; python-serial&#xA; python-service-identity&#xA; python-zope.interface&#xA; python-twisted-core&#xA; python-xapian&#xA; python-xdg&#xA; python-piston-mini-client&#xA; software-center-aptdaemon-plugins&#xA; python-defer&#xA; python-aptdaemon&#xA; python-aptdaemon.gtk3widgets&#xA; python-oneconf&#xA; software-center&#xA; python-dirspec&#xA; python-ubuntu-sso-client&#xA; python3&#xA; python3-apt&#xA; ubuntu-drivers-common&#xA; python3-debian&#xA; lsb-release&#xA; python3-distupgrade&#xA; python3-update-manager&#xA; ubuntu-release-upgrader-core&#xA; update-manager-core&#xA; update-notifier-common&#xA; python3-commandnotfound&#xA; ufw&#xA; python3-apport&#xA; apport&#xA; apport-gtk&#xA; python3-xapian1.3&#xA; apt-xapian-index&#xA;Processing was halted because there were too many errors.&#xA;E: Sub-process /usr/bin/dpkg returned an error code (1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""1171012"" LastEditorUserId=""1171012"" LastEditDate=""2021-01-15T11:06:52.213"" LastActivityDate=""2021-01-20T00:28:45.260"" Title=""A problem when Installing the Nvidia, Cuda on Ubuntu 16.04, and with upgrading the Ubuntu"" Tags=""&lt;16.04&gt;&lt;nvidia&gt;&lt;python3&gt;&lt;cuda&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""13675"" PostTypeId=""1"" CreationDate=""2016-08-25T20:37:49.077"" Score=""1"" ViewCount=""261"" Body=""&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import tensorflow as tf&#xA;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;&#xA;x = np.linspace(0,10,20)&#xA;train_x = np.array([[i] for i in x])&#xA;train_y = np.sin(x)&#xA;&#xA;regressor = tf.contrib.learn.DNNRegressor(hidden_units=[10,20, 10])&#xA;regressor.fit(x = train_x, y = train_y, steps = 2000)&#xA;predictions = regressor.predict(x = train_x)&#xA;&#xA;plt.plot(x, train_y)&#xA;plt.plot(x, predictions)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It generates the following error message/warning:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/python3.4 /home/ttt/Dropbox/Programming/Python/Tensor_flow/one_dim_case.py&#xA;/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py:1197: &#xA;VisibleDeprecationWarning: converting an array with ndim &amp;gt; 0 to an &#xA;index will result in an error in the future&#xA;  result_shape.insert(dim, 1)&#xA;&#xA;Process finished with exit code 0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me to understand what is wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional remark, with so many hidden layers and neurons and number of iterations, I really surprised with poor &lt;code&gt;sin&lt;/code&gt; approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. This is a toy example to help to understand tensorflow&lt;/p&gt;&#xA;"" OwnerUserId=""14999"" LastEditorUserId=""14999"" LastEditDate=""2016-08-25T20:55:15.173"" LastActivityDate=""2016-08-25T20:55:15.173"" Title=""Tensor flow error - conversion and peformance"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""14102"" PostTypeId=""1"" AcceptedAnswerId=""14123"" CreationDate=""2016-09-20T05:40:10.897"" Score=""6"" ViewCount=""5756"" Body=""&lt;p&gt;OK this is my first time in ML and for starter I am implementing Naive Bayes. I have Cricket(sports) data in which I have to check whether the team will win or lost based on Toss Won|Lost and Bat First|Second. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB&#xA;import numpy as np&#xA;&#xA;&quot;&quot;&quot;&#xA;    Labels : Lost, Draw, Won [-1,0,1]&#xA;    Features&#xA;    ==========&#xA;        Toss(Lost,Won) = [-1,1]&#xA;        Bat(First, Second) = [-1,1]&#xA;&quot;&quot;&quot;&#xA;#Based on Existing Data Features are:&#xA;features = np.array([[-1, 1],[-1, 1]])&#xA;labels = np.array([0,1])&#xA;# Create a Gaussian Classifier&#xA;model = GaussianNB()&#xA;&#xA;# Train the model using the training sets&#xA;model.fit(features, labels)&#xA;&#xA;# Predict Output&#xA;predicted = model.predict([[1,0]])&#xA;print(predicted)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On running this I get error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:393: RuntimeWarning: divide by zero encountered in log&#xA;[0]&#xA;  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: divide by zero encountered in true_divide&#xA;  (self.sigma_[i, :]), 1)&#xA;/anaconda3/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:395: RuntimeWarning: invalid value encountered in subtract&#xA;  (self.sigma_[i, :]), 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code given &lt;a href=&quot;https://github.com/kadnan/PakistanEnglanTestMatches&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""10879"" LastEditorUserId=""10879"" LastEditDate=""2016-09-22T04:26:22.217"" LastActivityDate=""2018-01-30T21:27:37.267"" Title=""Naive Bayes: Divide by Zero error"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;naive-bayes-classifier&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""19465"" PostTypeId=""1"" AcceptedAnswerId=""19470"" CreationDate=""2017-06-04T09:12:08.460"" Score=""1"" ViewCount=""12641"" Body=""&lt;p&gt;I have been trying to implement logistic regression in python. Basically the code works and it gives the accuracy of the predictive model at a level of 91% but for some reason the AUC score is 0.5 which is basically the worst possible score because it means that the model is completely random. Also the classification report returns error: &quot;UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for)&quot;. Does anyone know what should I change so it works properly?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; import numpy as np&#xA; import pandas as pd&#xA; from sklearn.cross_validation import train_test_split&#xA; from sklearn.linear_model import LogisticRegression&#xA; from sklearn.metrics import accuracy_score  &#xA; from sklearn.preprocessing import StandardScaler&#xA; from sklearn.metrics import roc_auc_score&#xA; from sklearn.metrics import classification_report&#xA;&#xA; data_file = pd.read_csv('loan.csv', delimiter=',')&#xA;&#xA; # variable preprocessing&#xA;&#xA; data_file['loan_status'] = np.where(data_file['loan_status'].isin(['Fully &#xA; Paid', 'Current']), 1, 0)&#xA; loan_stat=data_file['loan_status']&#xA; loan_stat=loan_stat.astype(np.float64)&#xA;&#xA; m = {&#xA;    'n/a': 0,     &#xA;    '&amp;lt; 1 year': 0,&#xA;    '1 year': 1,&#xA;    '2 years': 2,&#xA;    '3 years': 3,&#xA;    '4 years': 4,&#xA;    '5 years': 5,&#xA;    '6 years': 6,&#xA;    '7 years': 7,&#xA;    '8 years': 8,&#xA;    '9 years': 9,&#xA;    '10+ years': 10&#xA; }&#xA; emp_length=data_file.emp_length.map(m)&#xA; emp_length.astype(np.float64)&#xA;&#xA; annual_inc=data_file['annual_inc']&#xA; delinq_2yrs=data_file['delinq_2yrs']&#xA; dti=data_file['dti']&#xA; loan_amnt=data_file['loan_amnt']&#xA; installment=data_file['installment']&#xA; int_rate=data_file['int_rate']&#xA; total_acc=data_file['total_acc']&#xA; open_acc=data_file['open_acc']&#xA; pub_rec=data_file['pub_rec']&#xA; acc_now_delinq=data_file['acc_now_delinq']&#xA;&#xA; #variables combined into one dataframe&#xA;&#xA; X=pd.DataFrame()&#xA;&#xA; X['annua_inc']=annual_inc&#xA; X['delinq_2yrs']=delinq_2yrs&#xA; X['dti']=dti&#xA; X['emp_length']=emp_length&#xA; X['loan_amnt']=loan_amnt&#xA; X['installment']=installment&#xA; X['int_rate']=int_rate&#xA; X['total_acc']=total_acc&#xA; X['open_acc']=open_acc&#xA; X['pub_rec']=pub_rec&#xA; X['acc_now_delinq']=acc_now_delinq&#xA; X['loan_stat']=loan_stat&#xA;&#xA; X=X.dropna(axis=0)&#xA; y=X['loan_stat']&#xA; X=X.drop(['loan_stat'], axis=1)&#xA;&#xA; scaler=StandardScaler()&#xA; X=scaler.fit_transform(X)&#xA;&#xA; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, &#xA; random_state=42)&#xA;&#xA; model=LogisticRegression(penalty='l2', C=1)&#xA; model.fit(X_train, y_train)&#xA; score=accuracy_score(y_test, model.predict(X_test))&#xA; roc=roc_auc_score(y_test, model.predict(X_test))&#xA; cr=classification_report(y_test, model.predict(X_test))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the link to the data: &lt;a href=&quot;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.kaggle.com/wendykan/lending-club-loan-data/downloads/lending-club-loan-data.zip&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""33018"" LastEditorUserId=""14372"" LastEditDate=""2017-06-04T15:07:33.320"" LastActivityDate=""2017-06-04T15:07:33.320"" Title=""AUC and classification report in Logistic regression in python"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""41113"" PostTypeId=""1"" CreationDate=""2018-11-12T16:48:25.597"" Score=""10"" ViewCount=""24933"" Body=""&lt;p&gt;I was watching Machine Learning A- Z from &lt;a href=&quot;https://superdatascience.com/machine-learning&quot; rel=&quot;noreferrer&quot;&gt;SuperDataScience&lt;/a&gt; but when I was doing below code sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import matplotlib.pyplot as plt&#xA;import pandas as pd&#xA;&#xA;&#xA;dataset = pd.read_csv('Data.csv')&#xA;X = dataset.iloc[:, :-1].values&#xA;&#xA;from sklearn.impute import  SimpleImputer&#xA;imputer = SimpleImputer(missing_values=np.nan, strategy='mean')&#xA;imputer = imputer.fit(X[:, 1:3])&#xA;X[:, 1:3]= imputer.transform(X[:,1:3])&#xA;&#xA;&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;labelencoder_X = LabelEncoder()&#xA;X[:, 0] = labelencoder_X.fit_transform(X[:, 0])&#xA;onehotencoder = OneHotEncoder(categorical_features =[0])&#xA;X = onehotencoder.fit_transform(X).toarray()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got this warning message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&#xA;  /usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  And this below message also&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.&#xA;  If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.&#xA;  In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.&#xA;    warnings.warn(msg, FutureWarning)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;    &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;  I was reading ColumnTransfer in sklearn website library I didn't understand how to fix these error messages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;SampleFile:&lt;a href=&quot;http://s8.picofile.com/file/8342539418/Data.csv.html&quot; rel=&quot;noreferrer&quot;&gt;Data.csv&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""14886"" LastEditorUserId=""29575"" LastEditDate=""2018-11-12T23:05:18.507"" LastActivityDate=""2019-06-08T20:47:20.393"" Title=""DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""5"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53325"" PostTypeId=""1"" AcceptedAnswerId=""53338"" CreationDate=""2019-06-06T10:52:01.857"" Score=""2"" ViewCount=""614"" Body=""&lt;p&gt;I am working through Kaggle's Titanic competition.  I am mostly done with my model but the problem is that the logistic regression model does not predict for all of 418 rows in the test set but instead just returns predictions for 197 rows.  This is the error PyCharm gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 37, in &amp;lt;module&amp;gt;&#xA;    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 392, in __init__&#xA;    mgr = init_dict(data, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 212, in init_dict&#xA;    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 51, in arrays_to_mgr&#xA;    index = extract_index(arrays)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\internals\construction.py&quot;, line 328, in extract_index&#xA;    raise ValueError(msg)&#xA;ValueError: array length 197 does not match index length 418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I &lt;code&gt;print(predictions)&lt;/code&gt; to confirm, this is what it gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0&#xA; 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0&#xA; 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0&#xA; 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1&#xA; 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0&#xA; 0 1 0 0 1 1 0 1 1 0 0 0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my full code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;&#xA;# Fill missing values in Age feature with each sexs median value of Age&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;&#xA;# Creating a new column called &quot;HasCabin&quot;, where passengers with a cabin will get a score of 1 and those without cabins will get a score of 0&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;&#xA;logReg = LogisticRegression()&#xA;&#xA;data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;# implement train_test_split&#xA;x_train, x_test, y_train, y_test = train_test_split(data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;# Training the model with the Logistic Regression algorithm&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(x_test)&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per what the users have pointed out, I went ahead and tried to remedy my mistake (ignore the code repetition. I'll be solving that later):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;import warnings&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.model_selection import train_test_split&#xA;&#xA;warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv&quot;)&#xA;&#xA;train['Sex'] = train['Sex'].replace(['female', 'male'], [0, 1])&#xA;train['Embarked'] = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])&#xA;train['Age'].fillna(train.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;train['HasCabin'] = train['Cabin'].notnull().astype(int)&#xA;train['Relatives'] = train['SibSp'] + train['Parch']&#xA;train_data = train[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;x_train, x_validate, y_train, y_validate = train_test_split(train_data, train['Survived'], test_size=0.22, random_state=0)&#xA;&#xA;test['Sex'] = test['Sex'].replace(['female', 'male'], [0, 1])&#xA;test['Embarked'] = test['Embarked'].replace(['C', 'Q', 'R'], [1, 2, 3])&#xA;test['Age'].fillna(test.groupby('Sex')['Age'].transform(&quot;median&quot;), inplace=True)&#xA;test['HasCabin'] = test['Cabin'].notnull().astype(int)&#xA;test['Relatives'] = test['SibSp'] + test['Parch']&#xA;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;logReg = LogisticRegression()&#xA;logReg.fit(x_train, y_train)&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})&#xA;&#xA;filename = 'Titanic-Submission.csv'&#xA;submission.to_csv(filename, index=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I tried to input the select test features into my algorithm&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;test_data = test[['Pclass', 'Sex', 'Relatives', 'Fare', 'Age', 'Embarked', 'HasCabin']]&#xA;&#xA;...&#xA;&#xA;predictions = logReg.predict(test[test_data])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now, I'm getting the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Titanic-Kaggle/TItanic-Kaggle.py&quot;, line 29, in &amp;lt;module&amp;gt;&#xA;    predictions = logReg.predict(test[test_data])&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 2914, in __getitem__&#xA;    return self._getitem_frame(key)&#xA;  File &quot;C:\Users\security\Anaconda3\envs\TItanic-Kaggle.py\lib\site-packages\pandas\core\frame.py&quot;, line 3009, in _getitem_frame&#xA;    raise ValueError('Must pass DataFrame with boolean values only')&#xA;ValueError: Must pass DataFrame with boolean values only&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Its telling me that I need to pass boolean values into my algorithm but I don't understand why. There wasn't such a prerequisite when I was using the exact same data format while training the model.&lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastEditorUserId=""73912"" LastEditDate=""2019-06-07T05:43:14.210"" LastActivityDate=""2019-06-10T15:38:59.713"" Title=""Logistic Regression doesn't predict for the entire test set"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;&lt;kaggle&gt;"" AnswerCount=""3"" CommentCount=""4"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""53513"" PostTypeId=""1"" AcceptedAnswerId=""53515"" CreationDate=""2019-06-10T09:36:50.037"" Score=""3"" ViewCount=""4297"" Body=""&lt;p&gt;I am currently working on the Boston problem hosted on Kaggle.  The dataset is nothing like the Titanic dataset.  There are many categorical columns and I'm trying to one-hot-encode these columns.  I've decided to go with the column &lt;code&gt;MSZoning&lt;/code&gt; to get the approach working and work out a strategy to apply it to other categorical columns.  This is a small snippet of the dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9zdWq.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9zdWq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the different types of values present in &lt;code&gt;MSZoning&lt;/code&gt;, so obviously integer encoding only would be a bad idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;['RL' 'RM' 'C (all)' 'FV' 'RH']&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my attempt on Python to assign &lt;code&gt;MSZoning&lt;/code&gt; with the new one-hot-encoded data.  I do know that one-hot-encoding turns each value into a column of its own and assigns binary values to each of them so I realize that this isn't exactly a good idea.  I wanted to try it anyways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd&#xA;from sklearn.preprocessing import LabelEncoder, OneHotEncoder&#xA;&#xA;&#xA;train = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;test = pd.read_csv(&quot;https://raw.githubusercontent.com/oo92/Boston-Kaggle/master/train.csv&quot;)&#xA;&#xA;labelEncoder = LabelEncoder()&#xA;&#xA;train['MSZoning'] = labelEncoder.fit_transform(train['MSZoning'])&#xA;train_OHE = OneHotEncoder(categorical_features=train['MSZoning'])&#xA;train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;&#xA;&#xA;print(train['MSZoning'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which is giving me the following (obvious) error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.&#xA;  &quot;use the ColumnTransformer instead.&quot;, DeprecationWarning)&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/security/Downloads/AP/Boston-Kaggle/Boston.py&quot;, line 11, in &amp;lt;module&amp;gt;&#xA;    train['MSZoning'] = train_OHE.fit_transform(train['MSZoning']).toarray()&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 511, in fit_transform&#xA;    self._handle_deprecations(X)&#xA;  File &quot;C:\Users\security\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py&quot;, line 394, in _handle_deprecations&#xA;    n_features = X.shape[1]&#xA;IndexError: tuple index out of range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did read through some Medium posts on this but they didn't exactly relate to what I was trying to do with my dataset as they were working with dummy data with a couple of categorical columns.  What I want to know is, how do I make use of one-hot-encoding after the (attempted) step?  &lt;/p&gt;&#xA;"" OwnerUserId=""73912"" LastActivityDate=""2019-06-10T11:31:11.067"" Title=""one-hot-encoding categorical data gives error"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;pandas&gt;&lt;kaggle&gt;"" AnswerCount=""2"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""68738"" PostTypeId=""1"" CreationDate=""2020-02-26T14:47:41.647"" Score=""0"" ViewCount=""1304"" Body=""&lt;p&gt;I am working on a problem to predict the revenue, a film will generate. Some of the features available in the data set are json collection for the crew, cast which worked in the film. I applied onehotencoding to these columns.&lt;br&gt;&#xA;As a result, I have a (3000*1835) sized array. This too I got after extracting only director's data from 'Crew' columns and applying PCA with 60% variance retention.&lt;br&gt;&#xA;But, when I apply polynomial regression, I get the below mentioned error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$\lib\site-packages\sklearn\model_selection_validation.py:532:&#xA;  FitFailedWarning: Estimator fit failed. The score on this train-test&#xA;  partition for these parameters will be set to nan. Details:&#xA;  MemoryError: Unable to allocate 30.2 GiB for an array with shape&#xA;  (2400, 1686366) and data type float64&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am using the code as shown below for polynomial regression:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;polyFeature = PolynomialFeatures(degree=2)&#xA;linearRegression = LinearRegression()&#xA;pipeline = Pipeline([('polyFeature',polyFeature),('linearRegression',linearRegression)])&#xA;score = cross_val_score(pipeline,XTrain,YTrain,n_jobs=4,cv=5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using a system with 6 cores, 32 GB RAM.&lt;/p&gt;&#xA;"" OwnerUserId=""88138"" LastActivityDate=""2023-08-09T18:07:45.997"" Title=""Low memory error while performing degree 2 polynomial regression on (3000*1835) sized array"" Tags=""&lt;scikit-learn&gt;&lt;regression&gt;&lt;dimensionality-reduction&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""68738"" PostTypeId=""1"" CreationDate=""2020-02-26T14:47:41.647"" Score=""0"" ViewCount=""1304"" Body=""&lt;p&gt;I am working on a problem to predict the revenue, a film will generate. Some of the features available in the data set are json collection for the crew, cast which worked in the film. I applied onehotencoding to these columns.&lt;br&gt;&#xA;As a result, I have a (3000*1835) sized array. This too I got after extracting only director's data from 'Crew' columns and applying PCA with 60% variance retention.&lt;br&gt;&#xA;But, when I apply polynomial regression, I get the below mentioned error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$\lib\site-packages\sklearn\model_selection_validation.py:532:&#xA;  FitFailedWarning: Estimator fit failed. The score on this train-test&#xA;  partition for these parameters will be set to nan. Details:&#xA;  MemoryError: Unable to allocate 30.2 GiB for an array with shape&#xA;  (2400, 1686366) and data type float64&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am using the code as shown below for polynomial regression:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;polyFeature = PolynomialFeatures(degree=2)&#xA;linearRegression = LinearRegression()&#xA;pipeline = Pipeline([('polyFeature',polyFeature),('linearRegression',linearRegression)])&#xA;score = cross_val_score(pipeline,XTrain,YTrain,n_jobs=4,cv=5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using a system with 6 cores, 32 GB RAM.&lt;/p&gt;&#xA;"" OwnerUserId=""88138"" LastActivityDate=""2023-08-09T18:07:45.997"" Title=""Low memory error while performing degree 2 polynomial regression on (3000*1835) sized array"" Tags=""&lt;scikit-learn&gt;&lt;regression&gt;&lt;dimensionality-reduction&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""68738"" PostTypeId=""1"" CreationDate=""2020-02-26T14:47:41.647"" Score=""0"" ViewCount=""1304"" Body=""&lt;p&gt;I am working on a problem to predict the revenue, a film will generate. Some of the features available in the data set are json collection for the crew, cast which worked in the film. I applied onehotencoding to these columns.&lt;br&gt;&#xA;As a result, I have a (3000*1835) sized array. This too I got after extracting only director's data from 'Crew' columns and applying PCA with 60% variance retention.&lt;br&gt;&#xA;But, when I apply polynomial regression, I get the below mentioned error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$\lib\site-packages\sklearn\model_selection_validation.py:532:&#xA;  FitFailedWarning: Estimator fit failed. The score on this train-test&#xA;  partition for these parameters will be set to nan. Details:&#xA;  MemoryError: Unable to allocate 30.2 GiB for an array with shape&#xA;  (2400, 1686366) and data type float64&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am using the code as shown below for polynomial regression:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;polyFeature = PolynomialFeatures(degree=2)&#xA;linearRegression = LinearRegression()&#xA;pipeline = Pipeline([('polyFeature',polyFeature),('linearRegression',linearRegression)])&#xA;score = cross_val_score(pipeline,XTrain,YTrain,n_jobs=4,cv=5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using a system with 6 cores, 32 GB RAM.&lt;/p&gt;&#xA;"" OwnerUserId=""88138"" LastActivityDate=""2023-08-09T18:07:45.997"" Title=""Low memory error while performing degree 2 polynomial regression on (3000*1835) sized array"" Tags=""&lt;scikit-learn&gt;&lt;regression&gt;&lt;dimensionality-reduction&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""67983"" PostTypeId=""1"" CreationDate=""2020-02-12T16:09:57.147"" Score=""0"" ViewCount=""257"" Body=""&lt;p&gt;I am running a hate speech classifier &lt;a href=&quot;https://scholar.google.com/scholar_url?url=https://www.aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/download/15665/14843&amp;amp;hl=en&amp;amp;sa=T&amp;amp;oi=gsb-gga&amp;amp;ct=res&amp;amp;cd=0&amp;amp;d=8792494420729905567&amp;amp;ei=MxxEXuDfE5KjywTAlpqgCg&amp;amp;scisig=AAGBfm2ulitL0_o2F7cI0yUwT37oymfGKg&quot; rel=&quot;nofollow noreferrer&quot;&gt;published&lt;/a&gt; by Davidson et al. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The principle is simple, the classifier takes as an input an annotated ('hateful', 'offensive', 'neither') dataset of tweets. It then calculates several features (e.g., TF-IDF, part-of-speech, sentiment, etc.) and uses logistic regression to make predictions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The authors have shared an iPython version &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/src/Automated%20Hate%20Speech%20Detection%20and%20the%20Problem%20of%20Offensive%20Language%20Python%203.6.ipynb&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which I have rewritten as a standard Python script (see below). Their data, in case anyone wants to test the code is &lt;a href=&quot;https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from warnings import filterwarnings&#xA;filterwarnings(&quot;ignore&quot;, category=UserWarning)&#xA;filterwarnings(&quot;ignore&quot;, category=FutureWarning)&#xA;import datetime&#xA;import pandas as pd&#xA;import numpy as np&#xA;from sklearn.feature_extraction.text import TfidfVectorizer&#xA;import nltk&#xA;from nltk.stem.porter import *&#xA;from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS&#xA;from textstat.textstat import *&#xA;from sklearn.linear_model import LogisticRegression&#xA;from sklearn.feature_selection import SelectFromModel&#xA;from sklearn.metrics import classification_report&#xA;from sklearn.model_selection import train_test_split&#xA;from sklearn.model_selection import StratifiedKFold, GridSearchCV&#xA;from sklearn.pipeline import Pipeline&#xA;import matplotlib.pyplot as plt&#xA;&#xA;INPUT_PATH = 'DavidsonDataset.csv'&#xA;&#xA;stopwords = nltk.corpus.stopwords.words(&quot;english&quot;)&#xA;&#xA;other_exclusions = [&quot;#ff&quot;, &quot;ff&quot;, &quot;rt&quot;]&#xA;stopwords.extend(other_exclusions)&#xA;&#xA;stemmer = PorterStemmer()&#xA;sentiment_analyzer = VS()&#xA;&#xA;def preprocess(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, '', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, '', parsed_text)&#xA;    return parsed_text&#xA;&#xA;&#xA;def tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z]*&quot;, tweet.lower())).strip()&#xA;    tokens = [stemmer.stem(t) for t in tweet.split()]&#xA;    return tokens&#xA;&#xA;&#xA;def basic_tokenize(tweet):&#xA;&#xA;    tweet = &quot; &quot;.join(re.split(&quot;[^a-zA-Z.,!?]*&quot;, tweet.lower())).strip()&#xA;    return tweet.split()&#xA;&#xA;&#xA;def count_twitter_objs(text_string):&#xA;&#xA;    space_pattern = '\s+'&#xA;    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;amp;+]|'&#xA;                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')&#xA;    mention_regex = '@[\w\-]+'&#xA;    hashtag_regex = '#[\w\-]+'&#xA;    parsed_text = re.sub(space_pattern, ' ', text_string)&#xA;    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)&#xA;    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)&#xA;    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)&#xA;    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))&#xA;&#xA;&#xA;def other_features(tweet):&#xA;&#xA;    sentiment = sentiment_analyzer.polarity_scores(tweet)&#xA;&#xA;    words = preprocess(tweet)  # Get text only&#xA;&#xA;    syllables = textstat.syllable_count(words)&#xA;    num_chars = sum(len(w) for w in words)&#xA;    num_chars_total = len(tweet)&#xA;    num_terms = len(tweet.split())&#xA;    num_words = len(words.split())&#xA;    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)&#xA;    num_unique_terms = len(set(words.split()))&#xA;&#xA;    # Modified FK grade, where avg words per sentence is just num words/1&#xA;    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)&#xA;    # Modified FRE score, where sentence fixed to 1&#xA;    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)&#xA;&#xA;    twitter_objs = count_twitter_objs(tweet)&#xA;    retweet = 0&#xA;    if &quot;rt&quot; in words:&#xA;        retweet = 1&#xA;    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,&#xA;                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],&#xA;                twitter_objs[2], twitter_objs[1],&#xA;                twitter_objs[0], retweet]&#xA;    return features&#xA;&#xA;&#xA;def get_feature_array(tweets):&#xA;    feats = []&#xA;    for t in tweets:&#xA;        feats.append(other_features(t))&#xA;    return np.array(feats)&#xA;&#xA;&#xA;vectorizer = TfidfVectorizer(&#xA;    tokenizer=tokenize,&#xA;    preprocessor=preprocess,&#xA;    ngram_range=(1, 3),&#xA;    stop_words=stopwords,&#xA;    use_idf=True,&#xA;    smooth_idf=False,&#xA;    norm=None,&#xA;    decode_error='replace',&#xA;    max_features=10000,&#xA;    min_df=5,&#xA;    max_df=0.75&#xA;)&#xA;&#xA;&#xA;def main_function():&#xA;&#xA;    df = pd.read_csv(INPUT_PATH)&#xA;&#xA;    tweets = df.text   # Get tweets&#xA;&#xA;    # Construct tfidf matrix and get relevant scores&#xA;    print(&quot;Contructing TF-IDF matrix and getting relevant scores...&quot;)&#xA;    tfidf = vectorizer.fit_transform(tweets).toarray()&#xA;    vocab = {v: i for i, v in enumerate(vectorizer.get_feature_names())}&#xA;    idf_vals = vectorizer.idf_&#xA;    idf_dict = {i: idf_vals[i] for i in vocab.values()}  # keys are indices; values are IDF scores&#xA;&#xA;    # Get POS tags for tweets and save as a string&#xA;    print(&quot;Getting POS tags and saving them as a string...&quot;)&#xA;    tweet_tags = []&#xA;    for t in tweets:&#xA;        tokens = basic_tokenize(preprocess(t))&#xA;        tags = nltk.pos_tag(tokens)&#xA;        tag_list = [x[1] for x in tags]&#xA;        tag_str = &quot; &quot;.join(tag_list)&#xA;        tweet_tags.append(tag_str)&#xA;&#xA;    # We can use the TFIDF vectorizer to get a token matrix for the POS tags&#xA;    pos_vectorizer = TfidfVectorizer(&#xA;        tokenizer=None,&#xA;        lowercase=False,&#xA;        preprocessor=None,&#xA;        ngram_range=(1, 3),&#xA;        stop_words=None,&#xA;        use_idf=False,&#xA;        smooth_idf=False,&#xA;        norm=None,&#xA;        decode_error='replace',&#xA;        max_features=5000,&#xA;        min_df=5,&#xA;        max_df=0.75,&#xA;    )&#xA;&#xA;    # Construct POS TF matrix and get vocab dict&#xA;    print(&quot;Constructing POS TF matrix...&quot;)&#xA;    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()&#xA;    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}&#xA;&#xA;    other_features_names = [&quot;FKRA&quot;, &quot;FRE&quot;, &quot;num_syllables&quot;, &quot;avg_syl_per_word&quot;, &quot;num_chars&quot;, &quot;num_chars_total&quot;, &quot;num_terms&quot;, &quot;num_words&quot;, &quot;num_unique_words&quot;, &quot;vader neg&quot;, &quot;vader pos&quot;, &quot;vader neu&quot;, &quot;vader compound&quot;, &quot;num_hashtags&quot;, &quot;num_mentions&quot;, &quot;num_urls&quot;, &quot;is_retweet&quot;]&#xA;&#xA;    print(&quot;Generating features...&quot;)&#xA;    feats = get_feature_array(tweets)&#xA;&#xA;    # Now join them all up&#xA;    M = np.concatenate([tfidf, pos, feats], axis=1)&#xA;&#xA;    print(&quot;Feature table shape: &quot;)&#xA;    print(M.shape)&#xA;&#xA;    # Finally get a list of variable names&#xA;    variables = [''] * len(vocab)&#xA;    for k, v in vocab.items():&#xA;        variables[v] = k&#xA;&#xA;    pos_variables = [''] * len(pos_vocab)&#xA;    for k, v in pos_vocab.items():&#xA;        pos_variables[v] = k&#xA;&#xA;    feature_names = variables + pos_variables + other_features_names&#xA;&#xA;    print(&quot;\nRunning the model...&quot;)&#xA;&#xA;    X = pd.DataFrame(M)&#xA;    y = df['label'].astype(int)&#xA;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)&#xA;&#xA;    pipe = Pipeline([('select', SelectFromModel(LogisticRegression(class_weight='balanced', penalty=&quot;l1&quot;, C=0.01))),&#xA;                     ('model', LogisticRegression(class_weight='balanced', penalty='l2'))])&#xA;&#xA;    param_grid = [{}]  # Optionally add parameters here&#xA;&#xA;    print(&quot;The best model is selected using a GridSearch with 5-fold CV.&quot;)&#xA;&#xA;    grid_search = GridSearchCV(pipe,&#xA;                               param_grid,&#xA;                               cv=StratifiedKFold(n_splits=5, random_state=42).split(X_train, y_train),&#xA;                               verbose=2)&#xA;&#xA;    model = grid_search.fit(X_train, y_train)&#xA;    y_preds = model.predict(X_test)&#xA;&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;&#xA;    print('\nProcess started...\n')&#xA;&#xA;    # Start timer&#xA;    start = datetime.datetime.now()&#xA;&#xA;    # Run awesome code&#xA;    main_function()&#xA;&#xA;    # End timer&#xA;    end = datetime.datetime.now()&#xA;&#xA;    # Print results&#xA;    print(&quot;\nProcess finished&quot;)&#xA;    print(&quot;Total time: &quot; + str(end - start))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The classifier works and I can produce a classification report. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that now I want to use this model to make a simple prediction. For example, I want to feed model a tweet and learn whether it's 'hateful', 'offensive', or 'neither'. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run the code below: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict([&quot;I don't like you.&quot;]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Expected 2D array, got 1D array instead:&#xA;array=[&quot;I don't like you.&quot;].&#xA;Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are several similar questions on StackOverflow where the answer is simply to reshape the table (as the error suggests). However, this does not work. If I run: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(-1, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;print(model.predict(np.array([&quot;I don't like you.&quot;]).reshape(1, -1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: X has a different shape than during fitting.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question has two parts: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How can I fix this? How can I use the model to make a single prediction? &lt;/li&gt;&#xA;&lt;li&gt;Is this a feature or a bug? As far as I understand this error, sklearn wants an input that has been &quot;fitted&quot; to have the same dimentions of the training set. Doesn't this beat the purpose of training in the first place? The goal is to instantly be able to make a prediction. It is obvious that I lack some key insight on this matter so I would be grateful if someone explained to me what I have understood wrong. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Clarification: There are several questions regarding the &lt;code&gt;Reshape your data&lt;/code&gt; error. However, the suggested solution (i.e., simply reshape as instructed) does not work for me as shown above. More importantly, I am interested in &lt;em&gt;understanding&lt;/em&gt; why this behavior is normal, something that is not discussed in the similar questions.&lt;/p&gt;&#xA;"" OwnerUserId=""47466"" LastActivityDate=""2020-02-12T21:29:00.257"" Title=""Cannot make a single prediction: Is this behavior normal?"" Tags=""&lt;python&gt;&lt;classification&gt;&lt;scikit-learn&gt;&lt;logistic-regression&gt;"" AnswerCount=""2"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""68738"" PostTypeId=""1"" CreationDate=""2020-02-26T14:47:41.647"" Score=""0"" ViewCount=""1304"" Body=""&lt;p&gt;I am working on a problem to predict the revenue, a film will generate. Some of the features available in the data set are json collection for the crew, cast which worked in the film. I applied onehotencoding to these columns.&lt;br&gt;&#xA;As a result, I have a (3000*1835) sized array. This too I got after extracting only director's data from 'Crew' columns and applying PCA with 60% variance retention.&lt;br&gt;&#xA;But, when I apply polynomial regression, I get the below mentioned error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$\lib\site-packages\sklearn\model_selection_validation.py:532:&#xA;  FitFailedWarning: Estimator fit failed. The score on this train-test&#xA;  partition for these parameters will be set to nan. Details:&#xA;  MemoryError: Unable to allocate 30.2 GiB for an array with shape&#xA;  (2400, 1686366) and data type float64&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am using the code as shown below for polynomial regression:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;polyFeature = PolynomialFeatures(degree=2)&#xA;linearRegression = LinearRegression()&#xA;pipeline = Pipeline([('polyFeature',polyFeature),('linearRegression',linearRegression)])&#xA;score = cross_val_score(pipeline,XTrain,YTrain,n_jobs=4,cv=5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using a system with 6 cores, 32 GB RAM.&lt;/p&gt;&#xA;"" OwnerUserId=""88138"" LastActivityDate=""2023-08-09T18:07:45.997"" Title=""Low memory error while performing degree 2 polynomial regression on (3000*1835) sized array"" Tags=""&lt;scikit-learn&gt;&lt;regression&gt;&lt;dimensionality-reduction&gt;&lt;python-3.x&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""76538"" PostTypeId=""1"" AcceptedAnswerId=""76574"" CreationDate=""2020-06-23T17:56:05.780"" Score=""0"" ViewCount=""663"" Body=""&lt;p&gt;I am doing classificaion using random forest classifier in python (scikit learn).&#xA;I have many different databases, each one has 33 observations and the prediction is based on 600 columns.&#xA;The script is iteration which run the classifier and then create confusion matrix for each.&lt;/p&gt;&#xA;&lt;p&gt;When I run the script it works but for some databases I get the next error message:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;:25: RuntimeWarning: invalid value encountered in true_divide   matrix = matrix.astype('float') /&#xA;matrix.sum(axis=1)[:, np.newaxis]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;also the results look weird:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/l9xVm.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l9xVm.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;I understand that this might happen if I have Null values in my data, but I have used dropna to make sure that there are no null values:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;df=df.dropna(axis=0,how='any')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is my script for the confusion matrix iteration:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for h in dfch:&#xA;    print('Hour:',h)&#xA;    print('')&#xA;    list_dates=dfch[h]['date'].unique()&#xA;#     print(h)&#xA;#     print(list_dates)&#xA;    for d in list_dates:&#xA;        print('date:',d)&#xA;        print('hour:',h)&#xA;        dfhd=dfch[h]&#xA;        dfhd=dfhd.loc[dfhd['date']==d]&#xA;        print('database size for hour',h,'date',d,'is',len(dfhd))&#xA;        X=dfhd.iloc[:, 4:]&#xA;        y=dfhd.iloc[:,2:3]&#xA;        #split the data&#xA;        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)&#xA;        #reshape the y_train to fit the the model&#xA;        y_train=y_train.values.ravel()&#xA;        #fit the model&#xA;        rfc.fit(X_train,y_train)&#xA;        rfc_pred=rfc.predict(X_test)&#xA;        print('')&#xA;        # Get and reshape confusion matrix data&#xA;        matrix = confusion_matrix(y_test, rfc_pred)&#xA;        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]&#xA;        # Build the plot&#xA;        plt.figure(figsize=(16,7))&#xA;        sns.set(font_scale=1.4)&#xA;        sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)&#xA;        # Add labels to the plot&#xA;        class_names = ['high','medium','low']&#xA;        tick_marks = np.arange(len(class_names))&#xA;        tick_marks2 = tick_marks + 0.5&#xA;        plt.xticks(tick_marks, class_names, rotation=25)&#xA;        plt.yticks(tick_marks2, class_names, rotation=0)&#xA;        plt.xlabel('Predicted label')&#xA;        plt.ylabel('True label')&#xA;        plt.title('Confusion Matrix for Random Forest Model')&#xA;        plt.show()&#xA;&#xA;&#xA;        score=rfc.score(X_test,y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;My question : How can it happen that it will be divided in 0/null? (there is no null in the databases), and how does it still display the confusion matrix if it fails due to dividing by 0? How canI solve it?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""98535"" LastActivityDate=""2020-06-24T07:42:28.087"" Title=""Random forest confusion matrix encountered invalid values"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;random-forest&gt;&lt;confusion-matrix&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""76538"" PostTypeId=""1"" AcceptedAnswerId=""76574"" CreationDate=""2020-06-23T17:56:05.780"" Score=""0"" ViewCount=""663"" Body=""&lt;p&gt;I am doing classificaion using random forest classifier in python (scikit learn).&#xA;I have many different databases, each one has 33 observations and the prediction is based on 600 columns.&#xA;The script is iteration which run the classifier and then create confusion matrix for each.&lt;/p&gt;&#xA;&lt;p&gt;When I run the script it works but for some databases I get the next error message:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;:25: RuntimeWarning: invalid value encountered in true_divide   matrix = matrix.astype('float') /&#xA;matrix.sum(axis=1)[:, np.newaxis]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;also the results look weird:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/l9xVm.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l9xVm.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;I understand that this might happen if I have Null values in my data, but I have used dropna to make sure that there are no null values:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;df=df.dropna(axis=0,how='any')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is my script for the confusion matrix iteration:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for h in dfch:&#xA;    print('Hour:',h)&#xA;    print('')&#xA;    list_dates=dfch[h]['date'].unique()&#xA;#     print(h)&#xA;#     print(list_dates)&#xA;    for d in list_dates:&#xA;        print('date:',d)&#xA;        print('hour:',h)&#xA;        dfhd=dfch[h]&#xA;        dfhd=dfhd.loc[dfhd['date']==d]&#xA;        print('database size for hour',h,'date',d,'is',len(dfhd))&#xA;        X=dfhd.iloc[:, 4:]&#xA;        y=dfhd.iloc[:,2:3]&#xA;        #split the data&#xA;        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)&#xA;        #reshape the y_train to fit the the model&#xA;        y_train=y_train.values.ravel()&#xA;        #fit the model&#xA;        rfc.fit(X_train,y_train)&#xA;        rfc_pred=rfc.predict(X_test)&#xA;        print('')&#xA;        # Get and reshape confusion matrix data&#xA;        matrix = confusion_matrix(y_test, rfc_pred)&#xA;        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]&#xA;        # Build the plot&#xA;        plt.figure(figsize=(16,7))&#xA;        sns.set(font_scale=1.4)&#xA;        sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)&#xA;        # Add labels to the plot&#xA;        class_names = ['high','medium','low']&#xA;        tick_marks = np.arange(len(class_names))&#xA;        tick_marks2 = tick_marks + 0.5&#xA;        plt.xticks(tick_marks, class_names, rotation=25)&#xA;        plt.yticks(tick_marks2, class_names, rotation=0)&#xA;        plt.xlabel('Predicted label')&#xA;        plt.ylabel('True label')&#xA;        plt.title('Confusion Matrix for Random Forest Model')&#xA;        plt.show()&#xA;&#xA;&#xA;        score=rfc.score(X_test,y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;My question : How can it happen that it will be divided in 0/null? (there is no null in the databases), and how does it still display the confusion matrix if it fails due to dividing by 0? How canI solve it?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""98535"" LastActivityDate=""2020-06-24T07:42:28.087"" Title=""Random forest confusion matrix encountered invalid values"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;random-forest&gt;&lt;confusion-matrix&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""76538"" PostTypeId=""1"" AcceptedAnswerId=""76574"" CreationDate=""2020-06-23T17:56:05.780"" Score=""0"" ViewCount=""663"" Body=""&lt;p&gt;I am doing classificaion using random forest classifier in python (scikit learn).&#xA;I have many different databases, each one has 33 observations and the prediction is based on 600 columns.&#xA;The script is iteration which run the classifier and then create confusion matrix for each.&lt;/p&gt;&#xA;&lt;p&gt;When I run the script it works but for some databases I get the next error message:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;:25: RuntimeWarning: invalid value encountered in true_divide   matrix = matrix.astype('float') /&#xA;matrix.sum(axis=1)[:, np.newaxis]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;also the results look weird:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/l9xVm.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l9xVm.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;I understand that this might happen if I have Null values in my data, but I have used dropna to make sure that there are no null values:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;df=df.dropna(axis=0,how='any')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is my script for the confusion matrix iteration:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for h in dfch:&#xA;    print('Hour:',h)&#xA;    print('')&#xA;    list_dates=dfch[h]['date'].unique()&#xA;#     print(h)&#xA;#     print(list_dates)&#xA;    for d in list_dates:&#xA;        print('date:',d)&#xA;        print('hour:',h)&#xA;        dfhd=dfch[h]&#xA;        dfhd=dfhd.loc[dfhd['date']==d]&#xA;        print('database size for hour',h,'date',d,'is',len(dfhd))&#xA;        X=dfhd.iloc[:, 4:]&#xA;        y=dfhd.iloc[:,2:3]&#xA;        #split the data&#xA;        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)&#xA;        #reshape the y_train to fit the the model&#xA;        y_train=y_train.values.ravel()&#xA;        #fit the model&#xA;        rfc.fit(X_train,y_train)&#xA;        rfc_pred=rfc.predict(X_test)&#xA;        print('')&#xA;        # Get and reshape confusion matrix data&#xA;        matrix = confusion_matrix(y_test, rfc_pred)&#xA;        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]&#xA;        # Build the plot&#xA;        plt.figure(figsize=(16,7))&#xA;        sns.set(font_scale=1.4)&#xA;        sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)&#xA;        # Add labels to the plot&#xA;        class_names = ['high','medium','low']&#xA;        tick_marks = np.arange(len(class_names))&#xA;        tick_marks2 = tick_marks + 0.5&#xA;        plt.xticks(tick_marks, class_names, rotation=25)&#xA;        plt.yticks(tick_marks2, class_names, rotation=0)&#xA;        plt.xlabel('Predicted label')&#xA;        plt.ylabel('True label')&#xA;        plt.title('Confusion Matrix for Random Forest Model')&#xA;        plt.show()&#xA;&#xA;&#xA;        score=rfc.score(X_test,y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;My question : How can it happen that it will be divided in 0/null? (there is no null in the databases), and how does it still display the confusion matrix if it fails due to dividing by 0? How canI solve it?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""98535"" LastActivityDate=""2020-06-24T07:42:28.087"" Title=""Random forest confusion matrix encountered invalid values"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;random-forest&gt;&lt;confusion-matrix&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""76538"" PostTypeId=""1"" AcceptedAnswerId=""76574"" CreationDate=""2020-06-23T17:56:05.780"" Score=""0"" ViewCount=""663"" Body=""&lt;p&gt;I am doing classificaion using random forest classifier in python (scikit learn).&#xA;I have many different databases, each one has 33 observations and the prediction is based on 600 columns.&#xA;The script is iteration which run the classifier and then create confusion matrix for each.&lt;/p&gt;&#xA;&lt;p&gt;When I run the script it works but for some databases I get the next error message:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;:25: RuntimeWarning: invalid value encountered in true_divide   matrix = matrix.astype('float') /&#xA;matrix.sum(axis=1)[:, np.newaxis]&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;also the results look weird:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/l9xVm.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l9xVm.png&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;I understand that this might happen if I have Null values in my data, but I have used dropna to make sure that there are no null values:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;df=df.dropna(axis=0,how='any')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;This is my script for the confusion matrix iteration:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for h in dfch:&#xA;    print('Hour:',h)&#xA;    print('')&#xA;    list_dates=dfch[h]['date'].unique()&#xA;#     print(h)&#xA;#     print(list_dates)&#xA;    for d in list_dates:&#xA;        print('date:',d)&#xA;        print('hour:',h)&#xA;        dfhd=dfch[h]&#xA;        dfhd=dfhd.loc[dfhd['date']==d]&#xA;        print('database size for hour',h,'date',d,'is',len(dfhd))&#xA;        X=dfhd.iloc[:, 4:]&#xA;        y=dfhd.iloc[:,2:3]&#xA;        #split the data&#xA;        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)&#xA;        #reshape the y_train to fit the the model&#xA;        y_train=y_train.values.ravel()&#xA;        #fit the model&#xA;        rfc.fit(X_train,y_train)&#xA;        rfc_pred=rfc.predict(X_test)&#xA;        print('')&#xA;        # Get and reshape confusion matrix data&#xA;        matrix = confusion_matrix(y_test, rfc_pred)&#xA;        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]&#xA;        # Build the plot&#xA;        plt.figure(figsize=(16,7))&#xA;        sns.set(font_scale=1.4)&#xA;        sns.heatmap(matrix, annot=True, annot_kws={'size':10},cmap=plt.cm.Greens, linewidths=0.2)&#xA;        # Add labels to the plot&#xA;        class_names = ['high','medium','low']&#xA;        tick_marks = np.arange(len(class_names))&#xA;        tick_marks2 = tick_marks + 0.5&#xA;        plt.xticks(tick_marks, class_names, rotation=25)&#xA;        plt.yticks(tick_marks2, class_names, rotation=0)&#xA;        plt.xlabel('Predicted label')&#xA;        plt.ylabel('True label')&#xA;        plt.title('Confusion Matrix for Random Forest Model')&#xA;        plt.show()&#xA;&#xA;&#xA;        score=rfc.score(X_test,y_test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;My question : How can it happen that it will be divided in 0/null? (there is no null in the databases), and how does it still display the confusion matrix if it fails due to dividing by 0? How canI solve it?&lt;/strong&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""98535"" LastActivityDate=""2020-06-24T07:42:28.087"" Title=""Random forest confusion matrix encountered invalid values"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;scikit-learn&gt;&lt;random-forest&gt;&lt;confusion-matrix&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""87630"" PostTypeId=""1"" AcceptedAnswerId=""87643"" CreationDate=""2021-01-07T12:44:49.077"" Score=""1"" ViewCount=""593"" Body=""&lt;p&gt;I'm currently working on building an LSTM network to forecast time-series data using PyTorch. I tried to share all the code pieces that I thought would be helpful, but please feel free to let me know if there's anything further I can provide. I added some comments at the end of the post regarding what the underlying issue might be.&lt;/p&gt;&#xA;&lt;p&gt;From the univariate time-series data indexed by date, I created 3 date features and split the data into training and validation sets as below.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# X_train&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 17:00:00 3   8   17&#xA;2015-01-12 19:30:00 0   12  19&#xA;2014-12-01 15:30:00 0   1   15&#xA;2014-07-26 09:00:00 5   26  9&#xA;2014-10-17 20:30:00 4   17  20&#xA;... ... ... ...&#xA;2014-08-29 06:30:00 4   29  6&#xA;2014-10-13 14:30:00 0   13  14&#xA;2015-01-03 02:00:00 5   3   2&#xA;2014-12-06 16:00:00 5   6   16&#xA;2015-01-06 20:30:00 1   6   20&#xA;8256 rows  3 columns&#xA;&#xA;# y_train&#xA;                    value&#xA;timestamp   &#xA;2015-01-08 17:00:00 17871&#xA;2015-01-12 19:30:00 20321&#xA;2014-12-01 15:30:00 16870&#xA;2014-07-26 09:00:00 11209&#xA;2014-10-17 20:30:00 26144&#xA;... ...&#xA;2014-08-29 06:30:00 9008&#xA;2014-10-13 14:30:00 17698&#xA;2015-01-03 02:00:00 12850&#xA;2014-12-06 16:00:00 18277&#xA;2015-01-06 20:30:00 19640&#xA;8256 rows  1 columns&#xA;&#xA;# X_val&#xA;             weekday    monthday    hour&#xA;timestamp           &#xA;2015-01-08 07:00:00 3   8   7&#xA;2014-10-13 22:00:00 0   13  22&#xA;2014-12-07 01:30:00 6   7   1&#xA;2014-10-14 17:30:00 1   14  17&#xA;2014-10-25 09:30:00 5   25  9&#xA;... ... ... ...&#xA;2014-09-26 12:30:00 4   26  12&#xA;2014-10-08 16:00:00 2   8   16&#xA;2014-12-03 01:30:00 2   3   1&#xA;2014-09-11 08:00:00 3   11  8&#xA;2015-01-15 10:00:00 3   15  10&#xA;2064 rows  3 columns&#xA;&#xA;# y_val&#xA;                    value&#xA;timestamp   &#xA;2014-09-13 13:00:00 21345&#xA;2014-10-28 20:30:00 23210&#xA;2015-01-21 17:00:00 17001&#xA;2014-07-20 10:30:00 13936&#xA;2015-01-29 02:00:00 3604&#xA;... ...&#xA;2014-11-17 11:00:00 15247&#xA;2015-01-14 00:00:00 10584&#xA;2014-09-02 13:00:00 17698&#xA;2014-08-31 13:00:00 16652&#xA;2014-08-30 12:30:00 15775&#xA;2064 rows  1 columns&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I transformed the values in the datasets by using MinMaxScaler from the sklearn library.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;scaler = MinMaxScaler()&#xA;X_train_arr = scaler.fit_transform(X_train)&#xA;X_val_arr = scaler.transform(X_val)&#xA;y_train_arr = scaler.fit_transform(y_train)&#xA;y_val_arr = scaler.transform(y_val)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;After converting these NumPy arrays into PyTorch Tensors, I created iterable datasets using TensorDataset and DataLoader classes provided by PyTorch.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from torch.utils.data import TensorDataset, DataLoader&#xA;from torch.autograd import Variable&#xA;&#xA;train_features = torch.Tensor(X_train_arr)&#xA;train_targets = torch.Tensor(y_train_arr)&#xA;&#xA;val_features = torch.Tensor(X_val_arr)&#xA;val_targets = torch.Tensor(y_val_arr)&#xA;&#xA;train = TensorDataset(train_features, train_targets)&#xA;train_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&#xA;val = TensorDataset(val_features, val_targets)&#xA;val_loader = DataLoader(train, batch_size=64, shuffle=False)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Then, I defined my LSTM Model and train_step functions as follows:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class LSTMModel(nn.Module):&#xA;    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):&#xA;        super(LSTMModel, self).__init__()&#xA;        # Hidden dimensions&#xA;        self.hidden_dim = hidden_dim&#xA;        &#xA;        # Number of hidden layers&#xA;        self.layer_dim = layer_dim&#xA;        &#xA;        # Building your LSTM&#xA;        # batch_first=True causes input/output tensors to be of shape&#xA;        # (batch_dim, seq_dim, feature_dim)&#xA;        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)&#xA;        &#xA;        # Readout layer&#xA;        self.fc = nn.Linear(hidden_dim, output_dim)&#xA;    &#xA;    def forward(self, x):&#xA;        # Initialize hidden state with zeros&#xA;        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # Initialize cell state&#xA;        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()&#xA;        &#xA;        # We need to detach as we are doing truncated backpropagation through time (BPTT)&#xA;        # If we don't, we'll backprop all the way to the start even after going through another batch&#xA;        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))&#xA;        &#xA;        # Index hidden state of last time step&#xA;        out = self.fc(out[:, -1, :]) &#xA;        return out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;def make_train_step(model, loss_fn, optimizer):&#xA;    # Builds function that performs a step in the train loop&#xA;    def train_step(x, y):&#xA;        # Sets model to TRAIN mode&#xA;        model.train()&#xA;        # Makes predictions&#xA;        yhat = model(x)&#xA;        # Computes loss&#xA;        loss = loss_fn(y, yhat)&#xA;        # Computes gradients&#xA;        loss.backward()&#xA;        # Updates parameters and zeroes gradients&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;        # Returns the loss&#xA;        return loss.item()&#xA;    &#xA;    # Returns the function that will be called inside the train loop&#xA;    return train_step&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Finally, I start training my LSTM model in mini-batches with AdamOptimizer for 20 epochs, which is already long enough to see the model is not learning.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch.optim as optim&#xA;&#xA;input_dim = n_features&#xA;hidden_dim = 64&#xA;layer_dim = 3&#xA;output_dim = 1&#xA;&#xA;model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)&#xA;&#xA;criterion = nn.MSELoss(reduction='mean')&#xA;optimizer = optim.Adam(model.parameters(), lr=1e-2)&#xA;&#xA;train_losses = []&#xA;val_losses = []&#xA;train_step = make_train_step(model, criterion, optimizer)&#xA;n_epochs = 20&#xA;device = 'cuda' if torch.cuda.is_available() else 'cpu'&#xA;&#xA;for epoch in range(n_epochs):&#xA;    batch_losses = []&#xA;    for x_batch, y_batch in train_loader:&#xA;        x_batch = x_batch.unsqueeze(dim=0).to(device)&#xA;        y_batch = y_batch.to(device)&#xA;        loss = train_step(x_batch, y_batch)&#xA;        batch_losses.append(loss)&#xA;    training_loss = np.mean(batch_losses)&#xA;    train_losses.append(training_loss)    &#xA;    with torch.no_grad():&#xA;        batch_val_losses = []&#xA;        for x_val, y_val in val_loader:&#xA;            x_val = x_val.unsqueeze(dim=0).to(device)&#xA;            y_val = y_val.to(device)        &#xA;            model.eval()&#xA;            yhat = model(x_val)&#xA;            val_loss = criterion(y_val, yhat).item()&#xA;            batch_val_losses.append(val_loss)&#xA;        validation_loss = np.mean(batch_val_losses)&#xA;        val_losses.append(validation_loss)&#xA;    &#xA;    print(f&amp;quot;[{epoch+1}] Training loss: {training_loss:.4f}\t Validation loss: {validation_loss:.4f}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;And this is the output:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\VS32XI\Anaconda3\lib\site-packages\torch\nn\modules\loss.py:446: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.&#xA;  return F.mse_loss(input, target, reduction=self.reduction)&#xA;[1] Training loss: 0.0505    Validation loss: 0.0315&#xA;[2] Training loss: 0.0317    Validation loss: 0.0315&#xA;[3] Training loss: 0.0317    Validation loss: 0.0315&#xA;[4] Training loss: 0.0317    Validation loss: 0.0315&#xA;[5] Training loss: 0.0317    Validation loss: 0.0315&#xA;[6] Training loss: 0.0317    Validation loss: 0.0315&#xA;[7] Training loss: 0.0317    Validation loss: 0.0315&#xA;[8] Training loss: 0.0317    Validation loss: 0.0315&#xA;[9] Training loss: 0.0317    Validation loss: 0.0315&#xA;[10] Training loss: 0.0317   Validation loss: 0.0315&#xA;[11] Training loss: 0.0317   Validation loss: 0.0315&#xA;[12] Training loss: 0.0317   Validation loss: 0.0315&#xA;[13] Training loss: 0.0317   Validation loss: 0.0315&#xA;[14] Training loss: 0.0317   Validation loss: 0.0315&#xA;[15] Training loss: 0.0317   Validation loss: 0.0315&#xA;[16] Training loss: 0.0317   Validation loss: 0.0315&#xA;[17] Training loss: 0.0317   Validation loss: 0.0315&#xA;[18] Training loss: 0.0317   Validation loss: 0.0315&#xA;[19] Training loss: 0.0317   Validation loss: 0.0315&#xA;[20] Training loss: 0.0317   Validation loss: 0.0315&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt; Looking at the warning given, I'm not sure if that's the real reason why the model is not learning well. After all, I'm trying to predict the future values in the time-series data; therefore, 1 would be a plausible output dimension.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt; To train the model in mini-batches, I relied on the class DataLoader. When iterating over the X and Y batches in both train and validation DataLoaders, the dimensions of x_batches were 2, while the model expected 3. So, I used PyTorch's unsqueeze function to match the expected dimension as in &lt;code&gt;x_batch.unsqueeze(dim=0) &lt;/code&gt;. I'm not sure if this is how I should have gone about it, which could also be the issue.&lt;/p&gt;&#xA;"" OwnerUserId=""108211"" LastActivityDate=""2021-01-07T17:02:57.670"" Title=""PyTorch: LSTM for time-series failing to learn"" Tags=""&lt;python&gt;&lt;deep-learning&gt;&lt;time-series&gt;&lt;lstm&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""100520"" PostTypeId=""1"" AcceptedAnswerId=""100524"" CreationDate=""2021-08-25T22:38:25.163"" Score=""0"" ViewCount=""309"" Body=""&lt;p&gt;&lt;strong&gt;I have the following LSTM model and I can't make inference with it:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;print(&amp;quot;Define LSTM model&amp;quot;)&#xA;&#xA;rnnmodel=Sequential()&#xA;rnnmodel.add(embedding_layer)&#xA;&#xA;rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))&#xA;rnnmodel.add(Dense(2, activation=&amp;quot;sigmoid&amp;quot;))&#xA;&#xA;rnnmodel.compile(loss=&amp;quot;binary_crossentropy&amp;quot;,&#xA;                 optimizer=&amp;quot;adam&amp;quot;,&#xA;                 metrics=[&amp;quot;accuracy&amp;quot;])&#xA;&#xA;rnnmodel.fit(X_train, y_train,&#xA;             batch_size=256,&#xA;             epochs=1,&#xA;             validation_data=(x_val, y_val))&#xA;&#xA;score, acc=rnnmodel.evaluate(test_data, test_labels, batch_size=128)&#xA;print(f&amp;quot;Test accuracy with RNN: {acc}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;(epoch is 1 to test) I want to make an inference with the text, let's say&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I check the documentation of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict&quot; rel=&quot;nofollow noreferrer&quot;&gt;tf.keras.Sequential&lt;/a&gt; and it states I should use the &lt;code&gt;predict&lt;/code&gt; function and the input should be &lt;em&gt;&amp;quot;A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).&amp;quot;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So what I did is:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;inference_sequence=tokenizer.texts_to_sequences(text)&#xA;inference_data=pad_sequences(inference_sequence, maxlen=MAX_SEQUENCE_LENGTH)&#xA;&#xA;predictions=rnnmodel.predict(inference_data)&#xA;&#xA;print(predictions)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the result &lt;code&gt;[[0.63219154 0.33410403]]&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;However I've given only one sentence. Why it gives me two results? I checked the &lt;code&gt;sigmoid&lt;/code&gt; documentation from &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and for an confirmed it should return only one result. So what's the problem here?&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;I also tried other approaches to make inference like mentioned &lt;a href=&quot;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&quot;&gt;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So I did:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;rnnmodel.predict(text)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the warning: &lt;em&gt;WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(&amp;quot;embedding_input:0&amp;quot;, shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1).&lt;/em&gt; &lt;strong&gt;and stuck forever.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;What should I do I just can't make an inference.&lt;/p&gt;&#xA;"" OwnerUserId=""76779"" LastEditorUserId=""76779"" LastEditDate=""2021-08-26T04:17:35.180"" LastActivityDate=""2021-08-26T04:34:47.043"" Title=""How to Inference With Keras Sequential Models (Text Classification)"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;nlp&gt;&lt;tensorflow&gt;&lt;lstm&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""100520"" PostTypeId=""1"" AcceptedAnswerId=""100524"" CreationDate=""2021-08-25T22:38:25.163"" Score=""0"" ViewCount=""309"" Body=""&lt;p&gt;&lt;strong&gt;I have the following LSTM model and I can't make inference with it:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;print(&amp;quot;Define LSTM model&amp;quot;)&#xA;&#xA;rnnmodel=Sequential()&#xA;rnnmodel.add(embedding_layer)&#xA;&#xA;rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))&#xA;rnnmodel.add(Dense(2, activation=&amp;quot;sigmoid&amp;quot;))&#xA;&#xA;rnnmodel.compile(loss=&amp;quot;binary_crossentropy&amp;quot;,&#xA;                 optimizer=&amp;quot;adam&amp;quot;,&#xA;                 metrics=[&amp;quot;accuracy&amp;quot;])&#xA;&#xA;rnnmodel.fit(X_train, y_train,&#xA;             batch_size=256,&#xA;             epochs=1,&#xA;             validation_data=(x_val, y_val))&#xA;&#xA;score, acc=rnnmodel.evaluate(test_data, test_labels, batch_size=128)&#xA;print(f&amp;quot;Test accuracy with RNN: {acc}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;(epoch is 1 to test) I want to make an inference with the text, let's say&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I check the documentation of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict&quot; rel=&quot;nofollow noreferrer&quot;&gt;tf.keras.Sequential&lt;/a&gt; and it states I should use the &lt;code&gt;predict&lt;/code&gt; function and the input should be &lt;em&gt;&amp;quot;A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).&amp;quot;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So what I did is:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;inference_sequence=tokenizer.texts_to_sequences(text)&#xA;inference_data=pad_sequences(inference_sequence, maxlen=MAX_SEQUENCE_LENGTH)&#xA;&#xA;predictions=rnnmodel.predict(inference_data)&#xA;&#xA;print(predictions)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the result &lt;code&gt;[[0.63219154 0.33410403]]&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;However I've given only one sentence. Why it gives me two results? I checked the &lt;code&gt;sigmoid&lt;/code&gt; documentation from &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and for an confirmed it should return only one result. So what's the problem here?&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;I also tried other approaches to make inference like mentioned &lt;a href=&quot;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&quot;&gt;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So I did:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;rnnmodel.predict(text)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the warning: &lt;em&gt;WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(&amp;quot;embedding_input:0&amp;quot;, shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1).&lt;/em&gt; &lt;strong&gt;and stuck forever.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;What should I do I just can't make an inference.&lt;/p&gt;&#xA;"" OwnerUserId=""76779"" LastEditorUserId=""76779"" LastEditDate=""2021-08-26T04:17:35.180"" LastActivityDate=""2021-08-26T04:34:47.043"" Title=""How to Inference With Keras Sequential Models (Text Classification)"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;nlp&gt;&lt;tensorflow&gt;&lt;lstm&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""104638"" PostTypeId=""1"" CreationDate=""2021-11-29T22:22:46.813"" Score=""2"" ViewCount=""990"" Body=""&lt;p&gt;My CNN tensorflow model reports 100% validation accuracy within 2 epochs. But it incorrectly predicts on single new images. (It is multiclass problem. I have 3 classes). How to resolve this? Can you please help me understand these epoch results?&lt;/p&gt;&#xA;&lt;p&gt;I have 1,000 images per class that are representative of my testing data. How can validation accuracy reach 1.00 in just the first epoch when I have a dataset of 3,000 images in total, equal amount per class? (I would expect this to start at around 33% percent -- 1/ 3 classes.&lt;/p&gt;&#xA;&lt;p&gt;Attempts:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I tried to ensure that I have correctly split my dataset into training and validation.&lt;/li&gt;&#xA;&lt;li&gt;I understand overfitting can be a problem. I've added a dropout layer to try to solve this potential problem. From this question&lt;a href=&quot;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&quot;&gt;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&lt;/a&gt; I learned that a &amp;quot;model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered.&amp;quot; I don't believe my model is overfitting based on this description. (My model reports both high training and high validation accuracy. If my model was overfitting I'd expect high training accuracy and low validation accuracy.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My model:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def model():&#xA;  model_input = tf.keras.layers.Input(shape=(h, w, 3)) &#xA;  x = tf.keras.layers.Rescaling(rescale_factor)(model_input) &#xA;  x = tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same')(x)&#xA;  x = tf.keras.layers.Dropout(.5)(x)&#xA;  x = tf.keras.layers.MaxPooling2D()(x) &#xA;  x = tf.keras.layers.Flatten()(x)&#xA;  x = tf.keras.layers.Dense(128, activation='relu')(x)&#xA;  outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Epoch results:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Epoch 1/10&#xA;/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: &amp;quot;`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;27/27 [==============================] - 13s 124ms/step - loss: 1.0004 - accuracy: 0.5953 - val_loss: 0.5053 - val_accuracy: 0.8920&#xA;Epoch 2/10&#xA;27/27 [==============================] - 1s 46ms/step - loss: 0.1368 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000&#xA;Epoch 3/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9116e-04 - val_accuracy: 1.0000&#xA;Epoch 4/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 3.0633e-04 - accuracy: 1.0000 - val_loss: 3.5376e-04 - val_accuracy: 1.0000&#xA;Epoch 5/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.7445e-04 - accuracy: 1.0000 - val_loss: 2.2319e-04 - val_accuracy: 1.0000&#xA;Epoch 6/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.2910e-04 - accuracy: 1.0000 - val_loss: 1.8078e-04 - val_accuracy: 1.0000&#xA;Epoch 7/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.0425e-04 - accuracy: 1.0000 - val_loss: 1.4247e-04 - val_accuracy: 1.0000&#xA;Epoch 8/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 8.6284e-05 - accuracy: 1.0000 - val_loss: 1.2057e-04 - val_accuracy: 1.0000&#xA;Epoch 9/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 7.0085e-05 - accuracy: 1.0000 - val_loss: 9.3485e-05 - val_accuracy: 1.0000&#xA;Epoch 10/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 5.4979e-05 - accuracy: 1.0000 - val_loss: 8.5952e-05 - val_accuracy: 1.0000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Model.fit and model.compile:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;model = model()&#xA;&#xA;model = tf.keras.Model(model_input, outputs)&#xA;  &#xA; model.compile(optimizer='adam',&#xA;              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),&#xA;              metrics=['accuracy'])&#xA;  &#xA;hist = model.fit(&#xA;  train_ds,&#xA;  validation_data=val_ds,&#xA;  epochs=10&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Code to predict new image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def makePrediction(image):&#xA;  from IPython.display import display&#xA;  from PIL import Image&#xA;  from tensorflow.keras.preprocessing import image_dataset_from_directory &#xA;  img = keras.preprocessing.image.load_img(&#xA;  image, target_size=(h, q)&#xA;  )&#xA;  img_array = keras.preprocessing.image.img_to_array(img)&#xA;  img_array = tf.expand_dims(img_array, 0) #Create a batch&#xA; &#xA;  predicts = model.predict(img_array)&#xA;  p = class_names[np.argmax(predicts)]&#xA;  return p&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Going to the &amp;quot;data&amp;quot; directory and using the folders to create a dataset. Each folder is a class label:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.preprocessing import image&#xA;directory_data = &amp;quot;data&amp;quot;&#xA;tf.keras.utils.image_dataset_from_directory(&#xA;    directory_testData, labels='inferred', label_mode='int',&#xA;    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,&#xA;    256), shuffle=True, seed=123, validation_split=0.2, subset=&amp;quot;validation&amp;quot;,&#xA;    interpolation='bilinear', follow_links=False,&#xA;    crop_to_aspect_ratio=False&#xA;)&#xA; &#xA;tf.keras.utils.image_dataset_from_directory(directory_testData, labels='inferred')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Creating dataset and splitting it:&lt;/p&gt;&#xA;&lt;p&gt;Train_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 1284 files for training.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;train_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;  directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;training&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Val_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 321 files for validation.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;val_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;validation&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""128347"" LastActivityDate=""2021-12-07T22:10:34.553"" Title=""Why is val accuracy 100% within 2 epochs and incorrectly predicting new images? (1,000 images per class when training)"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""100520"" PostTypeId=""1"" AcceptedAnswerId=""100524"" CreationDate=""2021-08-25T22:38:25.163"" Score=""0"" ViewCount=""309"" Body=""&lt;p&gt;&lt;strong&gt;I have the following LSTM model and I can't make inference with it:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;print(&amp;quot;Define LSTM model&amp;quot;)&#xA;&#xA;rnnmodel=Sequential()&#xA;rnnmodel.add(embedding_layer)&#xA;&#xA;rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))&#xA;rnnmodel.add(Dense(2, activation=&amp;quot;sigmoid&amp;quot;))&#xA;&#xA;rnnmodel.compile(loss=&amp;quot;binary_crossentropy&amp;quot;,&#xA;                 optimizer=&amp;quot;adam&amp;quot;,&#xA;                 metrics=[&amp;quot;accuracy&amp;quot;])&#xA;&#xA;rnnmodel.fit(X_train, y_train,&#xA;             batch_size=256,&#xA;             epochs=1,&#xA;             validation_data=(x_val, y_val))&#xA;&#xA;score, acc=rnnmodel.evaluate(test_data, test_labels, batch_size=128)&#xA;print(f&amp;quot;Test accuracy with RNN: {acc}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;(epoch is 1 to test) I want to make an inference with the text, let's say&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I check the documentation of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict&quot; rel=&quot;nofollow noreferrer&quot;&gt;tf.keras.Sequential&lt;/a&gt; and it states I should use the &lt;code&gt;predict&lt;/code&gt; function and the input should be &lt;em&gt;&amp;quot;A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).&amp;quot;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So what I did is:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;inference_sequence=tokenizer.texts_to_sequences(text)&#xA;inference_data=pad_sequences(inference_sequence, maxlen=MAX_SEQUENCE_LENGTH)&#xA;&#xA;predictions=rnnmodel.predict(inference_data)&#xA;&#xA;print(predictions)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the result &lt;code&gt;[[0.63219154 0.33410403]]&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;However I've given only one sentence. Why it gives me two results? I checked the &lt;code&gt;sigmoid&lt;/code&gt; documentation from &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and for an confirmed it should return only one result. So what's the problem here?&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;I also tried other approaches to make inference like mentioned &lt;a href=&quot;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&quot;&gt;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So I did:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;rnnmodel.predict(text)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the warning: &lt;em&gt;WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(&amp;quot;embedding_input:0&amp;quot;, shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1).&lt;/em&gt; &lt;strong&gt;and stuck forever.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;What should I do I just can't make an inference.&lt;/p&gt;&#xA;"" OwnerUserId=""76779"" LastEditorUserId=""76779"" LastEditDate=""2021-08-26T04:17:35.180"" LastActivityDate=""2021-08-26T04:34:47.043"" Title=""How to Inference With Keras Sequential Models (Text Classification)"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;nlp&gt;&lt;tensorflow&gt;&lt;lstm&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""104638"" PostTypeId=""1"" CreationDate=""2021-11-29T22:22:46.813"" Score=""2"" ViewCount=""990"" Body=""&lt;p&gt;My CNN tensorflow model reports 100% validation accuracy within 2 epochs. But it incorrectly predicts on single new images. (It is multiclass problem. I have 3 classes). How to resolve this? Can you please help me understand these epoch results?&lt;/p&gt;&#xA;&lt;p&gt;I have 1,000 images per class that are representative of my testing data. How can validation accuracy reach 1.00 in just the first epoch when I have a dataset of 3,000 images in total, equal amount per class? (I would expect this to start at around 33% percent -- 1/ 3 classes.&lt;/p&gt;&#xA;&lt;p&gt;Attempts:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I tried to ensure that I have correctly split my dataset into training and validation.&lt;/li&gt;&#xA;&lt;li&gt;I understand overfitting can be a problem. I've added a dropout layer to try to solve this potential problem. From this question&lt;a href=&quot;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&quot;&gt;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&lt;/a&gt; I learned that a &amp;quot;model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered.&amp;quot; I don't believe my model is overfitting based on this description. (My model reports both high training and high validation accuracy. If my model was overfitting I'd expect high training accuracy and low validation accuracy.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My model:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def model():&#xA;  model_input = tf.keras.layers.Input(shape=(h, w, 3)) &#xA;  x = tf.keras.layers.Rescaling(rescale_factor)(model_input) &#xA;  x = tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same')(x)&#xA;  x = tf.keras.layers.Dropout(.5)(x)&#xA;  x = tf.keras.layers.MaxPooling2D()(x) &#xA;  x = tf.keras.layers.Flatten()(x)&#xA;  x = tf.keras.layers.Dense(128, activation='relu')(x)&#xA;  outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Epoch results:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Epoch 1/10&#xA;/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: &amp;quot;`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;27/27 [==============================] - 13s 124ms/step - loss: 1.0004 - accuracy: 0.5953 - val_loss: 0.5053 - val_accuracy: 0.8920&#xA;Epoch 2/10&#xA;27/27 [==============================] - 1s 46ms/step - loss: 0.1368 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000&#xA;Epoch 3/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9116e-04 - val_accuracy: 1.0000&#xA;Epoch 4/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 3.0633e-04 - accuracy: 1.0000 - val_loss: 3.5376e-04 - val_accuracy: 1.0000&#xA;Epoch 5/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.7445e-04 - accuracy: 1.0000 - val_loss: 2.2319e-04 - val_accuracy: 1.0000&#xA;Epoch 6/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.2910e-04 - accuracy: 1.0000 - val_loss: 1.8078e-04 - val_accuracy: 1.0000&#xA;Epoch 7/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.0425e-04 - accuracy: 1.0000 - val_loss: 1.4247e-04 - val_accuracy: 1.0000&#xA;Epoch 8/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 8.6284e-05 - accuracy: 1.0000 - val_loss: 1.2057e-04 - val_accuracy: 1.0000&#xA;Epoch 9/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 7.0085e-05 - accuracy: 1.0000 - val_loss: 9.3485e-05 - val_accuracy: 1.0000&#xA;Epoch 10/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 5.4979e-05 - accuracy: 1.0000 - val_loss: 8.5952e-05 - val_accuracy: 1.0000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Model.fit and model.compile:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;model = model()&#xA;&#xA;model = tf.keras.Model(model_input, outputs)&#xA;  &#xA; model.compile(optimizer='adam',&#xA;              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),&#xA;              metrics=['accuracy'])&#xA;  &#xA;hist = model.fit(&#xA;  train_ds,&#xA;  validation_data=val_ds,&#xA;  epochs=10&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Code to predict new image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def makePrediction(image):&#xA;  from IPython.display import display&#xA;  from PIL import Image&#xA;  from tensorflow.keras.preprocessing import image_dataset_from_directory &#xA;  img = keras.preprocessing.image.load_img(&#xA;  image, target_size=(h, q)&#xA;  )&#xA;  img_array = keras.preprocessing.image.img_to_array(img)&#xA;  img_array = tf.expand_dims(img_array, 0) #Create a batch&#xA; &#xA;  predicts = model.predict(img_array)&#xA;  p = class_names[np.argmax(predicts)]&#xA;  return p&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Going to the &amp;quot;data&amp;quot; directory and using the folders to create a dataset. Each folder is a class label:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.preprocessing import image&#xA;directory_data = &amp;quot;data&amp;quot;&#xA;tf.keras.utils.image_dataset_from_directory(&#xA;    directory_testData, labels='inferred', label_mode='int',&#xA;    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,&#xA;    256), shuffle=True, seed=123, validation_split=0.2, subset=&amp;quot;validation&amp;quot;,&#xA;    interpolation='bilinear', follow_links=False,&#xA;    crop_to_aspect_ratio=False&#xA;)&#xA; &#xA;tf.keras.utils.image_dataset_from_directory(directory_testData, labels='inferred')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Creating dataset and splitting it:&lt;/p&gt;&#xA;&lt;p&gt;Train_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 1284 files for training.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;train_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;  directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;training&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Val_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 321 files for validation.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;val_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;validation&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""128347"" LastActivityDate=""2021-12-07T22:10:34.553"" Title=""Why is val accuracy 100% within 2 epochs and incorrectly predicting new images? (1,000 images per class when training)"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""100520"" PostTypeId=""1"" AcceptedAnswerId=""100524"" CreationDate=""2021-08-25T22:38:25.163"" Score=""0"" ViewCount=""309"" Body=""&lt;p&gt;&lt;strong&gt;I have the following LSTM model and I can't make inference with it:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;print(&amp;quot;Define LSTM model&amp;quot;)&#xA;&#xA;rnnmodel=Sequential()&#xA;rnnmodel.add(embedding_layer)&#xA;&#xA;rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))&#xA;rnnmodel.add(Dense(2, activation=&amp;quot;sigmoid&amp;quot;))&#xA;&#xA;rnnmodel.compile(loss=&amp;quot;binary_crossentropy&amp;quot;,&#xA;                 optimizer=&amp;quot;adam&amp;quot;,&#xA;                 metrics=[&amp;quot;accuracy&amp;quot;])&#xA;&#xA;rnnmodel.fit(X_train, y_train,&#xA;             batch_size=256,&#xA;             epochs=1,&#xA;             validation_data=(x_val, y_val))&#xA;&#xA;score, acc=rnnmodel.evaluate(test_data, test_labels, batch_size=128)&#xA;print(f&amp;quot;Test accuracy with RNN: {acc}&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;(epoch is 1 to test) I want to make an inference with the text, let's say&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I check the documentation of &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict&quot; rel=&quot;nofollow noreferrer&quot;&gt;tf.keras.Sequential&lt;/a&gt; and it states I should use the &lt;code&gt;predict&lt;/code&gt; function and the input should be &lt;em&gt;&amp;quot;A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).&amp;quot;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So what I did is:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;inference_sequence=tokenizer.texts_to_sequences(text)&#xA;inference_data=pad_sequences(inference_sequence, maxlen=MAX_SEQUENCE_LENGTH)&#xA;&#xA;predictions=rnnmodel.predict(inference_data)&#xA;&#xA;print(predictions)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the result &lt;code&gt;[[0.63219154 0.33410403]]&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;However I've given only one sentence. Why it gives me two results? I checked the &lt;code&gt;sigmoid&lt;/code&gt; documentation from &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and for an confirmed it should return only one result. So what's the problem here?&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;I also tried other approaches to make inference like mentioned &lt;a href=&quot;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&quot;&gt;https://stackoverflow.com/questions/61443543/how-to-make-prediction-on-keras-text-classification&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;So I did:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;text=[&amp;quot;the product was horrible&amp;quot;]&#xA;rnnmodel.predict(text)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;and it gives me the warning: &lt;em&gt;WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(&amp;quot;embedding_input:0&amp;quot;, shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1).&lt;/em&gt; &lt;strong&gt;and stuck forever.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;What should I do I just can't make an inference.&lt;/p&gt;&#xA;"" OwnerUserId=""76779"" LastEditorUserId=""76779"" LastEditDate=""2021-08-26T04:17:35.180"" LastActivityDate=""2021-08-26T04:34:47.043"" Title=""How to Inference With Keras Sequential Models (Text Classification)"" Tags=""&lt;python&gt;&lt;keras&gt;&lt;nlp&gt;&lt;tensorflow&gt;&lt;lstm&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""104638"" PostTypeId=""1"" CreationDate=""2021-11-29T22:22:46.813"" Score=""2"" ViewCount=""990"" Body=""&lt;p&gt;My CNN tensorflow model reports 100% validation accuracy within 2 epochs. But it incorrectly predicts on single new images. (It is multiclass problem. I have 3 classes). How to resolve this? Can you please help me understand these epoch results?&lt;/p&gt;&#xA;&lt;p&gt;I have 1,000 images per class that are representative of my testing data. How can validation accuracy reach 1.00 in just the first epoch when I have a dataset of 3,000 images in total, equal amount per class? (I would expect this to start at around 33% percent -- 1/ 3 classes.&lt;/p&gt;&#xA;&lt;p&gt;Attempts:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I tried to ensure that I have correctly split my dataset into training and validation.&lt;/li&gt;&#xA;&lt;li&gt;I understand overfitting can be a problem. I've added a dropout layer to try to solve this potential problem. From this question&lt;a href=&quot;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&quot;&gt;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&lt;/a&gt; I learned that a &amp;quot;model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered.&amp;quot; I don't believe my model is overfitting based on this description. (My model reports both high training and high validation accuracy. If my model was overfitting I'd expect high training accuracy and low validation accuracy.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My model:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def model():&#xA;  model_input = tf.keras.layers.Input(shape=(h, w, 3)) &#xA;  x = tf.keras.layers.Rescaling(rescale_factor)(model_input) &#xA;  x = tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same')(x)&#xA;  x = tf.keras.layers.Dropout(.5)(x)&#xA;  x = tf.keras.layers.MaxPooling2D()(x) &#xA;  x = tf.keras.layers.Flatten()(x)&#xA;  x = tf.keras.layers.Dense(128, activation='relu')(x)&#xA;  outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Epoch results:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Epoch 1/10&#xA;/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: &amp;quot;`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;27/27 [==============================] - 13s 124ms/step - loss: 1.0004 - accuracy: 0.5953 - val_loss: 0.5053 - val_accuracy: 0.8920&#xA;Epoch 2/10&#xA;27/27 [==============================] - 1s 46ms/step - loss: 0.1368 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000&#xA;Epoch 3/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9116e-04 - val_accuracy: 1.0000&#xA;Epoch 4/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 3.0633e-04 - accuracy: 1.0000 - val_loss: 3.5376e-04 - val_accuracy: 1.0000&#xA;Epoch 5/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.7445e-04 - accuracy: 1.0000 - val_loss: 2.2319e-04 - val_accuracy: 1.0000&#xA;Epoch 6/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.2910e-04 - accuracy: 1.0000 - val_loss: 1.8078e-04 - val_accuracy: 1.0000&#xA;Epoch 7/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.0425e-04 - accuracy: 1.0000 - val_loss: 1.4247e-04 - val_accuracy: 1.0000&#xA;Epoch 8/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 8.6284e-05 - accuracy: 1.0000 - val_loss: 1.2057e-04 - val_accuracy: 1.0000&#xA;Epoch 9/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 7.0085e-05 - accuracy: 1.0000 - val_loss: 9.3485e-05 - val_accuracy: 1.0000&#xA;Epoch 10/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 5.4979e-05 - accuracy: 1.0000 - val_loss: 8.5952e-05 - val_accuracy: 1.0000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Model.fit and model.compile:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;model = model()&#xA;&#xA;model = tf.keras.Model(model_input, outputs)&#xA;  &#xA; model.compile(optimizer='adam',&#xA;              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),&#xA;              metrics=['accuracy'])&#xA;  &#xA;hist = model.fit(&#xA;  train_ds,&#xA;  validation_data=val_ds,&#xA;  epochs=10&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Code to predict new image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def makePrediction(image):&#xA;  from IPython.display import display&#xA;  from PIL import Image&#xA;  from tensorflow.keras.preprocessing import image_dataset_from_directory &#xA;  img = keras.preprocessing.image.load_img(&#xA;  image, target_size=(h, q)&#xA;  )&#xA;  img_array = keras.preprocessing.image.img_to_array(img)&#xA;  img_array = tf.expand_dims(img_array, 0) #Create a batch&#xA; &#xA;  predicts = model.predict(img_array)&#xA;  p = class_names[np.argmax(predicts)]&#xA;  return p&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Going to the &amp;quot;data&amp;quot; directory and using the folders to create a dataset. Each folder is a class label:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.preprocessing import image&#xA;directory_data = &amp;quot;data&amp;quot;&#xA;tf.keras.utils.image_dataset_from_directory(&#xA;    directory_testData, labels='inferred', label_mode='int',&#xA;    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,&#xA;    256), shuffle=True, seed=123, validation_split=0.2, subset=&amp;quot;validation&amp;quot;,&#xA;    interpolation='bilinear', follow_links=False,&#xA;    crop_to_aspect_ratio=False&#xA;)&#xA; &#xA;tf.keras.utils.image_dataset_from_directory(directory_testData, labels='inferred')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Creating dataset and splitting it:&lt;/p&gt;&#xA;&lt;p&gt;Train_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 1284 files for training.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;train_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;  directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;training&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Val_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 321 files for validation.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;val_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;validation&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""128347"" LastActivityDate=""2021-12-07T22:10:34.553"" Title=""Why is val accuracy 100% within 2 epochs and incorrectly predicting new images? (1,000 images per class when training)"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/datascience.stackexchange.com,"  <row Id=""104638"" PostTypeId=""1"" CreationDate=""2021-11-29T22:22:46.813"" Score=""2"" ViewCount=""990"" Body=""&lt;p&gt;My CNN tensorflow model reports 100% validation accuracy within 2 epochs. But it incorrectly predicts on single new images. (It is multiclass problem. I have 3 classes). How to resolve this? Can you please help me understand these epoch results?&lt;/p&gt;&#xA;&lt;p&gt;I have 1,000 images per class that are representative of my testing data. How can validation accuracy reach 1.00 in just the first epoch when I have a dataset of 3,000 images in total, equal amount per class? (I would expect this to start at around 33% percent -- 1/ 3 classes.&lt;/p&gt;&#xA;&lt;p&gt;Attempts:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I tried to ensure that I have correctly split my dataset into training and validation.&lt;/li&gt;&#xA;&lt;li&gt;I understand overfitting can be a problem. I've added a dropout layer to try to solve this potential problem. From this question&lt;a href=&quot;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&quot;&gt;https://ai.stackexchange.com/questions/5318/what-to-do-if-cnn-cannot-overfit-a-training-set-on-adding-dropout/23425&lt;/a&gt; I learned that a &amp;quot;model is over-fitting if during training your training loss continues to decrease but (in the later epochs) your validation loss begins to increase. That means the model can not generalize well to images it has not previously encountered.&amp;quot; I don't believe my model is overfitting based on this description. (My model reports both high training and high validation accuracy. If my model was overfitting I'd expect high training accuracy and low validation accuracy.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;My model:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def model():&#xA;  model_input = tf.keras.layers.Input(shape=(h, w, 3)) &#xA;  x = tf.keras.layers.Rescaling(rescale_factor)(model_input) &#xA;  x = tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same')(x)&#xA;  x = tf.keras.layers.Dropout(.5)(x)&#xA;  x = tf.keras.layers.MaxPooling2D()(x) &#xA;  x = tf.keras.layers.Flatten()(x)&#xA;  x = tf.keras.layers.Dense(128, activation='relu')(x)&#xA;  outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Epoch results:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Epoch 1/10&#xA;/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: &amp;quot;`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;27/27 [==============================] - 13s 124ms/step - loss: 1.0004 - accuracy: 0.5953 - val_loss: 0.5053 - val_accuracy: 0.8920&#xA;Epoch 2/10&#xA;27/27 [==============================] - 1s 46ms/step - loss: 0.1368 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000&#xA;Epoch 3/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.9116e-04 - val_accuracy: 1.0000&#xA;Epoch 4/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 3.0633e-04 - accuracy: 1.0000 - val_loss: 3.5376e-04 - val_accuracy: 1.0000&#xA;Epoch 5/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.7445e-04 - accuracy: 1.0000 - val_loss: 2.2319e-04 - val_accuracy: 1.0000&#xA;Epoch 6/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.2910e-04 - accuracy: 1.0000 - val_loss: 1.8078e-04 - val_accuracy: 1.0000&#xA;Epoch 7/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 1.0425e-04 - accuracy: 1.0000 - val_loss: 1.4247e-04 - val_accuracy: 1.0000&#xA;Epoch 8/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 8.6284e-05 - accuracy: 1.0000 - val_loss: 1.2057e-04 - val_accuracy: 1.0000&#xA;Epoch 9/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 7.0085e-05 - accuracy: 1.0000 - val_loss: 9.3485e-05 - val_accuracy: 1.0000&#xA;Epoch 10/10&#xA;27/27 [==============================] - 1s 42ms/step - loss: 5.4979e-05 - accuracy: 1.0000 - val_loss: 8.5952e-05 - val_accuracy: 1.0000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Model.fit and model.compile:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;model = model()&#xA;&#xA;model = tf.keras.Model(model_input, outputs)&#xA;  &#xA; model.compile(optimizer='adam',&#xA;              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),&#xA;              metrics=['accuracy'])&#xA;  &#xA;hist = model.fit(&#xA;  train_ds,&#xA;  validation_data=val_ds,&#xA;  epochs=10&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Code to predict new image:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def makePrediction(image):&#xA;  from IPython.display import display&#xA;  from PIL import Image&#xA;  from tensorflow.keras.preprocessing import image_dataset_from_directory &#xA;  img = keras.preprocessing.image.load_img(&#xA;  image, target_size=(h, q)&#xA;  )&#xA;  img_array = keras.preprocessing.image.img_to_array(img)&#xA;  img_array = tf.expand_dims(img_array, 0) #Create a batch&#xA; &#xA;  predicts = model.predict(img_array)&#xA;  p = class_names[np.argmax(predicts)]&#xA;  return p&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Going to the &amp;quot;data&amp;quot; directory and using the folders to create a dataset. Each folder is a class label:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.preprocessing import image&#xA;directory_data = &amp;quot;data&amp;quot;&#xA;tf.keras.utils.image_dataset_from_directory(&#xA;    directory_testData, labels='inferred', label_mode='int',&#xA;    class_names=None, color_mode='rgb', batch_size=32, image_size=(256,&#xA;    256), shuffle=True, seed=123, validation_split=0.2, subset=&amp;quot;validation&amp;quot;,&#xA;    interpolation='bilinear', follow_links=False,&#xA;    crop_to_aspect_ratio=False&#xA;)&#xA; &#xA;tf.keras.utils.image_dataset_from_directory(directory_testData, labels='inferred')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Creating dataset and splitting it:&lt;/p&gt;&#xA;&lt;p&gt;Train_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 1284 files for training.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;train_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;  directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;training&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Val_ds code: (Output: Found 1605 files belonging to 3 classes.&#xA;Using 321 files for validation.)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;val_ds = tf.keras.preprocessing.image_dataset_from_directory(&#xA;directory_data = &amp;quot;data&amp;quot;,&#xA;  validation_split=0.2,&#xA;  subset=&amp;quot;validation&amp;quot;,&#xA;  seed=123,&#xA;  image_size=(h, w),&#xA;  batch_size=batch_size)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""128347"" LastActivityDate=""2021-12-07T22:10:34.553"" Title=""Why is val accuracy 100% within 2 epochs and incorrectly predicting new images? (1,000 images per class when training)"" Tags=""&lt;machine-learning&gt;&lt;python&gt;&lt;deep-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/dsp.stackexchange.com,"  <row Id=""73060"" PostTypeId=""1"" AcceptedAnswerId=""73116"" CreationDate=""2021-02-06T11:52:42.537"" Score=""3"" ViewCount=""591"" Body=""&lt;p&gt;I am working with &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function&quot; rel=&quot;nofollow noreferrer&quot;&gt;HRIR&lt;/a&gt; filters, in particular I am trying to interpolate them.&#xA;One commod method in the literature to perform interpolation of HRIR is to use the minimum-phase decomposition and interpolate separately the minimum-phase part of the filter and then the all pass part.&lt;/p&gt;&#xA;&lt;p&gt;You can find more here: &lt;a href=&quot;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;What I am trying to do is to convert my arbitrary HRIR FIR into a minimum phase one. In particular, I am working in Python and I have been using this &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.minimum_phase.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;function&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However while performing the conversion from the original FIR into the minimum-phase one I get the following error:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeWarning: h does not appear to by symmetric, conversion may fail&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;With the following code:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;min_phase_HRIR_0 = minimum_phase(hrtf[0], 'hilbert')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Where &lt;code&gt;hrtf[0]&lt;/code&gt; is a 256 long FIR extracted from a HRIR database (in particular the &lt;a href=&quot;https://depositonce.tu-berlin.de/handle/11303/9429&quot; rel=&quot;nofollow noreferrer&quot;&gt;HUTUBS database&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;If I plot the magnitude  of my original FIR and the minimum-phase one I get the following plot:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; alt=&quot;Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Which is obviously wrong since I am expecting the min phase and the original FIR to have the same magnitude spectrum.&lt;/p&gt;&#xA;&lt;p&gt;I guess there is something wrong with the Scipy function. Is there anything for Python that converts a FIR into a minimum-phase version?&lt;/p&gt;&#xA;"" OwnerUserId=""41338"" LastActivityDate=""2021-02-09T15:39:23.303"" Title=""Compute minimum phase version of a FIR"" Tags=""&lt;python&gt;&lt;scipy&gt;&lt;decomposition&gt;&lt;minimum-phase&gt;&lt;hrtf&gt;"" AnswerCount=""1"" CommentCount=""12"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/dsp.stackexchange.com,"  <row Id=""73060"" PostTypeId=""1"" AcceptedAnswerId=""73116"" CreationDate=""2021-02-06T11:52:42.537"" Score=""3"" ViewCount=""591"" Body=""&lt;p&gt;I am working with &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function&quot; rel=&quot;nofollow noreferrer&quot;&gt;HRIR&lt;/a&gt; filters, in particular I am trying to interpolate them.&#xA;One commod method in the literature to perform interpolation of HRIR is to use the minimum-phase decomposition and interpolate separately the minimum-phase part of the filter and then the all pass part.&lt;/p&gt;&#xA;&lt;p&gt;You can find more here: &lt;a href=&quot;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;What I am trying to do is to convert my arbitrary HRIR FIR into a minimum phase one. In particular, I am working in Python and I have been using this &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.minimum_phase.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;function&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However while performing the conversion from the original FIR into the minimum-phase one I get the following error:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeWarning: h does not appear to by symmetric, conversion may fail&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;With the following code:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;min_phase_HRIR_0 = minimum_phase(hrtf[0], 'hilbert')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Where &lt;code&gt;hrtf[0]&lt;/code&gt; is a 256 long FIR extracted from a HRIR database (in particular the &lt;a href=&quot;https://depositonce.tu-berlin.de/handle/11303/9429&quot; rel=&quot;nofollow noreferrer&quot;&gt;HUTUBS database&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;If I plot the magnitude  of my original FIR and the minimum-phase one I get the following plot:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; alt=&quot;Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Which is obviously wrong since I am expecting the min phase and the original FIR to have the same magnitude spectrum.&lt;/p&gt;&#xA;&lt;p&gt;I guess there is something wrong with the Scipy function. Is there anything for Python that converts a FIR into a minimum-phase version?&lt;/p&gt;&#xA;"" OwnerUserId=""41338"" LastActivityDate=""2021-02-09T15:39:23.303"" Title=""Compute minimum phase version of a FIR"" Tags=""&lt;python&gt;&lt;scipy&gt;&lt;decomposition&gt;&lt;minimum-phase&gt;&lt;hrtf&gt;"" AnswerCount=""1"" CommentCount=""12"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/dsp.stackexchange.com,"  <row Id=""73060"" PostTypeId=""1"" AcceptedAnswerId=""73116"" CreationDate=""2021-02-06T11:52:42.537"" Score=""3"" ViewCount=""591"" Body=""&lt;p&gt;I am working with &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function&quot; rel=&quot;nofollow noreferrer&quot;&gt;HRIR&lt;/a&gt; filters, in particular I am trying to interpolate them.&#xA;One commod method in the literature to perform interpolation of HRIR is to use the minimum-phase decomposition and interpolate separately the minimum-phase part of the filter and then the all pass part.&lt;/p&gt;&#xA;&lt;p&gt;You can find more here: &lt;a href=&quot;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;What I am trying to do is to convert my arbitrary HRIR FIR into a minimum phase one. In particular, I am working in Python and I have been using this &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.minimum_phase.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;function&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However while performing the conversion from the original FIR into the minimum-phase one I get the following error:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeWarning: h does not appear to by symmetric, conversion may fail&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;With the following code:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;min_phase_HRIR_0 = minimum_phase(hrtf[0], 'hilbert')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Where &lt;code&gt;hrtf[0]&lt;/code&gt; is a 256 long FIR extracted from a HRIR database (in particular the &lt;a href=&quot;https://depositonce.tu-berlin.de/handle/11303/9429&quot; rel=&quot;nofollow noreferrer&quot;&gt;HUTUBS database&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;If I plot the magnitude  of my original FIR and the minimum-phase one I get the following plot:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; alt=&quot;Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Which is obviously wrong since I am expecting the min phase and the original FIR to have the same magnitude spectrum.&lt;/p&gt;&#xA;&lt;p&gt;I guess there is something wrong with the Scipy function. Is there anything for Python that converts a FIR into a minimum-phase version?&lt;/p&gt;&#xA;"" OwnerUserId=""41338"" LastActivityDate=""2021-02-09T15:39:23.303"" Title=""Compute minimum phase version of a FIR"" Tags=""&lt;python&gt;&lt;scipy&gt;&lt;decomposition&gt;&lt;minimum-phase&gt;&lt;hrtf&gt;"" AnswerCount=""1"" CommentCount=""12"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/dsp.stackexchange.com,"  <row Id=""73060"" PostTypeId=""1"" AcceptedAnswerId=""73116"" CreationDate=""2021-02-06T11:52:42.537"" Score=""3"" ViewCount=""591"" Body=""&lt;p&gt;I am working with &lt;a href=&quot;https://en.wikipedia.org/wiki/Head-related_transfer_function&quot; rel=&quot;nofollow noreferrer&quot;&gt;HRIR&lt;/a&gt; filters, in particular I am trying to interpolate them.&#xA;One commod method in the literature to perform interpolation of HRIR is to use the minimum-phase decomposition and interpolate separately the minimum-phase part of the filter and then the all pass part.&lt;/p&gt;&#xA;&lt;p&gt;You can find more here: &lt;a href=&quot;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.researchgate.net/publication/277879431_On_the_Minimum-Phase_Nature_of_Head-Related_Transfer_Functions&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;What I am trying to do is to convert my arbitrary HRIR FIR into a minimum phase one. In particular, I am working in Python and I have been using this &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.minimum_phase.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;function&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;However while performing the conversion from the original FIR into the minimum-phase one I get the following error:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;RuntimeWarning: h does not appear to by symmetric, conversion may fail&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;With the following code:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;min_phase_HRIR_0 = minimum_phase(hrtf[0], 'hilbert')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Where &lt;code&gt;hrtf[0]&lt;/code&gt; is a 256 long FIR extracted from a HRIR database (in particular the &lt;a href=&quot;https://depositonce.tu-berlin.de/handle/11303/9429&quot; rel=&quot;nofollow noreferrer&quot;&gt;HUTUBS database&lt;/a&gt;).&lt;/p&gt;&#xA;&lt;p&gt;If I plot the magnitude  of my original FIR and the minimum-phase one I get the following plot:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZDO3F.png&quot; alt=&quot;Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Which is obviously wrong since I am expecting the min phase and the original FIR to have the same magnitude spectrum.&lt;/p&gt;&#xA;&lt;p&gt;I guess there is something wrong with the Scipy function. Is there anything for Python that converts a FIR into a minimum-phase version?&lt;/p&gt;&#xA;"" OwnerUserId=""41338"" LastActivityDate=""2021-02-09T15:39:23.303"" Title=""Compute minimum phase version of a FIR"" Tags=""&lt;python&gt;&lt;scipy&gt;&lt;decomposition&gt;&lt;minimum-phase&gt;&lt;hrtf&gt;"" AnswerCount=""1"" CommentCount=""12"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""9625"" PostTypeId=""1"" CreationDate=""2016-05-11T13:45:45.327"" Score=""13"" ViewCount=""803"" Body=""&lt;p&gt;Una vez tengo la &lt;strong&gt;API&lt;/strong&gt; funcionando, al entrar en el entorno grfico (&lt;strong&gt;domain.com/API&lt;/strong&gt;) de pruebas me sale sin estilos, es decir que todos los &lt;a href=&quot;/questions/tagged/css&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;css&amp;quot;&quot; rel=&quot;tag&quot;&gt;css&lt;/a&gt; y los &lt;a href=&quot;/questions/tagged/javascript&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;javascript&amp;quot;&quot; rel=&quot;tag&quot;&gt;javascript&lt;/a&gt; no los puede encontrar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En el &lt;code&gt;settings.py&lt;/code&gt; he aadido los &lt;code&gt;STATIC_URL&lt;/code&gt; y &lt;code&gt;STATIC_ROOT&lt;/code&gt; despus he hecho el &lt;code&gt;python manage.py collectstatic&lt;/code&gt; y me ha puesto todos los de &lt;code&gt;admin&lt;/code&gt; dentro del directorio, pero todo lo que son de &lt;code&gt;rest_framework&lt;/code&gt; no. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;He visto de otro post en Stack Overflow que deca de aadir &lt;code&gt;STATICFILES_DIRS&lt;/code&gt; pero tampoco me ha funcionado. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidentemente he hecho un &lt;code&gt;service gunicorn restart&lt;/code&gt; despus de cada cambio. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alguien podra iluminarme?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;STATIC_URL = '/static/'&#xA;STATIC_ROOT = '/home/django/django_project/static/'&#xA;&#xA;MEDIA_URL = '/media/'&#xA;MEDIA_ROOT = '/home/django/django_project/media/'&#xA;&#xA;STATICFILES_DIRS = (&#xA;    os.path.join(BASE_DIR, '/home/django/django_project/'),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como cargar todos esos ficheros?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ERROR&lt;/strong&gt;: Aqu dejo uno de los errores para que veis la ruta a la que busca. La ruta a la que busca el navegador es: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;http://sub.domain.com/static/rest_framework/css/default.css&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;El caso es que dentro de mi carpeta &lt;code&gt;static&lt;/code&gt; estn todos los ficheros.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; Al ejecutar el comando que dice @csar  me sale el siguiente mensaje:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root@machine:/home/django/django_project# python manage.py findstatic rest_framework/css/default.css&#xA;&#xA;/home/django/django_project/django_project/urls.py:33: RemovedInDjango110Warning: Support for string view arguments to url() is deprecated and will be removed in Django 1.10 (got my_app.views.home). Pass the callable instead.&#xA;  url(r'^$', 'my_app.views.home', name='home'),&#xA;&#xA;/home/django/django_project/django_project/urls.py:37: RemovedInDjango110Warning: django.conf.urls.patterns() is deprecated and will be removed in Django 1.10. Update your urlpatterns to be a list of django.conf.urls.url() instances instead.&#xA;  url(r'^API', include(router.urls)),&#xA;&#xA;System check identified some issues:&#xA;&#xA;WARNINGS:&#xA;?: (1_8.W001) The standalone TEMPLATE_* settings were deprecated in Django 1.8 and the TEMPLATES dictionary takes precedence. You must put the values of the following settings into your default TEMPLATES dict: TEMPLATE_DEBUG.&#xA;&#xA;Found 'rest_framework/css/default.css' here:&#xA;  /usr/local/lib/python2.7/dist-packages/rest_framework/static/rest_framework/css/default.css&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como hacerlo? Alguna alma caritativa? He probado de todo y no hallo el modo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; He borrado toda la carpeta &lt;code&gt;static&lt;/code&gt; y he vuelto a hacer un &lt;code&gt;collectstatic&lt;/code&gt; de nuevo y me sigue pasando lo mismo. (No encuentra las &lt;code&gt;img&lt;/code&gt; de los boolean &lt;code&gt;/static/admin/img/icon-yes.svg&lt;/code&gt; y estn en dicho directorio) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RnhfT.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RnhfT.png&quot; alt=&quot;Ejemplo del admin sin las imgenes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/jImf9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/jImf9.png&quot; alt=&quot;Ejemplo de la api todo perfecto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""5972"" LastEditorUserId=""106915"" LastEditDate=""2020-02-26T02:43:03.663"" LastActivityDate=""2020-02-26T02:43:03.663"" Title=""Django Rest Framework sin estilos ni scripts"" Tags=""&lt;python&gt;&lt;django&gt;&lt;djangorestframework&gt;"" AnswerCount=""2"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""9625"" PostTypeId=""1"" CreationDate=""2016-05-11T13:45:45.327"" Score=""13"" ViewCount=""803"" Body=""&lt;p&gt;Una vez tengo la &lt;strong&gt;API&lt;/strong&gt; funcionando, al entrar en el entorno grfico (&lt;strong&gt;domain.com/API&lt;/strong&gt;) de pruebas me sale sin estilos, es decir que todos los &lt;a href=&quot;/questions/tagged/css&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;css&amp;quot;&quot; rel=&quot;tag&quot;&gt;css&lt;/a&gt; y los &lt;a href=&quot;/questions/tagged/javascript&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;javascript&amp;quot;&quot; rel=&quot;tag&quot;&gt;javascript&lt;/a&gt; no los puede encontrar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En el &lt;code&gt;settings.py&lt;/code&gt; he aadido los &lt;code&gt;STATIC_URL&lt;/code&gt; y &lt;code&gt;STATIC_ROOT&lt;/code&gt; despus he hecho el &lt;code&gt;python manage.py collectstatic&lt;/code&gt; y me ha puesto todos los de &lt;code&gt;admin&lt;/code&gt; dentro del directorio, pero todo lo que son de &lt;code&gt;rest_framework&lt;/code&gt; no. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;He visto de otro post en Stack Overflow que deca de aadir &lt;code&gt;STATICFILES_DIRS&lt;/code&gt; pero tampoco me ha funcionado. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidentemente he hecho un &lt;code&gt;service gunicorn restart&lt;/code&gt; despus de cada cambio. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alguien podra iluminarme?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;STATIC_URL = '/static/'&#xA;STATIC_ROOT = '/home/django/django_project/static/'&#xA;&#xA;MEDIA_URL = '/media/'&#xA;MEDIA_ROOT = '/home/django/django_project/media/'&#xA;&#xA;STATICFILES_DIRS = (&#xA;    os.path.join(BASE_DIR, '/home/django/django_project/'),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como cargar todos esos ficheros?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ERROR&lt;/strong&gt;: Aqu dejo uno de los errores para que veis la ruta a la que busca. La ruta a la que busca el navegador es: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;http://sub.domain.com/static/rest_framework/css/default.css&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;El caso es que dentro de mi carpeta &lt;code&gt;static&lt;/code&gt; estn todos los ficheros.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; Al ejecutar el comando que dice @csar  me sale el siguiente mensaje:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root@machine:/home/django/django_project# python manage.py findstatic rest_framework/css/default.css&#xA;&#xA;/home/django/django_project/django_project/urls.py:33: RemovedInDjango110Warning: Support for string view arguments to url() is deprecated and will be removed in Django 1.10 (got my_app.views.home). Pass the callable instead.&#xA;  url(r'^$', 'my_app.views.home', name='home'),&#xA;&#xA;/home/django/django_project/django_project/urls.py:37: RemovedInDjango110Warning: django.conf.urls.patterns() is deprecated and will be removed in Django 1.10. Update your urlpatterns to be a list of django.conf.urls.url() instances instead.&#xA;  url(r'^API', include(router.urls)),&#xA;&#xA;System check identified some issues:&#xA;&#xA;WARNINGS:&#xA;?: (1_8.W001) The standalone TEMPLATE_* settings were deprecated in Django 1.8 and the TEMPLATES dictionary takes precedence. You must put the values of the following settings into your default TEMPLATES dict: TEMPLATE_DEBUG.&#xA;&#xA;Found 'rest_framework/css/default.css' here:&#xA;  /usr/local/lib/python2.7/dist-packages/rest_framework/static/rest_framework/css/default.css&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como hacerlo? Alguna alma caritativa? He probado de todo y no hallo el modo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; He borrado toda la carpeta &lt;code&gt;static&lt;/code&gt; y he vuelto a hacer un &lt;code&gt;collectstatic&lt;/code&gt; de nuevo y me sigue pasando lo mismo. (No encuentra las &lt;code&gt;img&lt;/code&gt; de los boolean &lt;code&gt;/static/admin/img/icon-yes.svg&lt;/code&gt; y estn en dicho directorio) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RnhfT.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RnhfT.png&quot; alt=&quot;Ejemplo del admin sin las imgenes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/jImf9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/jImf9.png&quot; alt=&quot;Ejemplo de la api todo perfecto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""5972"" LastEditorUserId=""106915"" LastEditDate=""2020-02-26T02:43:03.663"" LastActivityDate=""2020-02-26T02:43:03.663"" Title=""Django Rest Framework sin estilos ni scripts"" Tags=""&lt;python&gt;&lt;django&gt;&lt;djangorestframework&gt;"" AnswerCount=""2"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""9625"" PostTypeId=""1"" CreationDate=""2016-05-11T13:45:45.327"" Score=""13"" ViewCount=""803"" Body=""&lt;p&gt;Una vez tengo la &lt;strong&gt;API&lt;/strong&gt; funcionando, al entrar en el entorno grfico (&lt;strong&gt;domain.com/API&lt;/strong&gt;) de pruebas me sale sin estilos, es decir que todos los &lt;a href=&quot;/questions/tagged/css&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;css&amp;quot;&quot; rel=&quot;tag&quot;&gt;css&lt;/a&gt; y los &lt;a href=&quot;/questions/tagged/javascript&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;javascript&amp;quot;&quot; rel=&quot;tag&quot;&gt;javascript&lt;/a&gt; no los puede encontrar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En el &lt;code&gt;settings.py&lt;/code&gt; he aadido los &lt;code&gt;STATIC_URL&lt;/code&gt; y &lt;code&gt;STATIC_ROOT&lt;/code&gt; despus he hecho el &lt;code&gt;python manage.py collectstatic&lt;/code&gt; y me ha puesto todos los de &lt;code&gt;admin&lt;/code&gt; dentro del directorio, pero todo lo que son de &lt;code&gt;rest_framework&lt;/code&gt; no. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;He visto de otro post en Stack Overflow que deca de aadir &lt;code&gt;STATICFILES_DIRS&lt;/code&gt; pero tampoco me ha funcionado. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidentemente he hecho un &lt;code&gt;service gunicorn restart&lt;/code&gt; despus de cada cambio. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alguien podra iluminarme?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;STATIC_URL = '/static/'&#xA;STATIC_ROOT = '/home/django/django_project/static/'&#xA;&#xA;MEDIA_URL = '/media/'&#xA;MEDIA_ROOT = '/home/django/django_project/media/'&#xA;&#xA;STATICFILES_DIRS = (&#xA;    os.path.join(BASE_DIR, '/home/django/django_project/'),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como cargar todos esos ficheros?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ERROR&lt;/strong&gt;: Aqu dejo uno de los errores para que veis la ruta a la que busca. La ruta a la que busca el navegador es: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;http://sub.domain.com/static/rest_framework/css/default.css&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;El caso es que dentro de mi carpeta &lt;code&gt;static&lt;/code&gt; estn todos los ficheros.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; Al ejecutar el comando que dice @csar  me sale el siguiente mensaje:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root@machine:/home/django/django_project# python manage.py findstatic rest_framework/css/default.css&#xA;&#xA;/home/django/django_project/django_project/urls.py:33: RemovedInDjango110Warning: Support for string view arguments to url() is deprecated and will be removed in Django 1.10 (got my_app.views.home). Pass the callable instead.&#xA;  url(r'^$', 'my_app.views.home', name='home'),&#xA;&#xA;/home/django/django_project/django_project/urls.py:37: RemovedInDjango110Warning: django.conf.urls.patterns() is deprecated and will be removed in Django 1.10. Update your urlpatterns to be a list of django.conf.urls.url() instances instead.&#xA;  url(r'^API', include(router.urls)),&#xA;&#xA;System check identified some issues:&#xA;&#xA;WARNINGS:&#xA;?: (1_8.W001) The standalone TEMPLATE_* settings were deprecated in Django 1.8 and the TEMPLATES dictionary takes precedence. You must put the values of the following settings into your default TEMPLATES dict: TEMPLATE_DEBUG.&#xA;&#xA;Found 'rest_framework/css/default.css' here:&#xA;  /usr/local/lib/python2.7/dist-packages/rest_framework/static/rest_framework/css/default.css&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como hacerlo? Alguna alma caritativa? He probado de todo y no hallo el modo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; He borrado toda la carpeta &lt;code&gt;static&lt;/code&gt; y he vuelto a hacer un &lt;code&gt;collectstatic&lt;/code&gt; de nuevo y me sigue pasando lo mismo. (No encuentra las &lt;code&gt;img&lt;/code&gt; de los boolean &lt;code&gt;/static/admin/img/icon-yes.svg&lt;/code&gt; y estn en dicho directorio) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RnhfT.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RnhfT.png&quot; alt=&quot;Ejemplo del admin sin las imgenes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/jImf9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/jImf9.png&quot; alt=&quot;Ejemplo de la api todo perfecto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""5972"" LastEditorUserId=""106915"" LastEditDate=""2020-02-26T02:43:03.663"" LastActivityDate=""2020-02-26T02:43:03.663"" Title=""Django Rest Framework sin estilos ni scripts"" Tags=""&lt;python&gt;&lt;django&gt;&lt;djangorestframework&gt;"" AnswerCount=""2"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""9625"" PostTypeId=""1"" CreationDate=""2016-05-11T13:45:45.327"" Score=""13"" ViewCount=""803"" Body=""&lt;p&gt;Una vez tengo la &lt;strong&gt;API&lt;/strong&gt; funcionando, al entrar en el entorno grfico (&lt;strong&gt;domain.com/API&lt;/strong&gt;) de pruebas me sale sin estilos, es decir que todos los &lt;a href=&quot;/questions/tagged/css&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;css&amp;quot;&quot; rel=&quot;tag&quot;&gt;css&lt;/a&gt; y los &lt;a href=&quot;/questions/tagged/javascript&quot; class=&quot;post-tag&quot; title=&quot;mostrar preguntas con la etiqueta &amp;quot;javascript&amp;quot;&quot; rel=&quot;tag&quot;&gt;javascript&lt;/a&gt; no los puede encontrar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En el &lt;code&gt;settings.py&lt;/code&gt; he aadido los &lt;code&gt;STATIC_URL&lt;/code&gt; y &lt;code&gt;STATIC_ROOT&lt;/code&gt; despus he hecho el &lt;code&gt;python manage.py collectstatic&lt;/code&gt; y me ha puesto todos los de &lt;code&gt;admin&lt;/code&gt; dentro del directorio, pero todo lo que son de &lt;code&gt;rest_framework&lt;/code&gt; no. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;He visto de otro post en Stack Overflow que deca de aadir &lt;code&gt;STATICFILES_DIRS&lt;/code&gt; pero tampoco me ha funcionado. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidentemente he hecho un &lt;code&gt;service gunicorn restart&lt;/code&gt; despus de cada cambio. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alguien podra iluminarme?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;STATIC_URL = '/static/'&#xA;STATIC_ROOT = '/home/django/django_project/static/'&#xA;&#xA;MEDIA_URL = '/media/'&#xA;MEDIA_ROOT = '/home/django/django_project/media/'&#xA;&#xA;STATICFILES_DIRS = (&#xA;    os.path.join(BASE_DIR, '/home/django/django_project/'),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como cargar todos esos ficheros?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ERROR&lt;/strong&gt;: Aqu dejo uno de los errores para que veis la ruta a la que busca. La ruta a la que busca el navegador es: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;http://sub.domain.com/static/rest_framework/css/default.css&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;El caso es que dentro de mi carpeta &lt;code&gt;static&lt;/code&gt; estn todos los ficheros.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; Al ejecutar el comando que dice @csar  me sale el siguiente mensaje:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root@machine:/home/django/django_project# python manage.py findstatic rest_framework/css/default.css&#xA;&#xA;/home/django/django_project/django_project/urls.py:33: RemovedInDjango110Warning: Support for string view arguments to url() is deprecated and will be removed in Django 1.10 (got my_app.views.home). Pass the callable instead.&#xA;  url(r'^$', 'my_app.views.home', name='home'),&#xA;&#xA;/home/django/django_project/django_project/urls.py:37: RemovedInDjango110Warning: django.conf.urls.patterns() is deprecated and will be removed in Django 1.10. Update your urlpatterns to be a list of django.conf.urls.url() instances instead.&#xA;  url(r'^API', include(router.urls)),&#xA;&#xA;System check identified some issues:&#xA;&#xA;WARNINGS:&#xA;?: (1_8.W001) The standalone TEMPLATE_* settings were deprecated in Django 1.8 and the TEMPLATES dictionary takes precedence. You must put the values of the following settings into your default TEMPLATES dict: TEMPLATE_DEBUG.&#xA;&#xA;Found 'rest_framework/css/default.css' here:&#xA;  /usr/local/lib/python2.7/dist-packages/rest_framework/static/rest_framework/css/default.css&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nadie sabe como hacerlo? Alguna alma caritativa? He probado de todo y no hallo el modo.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITO:&lt;/strong&gt; He borrado toda la carpeta &lt;code&gt;static&lt;/code&gt; y he vuelto a hacer un &lt;code&gt;collectstatic&lt;/code&gt; de nuevo y me sigue pasando lo mismo. (No encuentra las &lt;code&gt;img&lt;/code&gt; de los boolean &lt;code&gt;/static/admin/img/icon-yes.svg&lt;/code&gt; y estn en dicho directorio) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RnhfT.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RnhfT.png&quot; alt=&quot;Ejemplo del admin sin las imgenes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/jImf9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/jImf9.png&quot; alt=&quot;Ejemplo de la api todo perfecto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""5972"" LastEditorUserId=""106915"" LastEditDate=""2020-02-26T02:43:03.663"" LastActivityDate=""2020-02-26T02:43:03.663"" Title=""Django Rest Framework sin estilos ni scripts"" Tags=""&lt;python&gt;&lt;django&gt;&lt;djangorestframework&gt;"" AnswerCount=""2"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""59880"" PostTypeId=""1"" AcceptedAnswerId=""59885"" CreationDate=""2017-04-02T21:10:55.930"" Score=""1"" ViewCount=""221"" Body=""&lt;p&gt;Actualmente al ejecutar el mdulo de CrossValidation y GridSearch, me muestra este error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Program Files\Anaconda2\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.&#xA;&quot;This module will be removed in 0.20.&quot;, DeprecationWarning)&#xA;C:\Program Files\Anaconda2\lib\site-packages\sklearn\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.&#xA;DeprecationWarning)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A qu se debe? y Cmo lo soluciono?&lt;/p&gt;&#xA;"" OwnerUserId=""35153"" LastEditorUserId=""15089"" LastEditDate=""2018-08-05T11:33:23.987"" LastActivityDate=""2018-08-05T11:33:23.987"" Title=""Me muestra un DeprecationWarning al intentar usar cross_validation"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""59880"" PostTypeId=""1"" AcceptedAnswerId=""59885"" CreationDate=""2017-04-02T21:10:55.930"" Score=""1"" ViewCount=""221"" Body=""&lt;p&gt;Actualmente al ejecutar el mdulo de CrossValidation y GridSearch, me muestra este error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Program Files\Anaconda2\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.&#xA;&quot;This module will be removed in 0.20.&quot;, DeprecationWarning)&#xA;C:\Program Files\Anaconda2\lib\site-packages\sklearn\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.&#xA;DeprecationWarning)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A qu se debe? y Cmo lo soluciono?&lt;/p&gt;&#xA;"" OwnerUserId=""35153"" LastEditorUserId=""15089"" LastEditDate=""2018-08-05T11:33:23.987"" LastActivityDate=""2018-08-05T11:33:23.987"" Title=""Me muestra un DeprecationWarning al intentar usar cross_validation"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""59880"" PostTypeId=""1"" AcceptedAnswerId=""59885"" CreationDate=""2017-04-02T21:10:55.930"" Score=""1"" ViewCount=""221"" Body=""&lt;p&gt;Actualmente al ejecutar el mdulo de CrossValidation y GridSearch, me muestra este error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Program Files\Anaconda2\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.&#xA;&quot;This module will be removed in 0.20.&quot;, DeprecationWarning)&#xA;C:\Program Files\Anaconda2\lib\site-packages\sklearn\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.&#xA;DeprecationWarning)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A qu se debe? y Cmo lo soluciono?&lt;/p&gt;&#xA;"" OwnerUserId=""35153"" LastEditorUserId=""15089"" LastEditDate=""2018-08-05T11:33:23.987"" LastActivityDate=""2018-08-05T11:33:23.987"" Title=""Me muestra un DeprecationWarning al intentar usar cross_validation"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""59880"" PostTypeId=""1"" AcceptedAnswerId=""59885"" CreationDate=""2017-04-02T21:10:55.930"" Score=""1"" ViewCount=""221"" Body=""&lt;p&gt;Actualmente al ejecutar el mdulo de CrossValidation y GridSearch, me muestra este error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Program Files\Anaconda2\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.&#xA;&quot;This module will be removed in 0.20.&quot;, DeprecationWarning)&#xA;C:\Program Files\Anaconda2\lib\site-packages\sklearn\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.&#xA;DeprecationWarning)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A qu se debe? y Cmo lo soluciono?&lt;/p&gt;&#xA;"" OwnerUserId=""35153"" LastEditorUserId=""15089"" LastEditDate=""2018-08-05T11:33:23.987"" LastActivityDate=""2018-08-05T11:33:23.987"" Title=""Me muestra un DeprecationWarning al intentar usar cross_validation"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""138575"" PostTypeId=""1"" AcceptedAnswerId=""138625"" CreationDate=""2018-02-14T21:11:30.590"" Score=""1"" ViewCount=""1205"" Body=""&lt;p&gt;Estoy siguiendo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow luego de dos das preparando el entorno en Anaconda finalmente logr correr &lt;code&gt;premade_estimator.py&lt;/code&gt; usando el cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pero cuando trato de correr el mismo cdigo en jupyter obtengo este error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;He tratado de arreglarlo sin exito con estas lineas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cualquier idea ser apreciada! Gracias!&lt;/p&gt;&#xA;"" OwnerUserId=""71262"" LastEditorUserId=""71262"" LastEditDate=""2018-02-14T23:56:28.620"" LastActivityDate=""2018-02-14T23:56:28.620"" Title=""Cmo arreglar ipykernel_launcher.py: error: unrecognized arguments en jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;jupyter&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""138575"" PostTypeId=""1"" AcceptedAnswerId=""138625"" CreationDate=""2018-02-14T21:11:30.590"" Score=""1"" ViewCount=""1205"" Body=""&lt;p&gt;Estoy siguiendo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow luego de dos das preparando el entorno en Anaconda finalmente logr correr &lt;code&gt;premade_estimator.py&lt;/code&gt; usando el cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pero cuando trato de correr el mismo cdigo en jupyter obtengo este error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;He tratado de arreglarlo sin exito con estas lineas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cualquier idea ser apreciada! Gracias!&lt;/p&gt;&#xA;"" OwnerUserId=""71262"" LastEditorUserId=""71262"" LastEditDate=""2018-02-14T23:56:28.620"" LastActivityDate=""2018-02-14T23:56:28.620"" Title=""Cmo arreglar ipykernel_launcher.py: error: unrecognized arguments en jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;jupyter&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""138575"" PostTypeId=""1"" AcceptedAnswerId=""138625"" CreationDate=""2018-02-14T21:11:30.590"" Score=""1"" ViewCount=""1205"" Body=""&lt;p&gt;Estoy siguiendo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow luego de dos das preparando el entorno en Anaconda finalmente logr correr &lt;code&gt;premade_estimator.py&lt;/code&gt; usando el cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pero cuando trato de correr el mismo cdigo en jupyter obtengo este error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;He tratado de arreglarlo sin exito con estas lineas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cualquier idea ser apreciada! Gracias!&lt;/p&gt;&#xA;"" OwnerUserId=""71262"" LastEditorUserId=""71262"" LastEditDate=""2018-02-14T23:56:28.620"" LastActivityDate=""2018-02-14T23:56:28.620"" Title=""Cmo arreglar ipykernel_launcher.py: error: unrecognized arguments en jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;jupyter&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""138575"" PostTypeId=""1"" AcceptedAnswerId=""138625"" CreationDate=""2018-02-14T21:11:30.590"" Score=""1"" ViewCount=""1205"" Body=""&lt;p&gt;Estoy siguiendo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow luego de dos das preparando el entorno en Anaconda finalmente logr correr &lt;code&gt;premade_estimator.py&lt;/code&gt; usando el cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pero cuando trato de correr el mismo cdigo en jupyter obtengo este error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;He tratado de arreglarlo sin exito con estas lineas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cualquier idea ser apreciada! Gracias!&lt;/p&gt;&#xA;"" OwnerUserId=""71262"" LastEditorUserId=""71262"" LastEditDate=""2018-02-14T23:56:28.620"" LastActivityDate=""2018-02-14T23:56:28.620"" Title=""Cmo arreglar ipykernel_launcher.py: error: unrecognized arguments en jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;jupyter&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""208357"" PostTypeId=""1"" CreationDate=""2018-10-26T21:11:35.927"" Score=""0"" ViewCount=""59"" Body=""&lt;p&gt;estoy tratando de hacer clasificacin pero tengo este error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os,cv2&#xA;import numpy as np&#xA;from sklearn.utils import shuffle&#xA;from tensorflow.python.keras.preprocessing.image import ImageDataGenerator&#xA;from sklearn.model_selection import train_test_split&#xA;from keras import backend as k&#xA;k.set_image_dim_ordering('tf')&#xA;from keras.utils import np_utils&#xA;from keras.models import Sequential&#xA;from keras.layers.core import Dense, Dropout, Activation, Flatten&#xA;from keras.layers import Conv2D, MaxPooling2D&#xA;from keras.optimizers import SGD, RMSprop, Adam, &#xA;Adagrad,Adadelta,Adamax,Nadam&#xA;from keras import callbacks&#xA;from keras.models import load_model&#xA;&#xA;img_rows =150&#xA;img_cols=150&#xA;num_epoch=50&#xA;optimizador='Adadelta'&#xA;PATH = os.getcwd()&#xA;data_path = PATH + '\\variedades'&#xA;&#xA;&#xA;&#xA;labels_t=[]&#xA;num_classes=2&#xA;img_data=[]&#xA;input_shape=0&#xA;model=0&#xA;X_train=''&#xA;X_val=''&#xA;y_train=''&#xA;y_val=''&#xA;&#xA;&#xA;def carga_dataset():&#xA;    global img_rows&#xA;    global img_cols&#xA;    global data_path&#xA;    global labels_t&#xA;    global img_data&#xA;&#xA;    img_data_list=[]&#xA;    n_imag=0&#xA;    n_imag_array=[]&#xA;    n_imag_cont=0&#xA;&#xA;    print('ESTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: ' +data_path)&#xA;    data_dir_list = os.listdir(data_path)&#xA;&#xA;&#xA;    for dataset in data_dir_list:&#xA;        img_list=os.listdir(data_path+'/'+dataset)&#xA;        print('CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;'+'{}\n'.format(dataset))&#xA;        for img in img_list:&#xA;            n_imag+=1&#xA;            input_img=cv2.imread(data_path+'/'+dataset+'/'+img)&#xA;            input_img=cv2.cvtColor(input_img,cv2.COLOR_BGR2GRAY)&#xA;            input_img_resize=cv2.resize(input_img,(img_rows,img_cols))&#xA;            img_data_list.append(input_img_resize)&#xA;            n_imag_array.append(n_imag)&#xA;            n_imag_cont+=1&#xA;&#xA;    img_data = np.array(img_data_list)&#xA;    img_data = img_data.astype('float32')&#xA;    img_data/= 255&#xA;    img_data = np.expand_dims(img_data,axis=4)&#xA;&#xA;    num_of_samples = img_data.shape[0]&#xA;    labels = np.ones((num_of_samples,),dtype='int64')&#xA;&#xA;    labels[0:n_imag_array[0]]=0&#xA;    labels[n_imag_array[0]]:n_imag_array[1]=1&#xA;&#xA;    labels_t = labels&#xA;&#xA;&#xA;def clasificacion_imagenes(labels_t,num_classes):&#xA;    print('SE HAN CARGADO',end=' ')&#xA;    print (len(labels_t),end=' ')&#xA;    print('IMAGENES EN TOTAL')&#xA;&#xA;    Y = np_utils.to_categorical(labels_t,num_classes)&#xA;&#xA;&#xA;    x,y= shuffle(img_data,Y,random_state=2)&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(x),end='')&#xA;    print('VECTORES')&#xA;    print('SE HAN GENERADO',end=&quot; &quot;)&#xA;    print(len(y),end=&quot; &quot;)&#xA;    print('ETIQUETAS')&#xA;&#xA;    global X_train,X_val,y_train,y_val&#xA;    X_train,X_val,y_val,y_train=train_test_split(x,y,test_size=0.2, random_state=2)&#xA;    global input_shape&#xA;    input_shape=img_data[0].shape&#xA;&#xA;def crear_modelo(input_shape):&#xA;    global model&#xA;    global X_train, X_val, y_train, y_val&#xA;    global optimizador&#xA;&#xA;    model = Sequential()&#xA;    model.add(Conv2D(32,(3,3),padding='same',input_shape=input_shape))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Conv2D(32,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Conv2D(64,(3,3)))&#xA;    model.add(Activation('relu'))&#xA;    model.add(MaxPooling2D(pool_size=(2,2)))&#xA;    model.add(Dropout(0.5))&#xA;&#xA;    model.add(Flatten())&#xA;    model.add(Dense(64))&#xA;    model.add(Activation('relu'))&#xA;    model.add(Dropout(0.5))&#xA;    model.add(Dense(num_classes))&#xA;    model.add(Activation('softmax')) &#xA;model.compile(loss='categorical_crossentropy',optimizer=optimizador,metrics=[&quot;accuracy&quot;])&#xA;&#xA;tbCallBack =callbacks.TensorBoard(log_dir='./log'+optimizador+'-gpu-todo',histogram_freq=1,write_graph=True,write_images=False)&#xA;tbCallBack.set_model(model)&#xA;&#xA;print('REALIZANDO ENTRENAMIENTO')&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA;&#xA;def guardar_modelo(model):&#xA;    print('GUARDANDO MODELO')&#xA;    model.save('model.hdf5')&#xA;    loaded_model=load_model('model.hdf5')&#xA;    print('MODELO GUARDADO')&#xA;&#xA;carga_dataset()&#xA;clasificacion_imagenes(labels_t,num_classes)&#xA;crear_modelo(input_shape)&#xA;guardar_modelo(model)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;y este es el error que obtengo&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site- &#xA;packages\sklearn\externals\joblib\externals\cloudpickle\cloudpickle.py:47: &#xA;DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses&#xA;import imp&#xA;Using TensorFlow backend.&#xA;EXTRAYENDO DATASET DE LA SIGUIENTE DIRECCION: C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\variedades&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;gato&#xA;&#xA;CARGANDO IMAGENES DE LA CARPERTA -&amp;gt;perro&#xA;&#xA;SE HAN CARGADO 8000 IMAGENES EN TOTAL&#xA;SE HAN GENERADO 8000VECTORES&#xA;SE HAN GENERADO 8000 ETIQUETAS&#xA;REALIZANDO ENTRENAMIENTO&#xA;Traceback (most recent call last):&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 141, in &amp;lt;module&amp;gt;&#xA;crear_modelo(input_shape)&#xA; File &quot;C:\Users\Angelo\Desktop\app procesamiento de imagenes\2\aprendizaje.py&quot;, line 131, in crear_modelo&#xA;hist = model.fit(X_train,y_train,batch_size=4,epochs=num_epoch,verbose=1,validation_data=(X_val,y_val),callbacks=[tbCallBack])&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 952, in fit&#xA;batch_size=batch_size)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py&quot;, line 804, in _standardize_user_data&#xA;check_array_length_consistency(x, y, sample_weights)&#xA; File &quot;C:\Users\Angelo\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training_utils.py&quot;, line 237, in check_array_length_consistency&#xA;'and ' + str(list(set_y)[0]) + ' target samples.')&#xA;ValueError: Input arrays should have the same number of samples as target arrays. Found 6400 input samples and 1600 target samples. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""87691"" LastActivityDate=""2018-10-26T21:11:35.927"" Title=""Error: Input arrays should have the same number of samples as target arrays.Found 6400 input samples and 1600 target samples"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;redes-neuronales&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""336852"" PostTypeId=""1"" CreationDate=""2020-03-13T14:45:47.650"" Score=""1"" ViewCount=""210"" Body=""&lt;p&gt;Estoy trabajando en un &lt;strong&gt;problema de LSTM&lt;/strong&gt;. Estoy tratando de predecir el &lt;strong&gt;tipo de personalidad MBTI&lt;/strong&gt; (prueba de Myers-Briggs) segn &lt;strong&gt;clasificacin de texto&lt;/strong&gt; (hay 16 tipos de personalidad).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tengo un &lt;strong&gt;archivo csv&lt;/strong&gt;, que fue &lt;strong&gt;preprocesado&lt;/strong&gt;: &lt;em&gt;se eliminaron las stopwords, fue lematizado, tokenizado, secuenciado y paddeado&lt;/em&gt;. El dataframe no tiene ningn valor &lt;code&gt;NaN&lt;/code&gt; y la secuencia de texto solo tiene nmeros &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sin embargo, el problema se genera cuando intento entrenar el modelo, obtengo:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Odu4X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Odu4X.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EqN7G.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqN7G.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/8cdEf.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/8cdEf.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Cmo se ven los datos y las etiquetas x, y con los resultados?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(validation_label_seq)&#xA;[[ 5]&#xA; [10]&#xA; [ 4]&#xA; [ 4]&#xA; [15]&#xA; [12]&#xA; [ 1]...]&#xA;&#xA;print(validation_padded[0])&#xA;maxlen = 240&#xA;array([  23,  353,  147,  677,    1,    1,  409,   10,  845, 1530,    1,&#xA;        103,  107,  998,  117, 1389,   25,    1,   28, 1889,  165,    1,&#xA;       1520,   49,  718,   65,   55,   34,    0,    0,    0,    0,    0,&#xA;          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,...], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(train_label_seq)&#xA;[[ 8]&#xA; [ 9]&#xA; [ 3]&#xA; [ 7]&#xA; [ 4]&#xA; [10]&#xA; [15]&#xA; [11]...]&#xA;&#xA;print(train_data_padded[0])&#xA;maxlen = 240&#xA;array([ 19, 301, 133, 302, 562, 133,  28, 563, 895, 896, 897, 118,  99,&#xA;       564, 397,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,&#xA;         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0...], dtype=int32)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;results = model.evaluate(validation_padded, validation_label_seq)&#xA;&#xA;test = validation_padded[10]&#xA;predict = model.predict_classes([test])&#xA;print(predict[1])&#xA;&#xA;59/59 [==============================] - 0s 1ms/sample - loss: nan - accuracy: 0.0000e+00&#xA;[0]&#xA;/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/sequential.py:342: RuntimeWarning: invalid value encountered in greater&#xA;  return (proba &amp;gt; 0.5).astype('int32')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(predict)&#xA;&#xA;array([[0],&#xA;       [0],&#xA;       ...&#xA;       [0],&#xA;       [0]], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Qu ya intent?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ya intent cambiar a diferentes optimizadores&lt;/li&gt;&#xA;&lt;li&gt;reducir el batch size&lt;/li&gt;&#xA;&lt;li&gt;Verifique errores de valores en el dataframe y en las secuencias, como en los paddeados de &lt;code&gt;x&lt;/code&gt; e &lt;code&gt;y&lt;/code&gt; (datos de entrenamiento y validacin).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Output esperado:&lt;/strong&gt;&#xA;Tal vez estoy construyendo mal el modelo, as que explicar cul es la idea principal. Me gustara obtener un output o o diecisis outpus, que determina el &lt;code&gt;accuracy&lt;/code&gt; del tipo de personalidad.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 output:&#xA;INTP: 89%&#xA;&#xA;16 outputs:&#xA;ENTP: 5% | INTP: 81% | INTJ: 1% | ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dataframe:&lt;/strong&gt; &lt;a href=&quot;https://github.com/GUNTERMAXIMUS/mbti/blob/master/mbti_new.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;mbti_df.csv&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Cualquier sugerencia para mejorar la pregunta ser considerada.&lt;/em&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""134251"" LastActivityDate=""2020-03-13T14:45:47.650"" Title=""Cmo resolver loss: nan y accuracy: 0.0000e + 00 en un problema LSTM? Tensorflow 2.x"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;regresin-lineal&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""336852"" PostTypeId=""1"" CreationDate=""2020-03-13T14:45:47.650"" Score=""1"" ViewCount=""210"" Body=""&lt;p&gt;Estoy trabajando en un &lt;strong&gt;problema de LSTM&lt;/strong&gt;. Estoy tratando de predecir el &lt;strong&gt;tipo de personalidad MBTI&lt;/strong&gt; (prueba de Myers-Briggs) segn &lt;strong&gt;clasificacin de texto&lt;/strong&gt; (hay 16 tipos de personalidad).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tengo un &lt;strong&gt;archivo csv&lt;/strong&gt;, que fue &lt;strong&gt;preprocesado&lt;/strong&gt;: &lt;em&gt;se eliminaron las stopwords, fue lematizado, tokenizado, secuenciado y paddeado&lt;/em&gt;. El dataframe no tiene ningn valor &lt;code&gt;NaN&lt;/code&gt; y la secuencia de texto solo tiene nmeros &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sin embargo, el problema se genera cuando intento entrenar el modelo, obtengo:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Odu4X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Odu4X.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EqN7G.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqN7G.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/8cdEf.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/8cdEf.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Cmo se ven los datos y las etiquetas x, y con los resultados?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(validation_label_seq)&#xA;[[ 5]&#xA; [10]&#xA; [ 4]&#xA; [ 4]&#xA; [15]&#xA; [12]&#xA; [ 1]...]&#xA;&#xA;print(validation_padded[0])&#xA;maxlen = 240&#xA;array([  23,  353,  147,  677,    1,    1,  409,   10,  845, 1530,    1,&#xA;        103,  107,  998,  117, 1389,   25,    1,   28, 1889,  165,    1,&#xA;       1520,   49,  718,   65,   55,   34,    0,    0,    0,    0,    0,&#xA;          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,...], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(train_label_seq)&#xA;[[ 8]&#xA; [ 9]&#xA; [ 3]&#xA; [ 7]&#xA; [ 4]&#xA; [10]&#xA; [15]&#xA; [11]...]&#xA;&#xA;print(train_data_padded[0])&#xA;maxlen = 240&#xA;array([ 19, 301, 133, 302, 562, 133,  28, 563, 895, 896, 897, 118,  99,&#xA;       564, 397,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,&#xA;         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0...], dtype=int32)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;results = model.evaluate(validation_padded, validation_label_seq)&#xA;&#xA;test = validation_padded[10]&#xA;predict = model.predict_classes([test])&#xA;print(predict[1])&#xA;&#xA;59/59 [==============================] - 0s 1ms/sample - loss: nan - accuracy: 0.0000e+00&#xA;[0]&#xA;/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/sequential.py:342: RuntimeWarning: invalid value encountered in greater&#xA;  return (proba &amp;gt; 0.5).astype('int32')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(predict)&#xA;&#xA;array([[0],&#xA;       [0],&#xA;       ...&#xA;       [0],&#xA;       [0]], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Qu ya intent?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ya intent cambiar a diferentes optimizadores&lt;/li&gt;&#xA;&lt;li&gt;reducir el batch size&lt;/li&gt;&#xA;&lt;li&gt;Verifique errores de valores en el dataframe y en las secuencias, como en los paddeados de &lt;code&gt;x&lt;/code&gt; e &lt;code&gt;y&lt;/code&gt; (datos de entrenamiento y validacin).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Output esperado:&lt;/strong&gt;&#xA;Tal vez estoy construyendo mal el modelo, as que explicar cul es la idea principal. Me gustara obtener un output o o diecisis outpus, que determina el &lt;code&gt;accuracy&lt;/code&gt; del tipo de personalidad.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 output:&#xA;INTP: 89%&#xA;&#xA;16 outputs:&#xA;ENTP: 5% | INTP: 81% | INTJ: 1% | ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dataframe:&lt;/strong&gt; &lt;a href=&quot;https://github.com/GUNTERMAXIMUS/mbti/blob/master/mbti_new.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;mbti_df.csv&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Cualquier sugerencia para mejorar la pregunta ser considerada.&lt;/em&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""134251"" LastActivityDate=""2020-03-13T14:45:47.650"" Title=""Cmo resolver loss: nan y accuracy: 0.0000e + 00 en un problema LSTM? Tensorflow 2.x"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;regresin-lineal&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""336852"" PostTypeId=""1"" CreationDate=""2020-03-13T14:45:47.650"" Score=""1"" ViewCount=""210"" Body=""&lt;p&gt;Estoy trabajando en un &lt;strong&gt;problema de LSTM&lt;/strong&gt;. Estoy tratando de predecir el &lt;strong&gt;tipo de personalidad MBTI&lt;/strong&gt; (prueba de Myers-Briggs) segn &lt;strong&gt;clasificacin de texto&lt;/strong&gt; (hay 16 tipos de personalidad).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tengo un &lt;strong&gt;archivo csv&lt;/strong&gt;, que fue &lt;strong&gt;preprocesado&lt;/strong&gt;: &lt;em&gt;se eliminaron las stopwords, fue lematizado, tokenizado, secuenciado y paddeado&lt;/em&gt;. El dataframe no tiene ningn valor &lt;code&gt;NaN&lt;/code&gt; y la secuencia de texto solo tiene nmeros &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sin embargo, el problema se genera cuando intento entrenar el modelo, obtengo:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Odu4X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Odu4X.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EqN7G.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqN7G.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/8cdEf.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/8cdEf.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Cmo se ven los datos y las etiquetas x, y con los resultados?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(validation_label_seq)&#xA;[[ 5]&#xA; [10]&#xA; [ 4]&#xA; [ 4]&#xA; [15]&#xA; [12]&#xA; [ 1]...]&#xA;&#xA;print(validation_padded[0])&#xA;maxlen = 240&#xA;array([  23,  353,  147,  677,    1,    1,  409,   10,  845, 1530,    1,&#xA;        103,  107,  998,  117, 1389,   25,    1,   28, 1889,  165,    1,&#xA;       1520,   49,  718,   65,   55,   34,    0,    0,    0,    0,    0,&#xA;          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,...], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(train_label_seq)&#xA;[[ 8]&#xA; [ 9]&#xA; [ 3]&#xA; [ 7]&#xA; [ 4]&#xA; [10]&#xA; [15]&#xA; [11]...]&#xA;&#xA;print(train_data_padded[0])&#xA;maxlen = 240&#xA;array([ 19, 301, 133, 302, 562, 133,  28, 563, 895, 896, 897, 118,  99,&#xA;       564, 397,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,&#xA;         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0...], dtype=int32)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;results = model.evaluate(validation_padded, validation_label_seq)&#xA;&#xA;test = validation_padded[10]&#xA;predict = model.predict_classes([test])&#xA;print(predict[1])&#xA;&#xA;59/59 [==============================] - 0s 1ms/sample - loss: nan - accuracy: 0.0000e+00&#xA;[0]&#xA;/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/sequential.py:342: RuntimeWarning: invalid value encountered in greater&#xA;  return (proba &amp;gt; 0.5).astype('int32')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(predict)&#xA;&#xA;array([[0],&#xA;       [0],&#xA;       ...&#xA;       [0],&#xA;       [0]], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Qu ya intent?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ya intent cambiar a diferentes optimizadores&lt;/li&gt;&#xA;&lt;li&gt;reducir el batch size&lt;/li&gt;&#xA;&lt;li&gt;Verifique errores de valores en el dataframe y en las secuencias, como en los paddeados de &lt;code&gt;x&lt;/code&gt; e &lt;code&gt;y&lt;/code&gt; (datos de entrenamiento y validacin).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Output esperado:&lt;/strong&gt;&#xA;Tal vez estoy construyendo mal el modelo, as que explicar cul es la idea principal. Me gustara obtener un output o o diecisis outpus, que determina el &lt;code&gt;accuracy&lt;/code&gt; del tipo de personalidad.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 output:&#xA;INTP: 89%&#xA;&#xA;16 outputs:&#xA;ENTP: 5% | INTP: 81% | INTJ: 1% | ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dataframe:&lt;/strong&gt; &lt;a href=&quot;https://github.com/GUNTERMAXIMUS/mbti/blob/master/mbti_new.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;mbti_df.csv&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Cualquier sugerencia para mejorar la pregunta ser considerada.&lt;/em&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""134251"" LastActivityDate=""2020-03-13T14:45:47.650"" Title=""Cmo resolver loss: nan y accuracy: 0.0000e + 00 en un problema LSTM? Tensorflow 2.x"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;regresin-lineal&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""355010"" PostTypeId=""1"" CreationDate=""2020-05-12T19:15:42.267"" Score=""1"" ViewCount=""1121"" Body=""&lt;p&gt;Cuando utilizo los clasificadores que os muestro a continuacin, me aparece el numero de aciertos pero sin embargo, me aparece un aviso, no entiendo el aviso ni que parmetro debo modificar Alguien me puede orientar?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ademas, he modificado el &lt;code&gt;radmon_state&lt;/code&gt; a &lt;code&gt;None&lt;/code&gt; y tampoco se me soluciona &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1000)&#xA;names = [&#xA;         &quot;SVM&quot;,&#xA;         &quot;Decision Tree&quot;,&#xA;         &quot;GaussianNB&quot;,&#xA;         &quot;RandomForestClassifier&quot;,&#xA;         &quot;KNeighborsClassifier(3)&quot;&#xA;&#xA;]&#xA;classifiers = [&#xA;  SVC(probability=True),&#xA;  DecisionTreeClassifier(),&#xA;  GaussianNB(),&#xA;  RandomForestClassifier(),&#xA;  KNeighborsClassifier(13)&#xA;&#xA;]&#xA;&#xA;&#xA;cv = sklearn.model_selection.StratifiedKFold(n_splits=10, random_state=123)#semilla por donde tiene que empezar a barajar&#xA;for name, clf in zip(names, classifiers):&#xA;  results = np.round(cross_val_score(clf, X, y, cv=cv),2)#le pasamos cv para que lo resuelva&#xA;  print(f'{name:22s} media aciertos: {results.mean():.2} resultados: {results}')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;la salida es:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True FutureWarning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""129062"" LastEditorUserId=""15089"" LastEditDate=""2020-05-12T20:44:44.153"" LastActivityDate=""2020-05-12T20:44:44.153"" Title=""Por qu me aparece este aviso: Setting a random_state has no effect since shuffle is False?"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""357637"" PostTypeId=""1"" CreationDate=""2020-05-20T18:42:05.747"" Score=""0"" ViewCount=""40"" Body=""&lt;p&gt;Estoy siguiendo el Curso intensivo de aprendizaje automtico de google. Pero al ser algo antiguo usa la version1.x de TensorFlow, asi que pensaba cambiar los ejercicios para poder ejecutarlos en TensorFlow 2.0. Pero estoy atascado en ese ejercicio:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En concreto el codigo:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):&#xA;    &quot;&quot;&quot;Trains a linear regression model of one feature.&#xA;&#xA;    Args:&#xA;      features: pandas DataFrame of features&#xA;      targets: pandas DataFrame of targets&#xA;      batch_size: Size of batches to be passed to the model&#xA;      shuffle: True or False. Whether to shuffle the data.&#xA;      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely&#xA;    Returns:&#xA;      Tuple of (features, labels) for next data batch&#xA;    &quot;&quot;&quot;&#xA;&#xA;    # Convert pandas data into a dict of np arrays.&#xA;    features = {key:np.array(value) for key,value in dict(features).items()}                                           &#xA;&#xA;    # Construct a dataset, and configure batching/repeating.&#xA;    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit&#xA;    ds = ds.batch(batch_size).repeat(num_epochs)&#xA;&#xA;    # Shuffle the data, if specified.&#xA;    if shuffle:&#xA;      ds = ds.shuffle(buffer_size=10000)&#xA;&#xA;    # Return the next batch of data.&#xA;    features, labels = ds.make_one_shot_iterator().get_next()&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;He reemplazado la linea &lt;code&gt;features, labels = ds.make_one_shot_iterator().get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;por &lt;code&gt;features, labels = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y parece funcionar pero despues la linea:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_ = linear_regressor.train(&#xA;    input_fn = lambda:my_input_fn(my_feature, targets),&#xA;    steps=100&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;que se usa para entrenar el modelo, provoca que se cuelgue python&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tambien he intentado algo como:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    features, labels = ds.__iter__()&#xA;    next(ds.__iter__())&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;pero devuelve el error &lt;code&gt;__iter__() is only supported inside of tf.function or when eager execution is enabled.&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Soy bastante inexperto en python y sigo el curso como aficionado.&#xA;Alguna idea sobre como solventarlo?&#xA;Gracias.&lt;/p&gt;&#xA;"" OwnerUserId=""173823"" LastEditorUserId=""157171"" LastEditDate=""2020-05-21T10:09:46.587"" LastActivityDate=""2020-06-08T11:48:22.207"" Title=""Como reemplazar make_one_shot_iterator() del Curso intensivo de aprendizaje automtico de google"" Tags=""&lt;python&gt;&lt;machine-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""355010"" PostTypeId=""1"" CreationDate=""2020-05-12T19:15:42.267"" Score=""1"" ViewCount=""1121"" Body=""&lt;p&gt;Cuando utilizo los clasificadores que os muestro a continuacin, me aparece el numero de aciertos pero sin embargo, me aparece un aviso, no entiendo el aviso ni que parmetro debo modificar Alguien me puede orientar?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ademas, he modificado el &lt;code&gt;radmon_state&lt;/code&gt; a &lt;code&gt;None&lt;/code&gt; y tampoco se me soluciona &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1000)&#xA;names = [&#xA;         &quot;SVM&quot;,&#xA;         &quot;Decision Tree&quot;,&#xA;         &quot;GaussianNB&quot;,&#xA;         &quot;RandomForestClassifier&quot;,&#xA;         &quot;KNeighborsClassifier(3)&quot;&#xA;&#xA;]&#xA;classifiers = [&#xA;  SVC(probability=True),&#xA;  DecisionTreeClassifier(),&#xA;  GaussianNB(),&#xA;  RandomForestClassifier(),&#xA;  KNeighborsClassifier(13)&#xA;&#xA;]&#xA;&#xA;&#xA;cv = sklearn.model_selection.StratifiedKFold(n_splits=10, random_state=123)#semilla por donde tiene que empezar a barajar&#xA;for name, clf in zip(names, classifiers):&#xA;  results = np.round(cross_val_score(clf, X, y, cv=cv),2)#le pasamos cv para que lo resuelva&#xA;  print(f'{name:22s} media aciertos: {results.mean():.2} resultados: {results}')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;la salida es:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True FutureWarning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""129062"" LastEditorUserId=""15089"" LastEditDate=""2020-05-12T20:44:44.153"" LastActivityDate=""2020-05-12T20:44:44.153"" Title=""Por qu me aparece este aviso: Setting a random_state has no effect since shuffle is False?"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""355010"" PostTypeId=""1"" CreationDate=""2020-05-12T19:15:42.267"" Score=""1"" ViewCount=""1121"" Body=""&lt;p&gt;Cuando utilizo los clasificadores que os muestro a continuacin, me aparece el numero de aciertos pero sin embargo, me aparece un aviso, no entiendo el aviso ni que parmetro debo modificar Alguien me puede orientar?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ademas, he modificado el &lt;code&gt;radmon_state&lt;/code&gt; a &lt;code&gt;None&lt;/code&gt; y tampoco se me soluciona &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1000)&#xA;names = [&#xA;         &quot;SVM&quot;,&#xA;         &quot;Decision Tree&quot;,&#xA;         &quot;GaussianNB&quot;,&#xA;         &quot;RandomForestClassifier&quot;,&#xA;         &quot;KNeighborsClassifier(3)&quot;&#xA;&#xA;]&#xA;classifiers = [&#xA;  SVC(probability=True),&#xA;  DecisionTreeClassifier(),&#xA;  GaussianNB(),&#xA;  RandomForestClassifier(),&#xA;  KNeighborsClassifier(13)&#xA;&#xA;]&#xA;&#xA;&#xA;cv = sklearn.model_selection.StratifiedKFold(n_splits=10, random_state=123)#semilla por donde tiene que empezar a barajar&#xA;for name, clf in zip(names, classifiers):&#xA;  results = np.round(cross_val_score(clf, X, y, cv=cv),2)#le pasamos cv para que lo resuelva&#xA;  print(f'{name:22s} media aciertos: {results.mean():.2} resultados: {results}')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;la salida es:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True FutureWarning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""129062"" LastEditorUserId=""15089"" LastEditDate=""2020-05-12T20:44:44.153"" LastActivityDate=""2020-05-12T20:44:44.153"" Title=""Por qu me aparece este aviso: Setting a random_state has no effect since shuffle is False?"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""357637"" PostTypeId=""1"" CreationDate=""2020-05-20T18:42:05.747"" Score=""0"" ViewCount=""40"" Body=""&lt;p&gt;Estoy siguiendo el Curso intensivo de aprendizaje automtico de google. Pero al ser algo antiguo usa la version1.x de TensorFlow, asi que pensaba cambiar los ejercicios para poder ejecutarlos en TensorFlow 2.0. Pero estoy atascado en ese ejercicio:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En concreto el codigo:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):&#xA;    &quot;&quot;&quot;Trains a linear regression model of one feature.&#xA;&#xA;    Args:&#xA;      features: pandas DataFrame of features&#xA;      targets: pandas DataFrame of targets&#xA;      batch_size: Size of batches to be passed to the model&#xA;      shuffle: True or False. Whether to shuffle the data.&#xA;      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely&#xA;    Returns:&#xA;      Tuple of (features, labels) for next data batch&#xA;    &quot;&quot;&quot;&#xA;&#xA;    # Convert pandas data into a dict of np arrays.&#xA;    features = {key:np.array(value) for key,value in dict(features).items()}                                           &#xA;&#xA;    # Construct a dataset, and configure batching/repeating.&#xA;    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit&#xA;    ds = ds.batch(batch_size).repeat(num_epochs)&#xA;&#xA;    # Shuffle the data, if specified.&#xA;    if shuffle:&#xA;      ds = ds.shuffle(buffer_size=10000)&#xA;&#xA;    # Return the next batch of data.&#xA;    features, labels = ds.make_one_shot_iterator().get_next()&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;He reemplazado la linea &lt;code&gt;features, labels = ds.make_one_shot_iterator().get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;por &lt;code&gt;features, labels = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y parece funcionar pero despues la linea:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_ = linear_regressor.train(&#xA;    input_fn = lambda:my_input_fn(my_feature, targets),&#xA;    steps=100&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;que se usa para entrenar el modelo, provoca que se cuelgue python&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tambien he intentado algo como:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    features, labels = ds.__iter__()&#xA;    next(ds.__iter__())&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;pero devuelve el error &lt;code&gt;__iter__() is only supported inside of tf.function or when eager execution is enabled.&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Soy bastante inexperto en python y sigo el curso como aficionado.&#xA;Alguna idea sobre como solventarlo?&#xA;Gracias.&lt;/p&gt;&#xA;"" OwnerUserId=""173823"" LastEditorUserId=""157171"" LastEditDate=""2020-05-21T10:09:46.587"" LastActivityDate=""2020-06-08T11:48:22.207"" Title=""Como reemplazar make_one_shot_iterator() del Curso intensivo de aprendizaje automtico de google"" Tags=""&lt;python&gt;&lt;machine-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""357637"" PostTypeId=""1"" CreationDate=""2020-05-20T18:42:05.747"" Score=""0"" ViewCount=""40"" Body=""&lt;p&gt;Estoy siguiendo el Curso intensivo de aprendizaje automtico de google. Pero al ser algo antiguo usa la version1.x de TensorFlow, asi que pensaba cambiar los ejercicios para poder ejecutarlos en TensorFlow 2.0. Pero estoy atascado en ese ejercicio:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En concreto el codigo:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):&#xA;    &quot;&quot;&quot;Trains a linear regression model of one feature.&#xA;&#xA;    Args:&#xA;      features: pandas DataFrame of features&#xA;      targets: pandas DataFrame of targets&#xA;      batch_size: Size of batches to be passed to the model&#xA;      shuffle: True or False. Whether to shuffle the data.&#xA;      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely&#xA;    Returns:&#xA;      Tuple of (features, labels) for next data batch&#xA;    &quot;&quot;&quot;&#xA;&#xA;    # Convert pandas data into a dict of np arrays.&#xA;    features = {key:np.array(value) for key,value in dict(features).items()}                                           &#xA;&#xA;    # Construct a dataset, and configure batching/repeating.&#xA;    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit&#xA;    ds = ds.batch(batch_size).repeat(num_epochs)&#xA;&#xA;    # Shuffle the data, if specified.&#xA;    if shuffle:&#xA;      ds = ds.shuffle(buffer_size=10000)&#xA;&#xA;    # Return the next batch of data.&#xA;    features, labels = ds.make_one_shot_iterator().get_next()&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;He reemplazado la linea &lt;code&gt;features, labels = ds.make_one_shot_iterator().get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;por &lt;code&gt;features, labels = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y parece funcionar pero despues la linea:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_ = linear_regressor.train(&#xA;    input_fn = lambda:my_input_fn(my_feature, targets),&#xA;    steps=100&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;que se usa para entrenar el modelo, provoca que se cuelgue python&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tambien he intentado algo como:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    features, labels = ds.__iter__()&#xA;    next(ds.__iter__())&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;pero devuelve el error &lt;code&gt;__iter__() is only supported inside of tf.function or when eager execution is enabled.&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Soy bastante inexperto en python y sigo el curso como aficionado.&#xA;Alguna idea sobre como solventarlo?&#xA;Gracias.&lt;/p&gt;&#xA;"" OwnerUserId=""173823"" LastEditorUserId=""157171"" LastEditDate=""2020-05-21T10:09:46.587"" LastActivityDate=""2020-06-08T11:48:22.207"" Title=""Como reemplazar make_one_shot_iterator() del Curso intensivo de aprendizaje automtico de google"" Tags=""&lt;python&gt;&lt;machine-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""336852"" PostTypeId=""1"" CreationDate=""2020-03-13T14:45:47.650"" Score=""1"" ViewCount=""210"" Body=""&lt;p&gt;Estoy trabajando en un &lt;strong&gt;problema de LSTM&lt;/strong&gt;. Estoy tratando de predecir el &lt;strong&gt;tipo de personalidad MBTI&lt;/strong&gt; (prueba de Myers-Briggs) segn &lt;strong&gt;clasificacin de texto&lt;/strong&gt; (hay 16 tipos de personalidad).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tengo un &lt;strong&gt;archivo csv&lt;/strong&gt;, que fue &lt;strong&gt;preprocesado&lt;/strong&gt;: &lt;em&gt;se eliminaron las stopwords, fue lematizado, tokenizado, secuenciado y paddeado&lt;/em&gt;. El dataframe no tiene ningn valor &lt;code&gt;NaN&lt;/code&gt; y la secuencia de texto solo tiene nmeros &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sin embargo, el problema se genera cuando intento entrenar el modelo, obtengo:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Odu4X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Odu4X.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EqN7G.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EqN7G.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/8cdEf.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/8cdEf.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Cmo se ven los datos y las etiquetas x, y con los resultados?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(validation_label_seq)&#xA;[[ 5]&#xA; [10]&#xA; [ 4]&#xA; [ 4]&#xA; [15]&#xA; [12]&#xA; [ 1]...]&#xA;&#xA;print(validation_padded[0])&#xA;maxlen = 240&#xA;array([  23,  353,  147,  677,    1,    1,  409,   10,  845, 1530,    1,&#xA;        103,  107,  998,  117, 1389,   25,    1,   28, 1889,  165,    1,&#xA;       1520,   49,  718,   65,   55,   34,    0,    0,    0,    0,    0,&#xA;          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,...], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(train_label_seq)&#xA;[[ 8]&#xA; [ 9]&#xA; [ 3]&#xA; [ 7]&#xA; [ 4]&#xA; [10]&#xA; [15]&#xA; [11]...]&#xA;&#xA;print(train_data_padded[0])&#xA;maxlen = 240&#xA;array([ 19, 301, 133, 302, 562, 133,  28, 563, 895, 896, 897, 118,  99,&#xA;       564, 397,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,&#xA;         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0...], dtype=int32)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;results = model.evaluate(validation_padded, validation_label_seq)&#xA;&#xA;test = validation_padded[10]&#xA;predict = model.predict_classes([test])&#xA;print(predict[1])&#xA;&#xA;59/59 [==============================] - 0s 1ms/sample - loss: nan - accuracy: 0.0000e+00&#xA;[0]&#xA;/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/sequential.py:342: RuntimeWarning: invalid value encountered in greater&#xA;  return (proba &amp;gt; 0.5).astype('int32')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;print(predict)&#xA;&#xA;array([[0],&#xA;       [0],&#xA;       ...&#xA;       [0],&#xA;       [0]], dtype=int32)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Qu ya intent?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ya intent cambiar a diferentes optimizadores&lt;/li&gt;&#xA;&lt;li&gt;reducir el batch size&lt;/li&gt;&#xA;&lt;li&gt;Verifique errores de valores en el dataframe y en las secuencias, como en los paddeados de &lt;code&gt;x&lt;/code&gt; e &lt;code&gt;y&lt;/code&gt; (datos de entrenamiento y validacin).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Output esperado:&lt;/strong&gt;&#xA;Tal vez estoy construyendo mal el modelo, as que explicar cul es la idea principal. Me gustara obtener un output o o diecisis outpus, que determina el &lt;code&gt;accuracy&lt;/code&gt; del tipo de personalidad.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 output:&#xA;INTP: 89%&#xA;&#xA;16 outputs:&#xA;ENTP: 5% | INTP: 81% | INTJ: 1% | ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Dataframe:&lt;/strong&gt; &lt;a href=&quot;https://github.com/GUNTERMAXIMUS/mbti/blob/master/mbti_new.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;mbti_df.csv&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Cualquier sugerencia para mejorar la pregunta ser considerada.&lt;/em&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""134251"" LastActivityDate=""2020-03-13T14:45:47.650"" Title=""Cmo resolver loss: nan y accuracy: 0.0000e + 00 en un problema LSTM? Tensorflow 2.x"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;regresin-lineal&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""355010"" PostTypeId=""1"" CreationDate=""2020-05-12T19:15:42.267"" Score=""1"" ViewCount=""1121"" Body=""&lt;p&gt;Cuando utilizo los clasificadores que os muestro a continuacin, me aparece el numero de aciertos pero sin embargo, me aparece un aviso, no entiendo el aviso ni que parmetro debo modificar Alguien me puede orientar?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ademas, he modificado el &lt;code&gt;radmon_state&lt;/code&gt; a &lt;code&gt;None&lt;/code&gt; y tampoco se me soluciona &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1000)&#xA;names = [&#xA;         &quot;SVM&quot;,&#xA;         &quot;Decision Tree&quot;,&#xA;         &quot;GaussianNB&quot;,&#xA;         &quot;RandomForestClassifier&quot;,&#xA;         &quot;KNeighborsClassifier(3)&quot;&#xA;&#xA;]&#xA;classifiers = [&#xA;  SVC(probability=True),&#xA;  DecisionTreeClassifier(),&#xA;  GaussianNB(),&#xA;  RandomForestClassifier(),&#xA;  KNeighborsClassifier(13)&#xA;&#xA;]&#xA;&#xA;&#xA;cv = sklearn.model_selection.StratifiedKFold(n_splits=10, random_state=123)#semilla por donde tiene que empezar a barajar&#xA;for name, clf in zip(names, classifiers):&#xA;  results = np.round(cross_val_score(clf, X, y, cv=cv),2)#le pasamos cv para que lo resuelva&#xA;  print(f'{name:22s} media aciertos: {results.mean():.2} resultados: {results}')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;la salida es:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True FutureWarning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""129062"" LastEditorUserId=""15089"" LastEditDate=""2020-05-12T20:44:44.153"" LastActivityDate=""2020-05-12T20:44:44.153"" Title=""Por qu me aparece este aviso: Setting a random_state has no effect since shuffle is False?"" Tags=""&lt;python&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""357637"" PostTypeId=""1"" CreationDate=""2020-05-20T18:42:05.747"" Score=""0"" ViewCount=""40"" Body=""&lt;p&gt;Estoy siguiendo el Curso intensivo de aprendizaje automtico de google. Pero al ser algo antiguo usa la version1.x de TensorFlow, asi que pensaba cambiar los ejercicios para poder ejecutarlos en TensorFlow 2.0. Pero estoy atascado en ese ejercicio:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=firststeps-colab&amp;amp;hl=es#scrollTo=7UwqGbbxP53O&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;En concreto el codigo:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):&#xA;    &quot;&quot;&quot;Trains a linear regression model of one feature.&#xA;&#xA;    Args:&#xA;      features: pandas DataFrame of features&#xA;      targets: pandas DataFrame of targets&#xA;      batch_size: Size of batches to be passed to the model&#xA;      shuffle: True or False. Whether to shuffle the data.&#xA;      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely&#xA;    Returns:&#xA;      Tuple of (features, labels) for next data batch&#xA;    &quot;&quot;&quot;&#xA;&#xA;    # Convert pandas data into a dict of np arrays.&#xA;    features = {key:np.array(value) for key,value in dict(features).items()}                                           &#xA;&#xA;    # Construct a dataset, and configure batching/repeating.&#xA;    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit&#xA;    ds = ds.batch(batch_size).repeat(num_epochs)&#xA;&#xA;    # Shuffle the data, if specified.&#xA;    if shuffle:&#xA;      ds = ds.shuffle(buffer_size=10000)&#xA;&#xA;    # Return the next batch of data.&#xA;    features, labels = ds.make_one_shot_iterator().get_next()&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;He reemplazado la linea &lt;code&gt;features, labels = ds.make_one_shot_iterator().get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;por &lt;code&gt;features, labels = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;y parece funcionar pero despues la linea:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_ = linear_regressor.train(&#xA;    input_fn = lambda:my_input_fn(my_feature, targets),&#xA;    steps=100&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;que se usa para entrenar el modelo, provoca que se cuelgue python&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tambien he intentado algo como:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    features, labels = ds.__iter__()&#xA;    next(ds.__iter__())&#xA;    return features, labels&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;pero devuelve el error &lt;code&gt;__iter__() is only supported inside of tf.function or when eager execution is enabled.&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Soy bastante inexperto en python y sigo el curso como aficionado.&#xA;Alguna idea sobre como solventarlo?&#xA;Gracias.&lt;/p&gt;&#xA;"" OwnerUserId=""173823"" LastEditorUserId=""157171"" LastEditDate=""2020-05-21T10:09:46.587"" LastActivityDate=""2020-06-08T11:48:22.207"" Title=""Como reemplazar make_one_shot_iterator() del Curso intensivo de aprendizaje automtico de google"" Tags=""&lt;python&gt;&lt;machine-learning&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""453508"" PostTypeId=""1"" CreationDate=""2021-05-20T11:59:01.210"" Score=""1"" ViewCount=""131"" Body=""&lt;p&gt;Entren varios modelos descargados del model zoo de Tensorflow 2, pero cuando los paso a formato .pb con exporter_main_v2.py del object detection de TensorFlow model garden, durante el proceso me alerta con varios Warnings que dicen:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;WARNING:tensorflow:Skipping full serialization of Keras layer &amp;lt;tensorflow.python.keras.layers.core.Lambda object at 0x000002B0CDD382C8&amp;gt;, because it is not built.&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;La versin de Tensorflow que uso es la 2.4.1&lt;/p&gt;&#xA;&lt;p&gt;Me falta instalar algo?&lt;/p&gt;&#xA;"" OwnerUserId=""229142"" LastEditorUserId=""108737"" LastEditDate=""2021-05-20T12:33:08.180"" LastActivityDate=""2021-05-20T12:33:08.180"" Title=""Problema al guardar modelo Tensorflow 2"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;inteligencia-artificial&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""453508"" PostTypeId=""1"" CreationDate=""2021-05-20T11:59:01.210"" Score=""1"" ViewCount=""131"" Body=""&lt;p&gt;Entren varios modelos descargados del model zoo de Tensorflow 2, pero cuando los paso a formato .pb con exporter_main_v2.py del object detection de TensorFlow model garden, durante el proceso me alerta con varios Warnings que dicen:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;WARNING:tensorflow:Skipping full serialization of Keras layer &amp;lt;tensorflow.python.keras.layers.core.Lambda object at 0x000002B0CDD382C8&amp;gt;, because it is not built.&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;La versin de Tensorflow que uso es la 2.4.1&lt;/p&gt;&#xA;&lt;p&gt;Me falta instalar algo?&lt;/p&gt;&#xA;"" OwnerUserId=""229142"" LastEditorUserId=""108737"" LastEditDate=""2021-05-20T12:33:08.180"" LastActivityDate=""2021-05-20T12:33:08.180"" Title=""Problema al guardar modelo Tensorflow 2"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;inteligencia-artificial&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""440970"" PostTypeId=""1"" CreationDate=""2021-04-01T16:12:33.650"" Score=""5"" ViewCount=""870"" Body=""&lt;p&gt;Use GPU-Z para obtener las especificaciones de mi GPU, y su controlador en este caso 461.62&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RKxZ5.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Aparentemente tendria que estar todo bien entre la version de mis drivers y la version de CUDA, no? (que esto este asi solo garantiza lo de los drivers, no que sea compatible con el hardware osea la placa de video)&#xA;&lt;a href=&quot;https://i.stack.imgur.com/Ukt77.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ukt77.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Luego instale la GPU-accelerated library of primitives for DL, NVIDIA cuDNN en su version...&#xA;&lt;a href=&quot;https://i.stack.imgur.com/eZXS9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/eZXS9.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Esta version que es compatible (en teoria), para CUDA 11.0, 11.1 y 11.2&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5RE2Z.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Se que se debe escoger el pytorch en funcion del CUDA que quieras instalar, pero en este caso se que usare el pytorch para la 11.1 osea que elegi esa version.&lt;/p&gt;&#xA;&lt;p&gt;Y puse la carpeta en la direccion MiPC/C:/y ahi cuda  y tambien coloque las 3 variables de entorno.&#xA;Me guie con este video: &lt;a href=&quot;https://www.youtube.com/watch?v=StH5YNrY0mE&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=StH5YNrY0mE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Tambien instale el CUDA Toolkit 11.1.0, que creo en mi caso es el que es consistente con el resto pero estoy en dudas. Aun asi aqui dejo el link de donde lo baje con el exe[local].&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-11.1.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/g3OR8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/g3OR8.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Ahora instale el pytorch para la version 11.1 (que es la que queria) desde el gestor pip, simplemente poniendo el siguiente code copiado de la page:&lt;/p&gt;&#xA;&lt;p&gt;pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f &lt;a href=&quot;https://download.pytorch.org/whl/torch_stable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://download.pytorch.org/whl/torch_stable.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7ywaR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7ywaR.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estube probando pytorch en consola con la impresion de un tensor, y aparentemente funciona perfecto, pero claro hasta ahora con eso solo pruebo que funcione torch con la CPU, ya que no especifique el device.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; x = torch.rand(5, 3)&#xA;&amp;gt;&amp;gt;&amp;gt; print(x)&#xA;tensor([[0.1242, 0.4253, 0.9530],&#xA;        [0.2290, 0.8633, 0.2871],&#xA;        [0.3668, 0.5047, 0.7253],&#xA;        [0.9148, 0.0506, 0.3024],&#xA;        [0.3645, 0.1265, 0.1900]])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Luego ejecute esto:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y me devolvio True, a lo que entiendo que CUDA si funciona (pero no es asi).&lt;/p&gt;&#xA;&lt;p&gt;Lo cual es extrao, aqui encontre una page, donde dicen &amp;quot;que esto funciona&amp;quot; pero en mi caso que devuelva un True parece que NO me garantiza que realmente funcione..., osea que devuelva True solo te indica que el pytorch cuda que pusiste este instalado (y supuestamente verificar si su controlador de GPU y CUDA estn habilitados) pero no te indica realmente si esta funcionando o no, eso es lo que note (te daras cuenta si funciona o no al intentar usar pytorch con GPU).&#xA;Igual paso el link:&#xA;&lt;a href=&quot;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://mundowin.com/como-instalar-pytorch-en-windows-paso-a-paso/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Estuve viendo gente a la que le ocurrio algo similar, pero no me funcionan las soluciones que plantean(o porque estan desactualizadas las soluciones, o quizas yo no se hacerlo bien). Ellos dicen que instale pytorch desde el codigo fuente o algo asi...&lt;/p&gt;&#xA;&lt;p&gt;Aun asi creo que el problema es pytorch.&#xA;y el cuda cc, imagino que debe ser un compiler pero no lo se con seguridad, que dicen?&lt;/p&gt;&#xA;&lt;p&gt;En el siguiente link, plantean una &amp;quot;guia de instalacion algo complicada para mi al menos&amp;quot;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wwNR6.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wwNR6.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch#from-source&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pytorch/pytorch#from-source&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Fui a ese repositorio de github y descargue el proyecto a mi pc.&lt;/p&gt;&#xA;&lt;p&gt;Intente ejecutar ese setup.py con torch anterior eliminado y sin torch anterior eliminado, y tira...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;python setup.py&#xA;Building wheel torch-1.9.0a0+gitUnknown&#xA;usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]&#xA;   or: setup.py --help [cmd1 cmd2 ...]&#xA;   or: setup.py --help-commands&#xA;   or: setup.py cmd --help&#xA;&#xA;error: no commands supplied&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Realmente no entiendo para que es eso...&lt;/p&gt;&#xA;&lt;p&gt;Lo que me sigue dejando en duda es eso del compilador que pide en C++&#xA;Y respecto al CUDA Toolkit 11.1 y el NVIDIA cudDNN (en version 11.1) en teoria los podria dejar asi... como mostre que les instale mas arriba, no?&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/b85BK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/b85BK.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WtWPg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WtWPg.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;De todos modos, al no poder usar con GPU, adapte mi proyecto a CPU modificando todo lo que diga to_gpu o to_device, y andubo con CPU usando los 3 en 11.1, pero como CPU (lento, muy lento, per andubo, osea que con eso ya descarto que sea mi proyecto)&lt;/p&gt;&#xA;&lt;p&gt;Si lo ejecuto con GPU, usando el supuesto CUDA 11.1 instalado me tira estos errores, y ahi el problema:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 13676)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Trate de describir d ela mejor manera que pude todo lo que hice haber si ustedes encuentran el error :( , pero sigue sin funcionar...&#xA;Probe si la camara es correcta y opencv la detecta y da video streaming osea que un problema con la webcam esta descartado.&lt;/p&gt;&#xA;&lt;p&gt;Aun asi sigue tirando esto...&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Ya no se mas que hacer para hacer funcionar a pytorch en mi pc, espero realmente puedan ayudarme. Como veran trate de explicarme lo mejor posible, pero encerio que no se mas que hacerle.&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;Si bien siempre trabaje con Python 3.8.5 (el que me vino con Anaconda) desde la propia Anaconda prompt, hice las instalaciones con el gesto pip, ahora lo probe con conda install&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;pip uninstall torch&#xA;Found existing installation: torch 1.8.1+cu111&#xA;Uninstalling torch-1.8.1+cu111:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1+cu111.dist-info\*&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch\*&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1+cu111&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\pytorch-master&amp;gt;conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit=11.1&#xA;    - pytorch&#xA;    - torchaudio&#xA;    - torchvision&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    conda-4.10.0               |   py38haa244fe_0         3.1 MB  conda-forge&#xA;    cudatoolkit-11.1.1         |       heb2d755_7        1.20 GB  conda-forge&#xA;    libuv-1.41.0               |       h8ffe710_0         341 KB  conda-forge&#xA;    ninja-1.10.2               |       h5362a0b_0         273 KB  conda-forge&#xA;    python_abi-3.8             |           1_cp38           4 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.8_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    torchaudio-0.8.1           |             py38         2.7 MB  pytorch&#xA;    torchvision-0.9.1          |       py38_cu111         7.5 MB  pytorch&#xA;    ------------------------------------------------------------&#xA;                                           Total:        2.74 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.8_cuda11.1_cudnn8_0&#xA;  torchaudio         pytorch/win-64::torchaudio-0.8.1-py38&#xA;  torchvision        pytorch/win-64::torchvision-0.9.1-py38_cu111&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;torchvision-0.9.1    | 7.5 MB    | ############################################################################ | 100%&#xA;conda-4.10.0         | 3.1 MB    | ############################################################################ | 100%&#xA;python_abi-3.8       | 4 KB      | ############################################################################ | 100%&#xA;libuv-1.41.0         | 341 KB    | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;cudatoolkit-11.1.1   | 1.20 GB   | ############################################################################ | 100%&#xA;torchaudio-0.8.1     | 2.7 MB    | ############################################################################ | 100%&#xA;ninja-1.10.2         | 273 KB    | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: / &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Veo que tambien actualizo el channel de paquetes aqui:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;The following packages will be UPDATED:&#xA;&#xA;  conda               pkgs/main::conda-4.9.2-py38haa95532_0 --&amp;gt; conda-forge::conda-4.10.0-py38haa244fe_0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Desafortunadamente tampoco funciono repitiendo el mismo error de cuando lo instale con el gestor pip&lt;/p&gt;&#xA;&lt;p&gt;Ahora vi que algunos usan un virtual enviroment para hacerle funcionar y que no tenga conflictos con otros paquetes&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://conda-forge.org/docs/user/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://conda-forge.org/docs/user/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://tenpy.readthedocs.io/en/latest/install/conda.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tenpy.readthedocs.io/en/latest/install/conda.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&quot;&gt;https://stackoverflow.com/questions/57518050/conda-install-and-update-do-not-work-also-solving-environment-get-errors&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=vBfM5l9VK5c&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.youtube.com/watch?v=vBfM5l9VK5c&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;python&#xA;Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; torch.cuda.is_available()&#xA;True&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(base) C:\Users\MIPC&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python List_Available_Webcams.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;[0, 1, 3, 4]&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:1] global&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;python Video_Camera_Basic_Script.py&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP&amp;gt;cd &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;quot;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;waiting for a connection&#xA;connection from ('127.0.0.1', 47773)&#xA;Connection closed&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;main.py&amp;quot;, line 39, in &amp;lt;module&amp;gt;&#xA;    pose_data = pose_estimator.get_pose_data(img.copy())&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 74, in get_pose_data&#xA;    heatmaps, pafs, scale, pad = self.infer_fast(img)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\pose_estimator.py&amp;quot;, line 49, in infer_fast&#xA;    stages_output = self.net(tensor_img)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\emotion_models\with_mobilenet.py&amp;quot;, line 134, in forward&#xA;    backbone_features = self.model(x)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\container.py&amp;quot;, line 119, in forward&#xA;    input = module(input)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\module.py&amp;quot;, line 889, in _call_impl&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 399, in forward&#xA;    return self._conv_forward(input, self.weight, self.bias)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\lib\site-packages\torch\nn\modules\conv.py&amp;quot;, line 395, in _conv_forward&#xA;    return F.conv2d(input, weight, bias, self.stride,&#xA;RuntimeError: CUDA error: no kernel image is available for execution on the device&#xA;[ WARN:1] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-kh7iq4w7\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~Sourc&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda uninstall torch&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed&#xA;&#xA;PackagesNotFoundError: The following packages are missing from the target environment:&#xA;  - torch&#xA;&#xA;&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt; pip uninstall torch&#xA;Found existing installation: torch 1.8.1&#xA;Uninstalling torch-1.8.1:&#xA;  Would remove:&#xA;    c:\users\mipc\anaconda3\lib\site-packages\caffe2&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch&#xA;    c:\users\mipc\anaconda3\lib\site-packages\torch-1.8.1-py3.8.egg-info&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-caffe2-to-onnx.exe&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2-script.py&#xA;    c:\users\mipc\anaconda3\scripts\convert-onnx-to-caffe2.exe&#xA;Proceed (y/n)? y&#xA;  Successfully uninstalled torch-1.8.1&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda --version&#xA;conda 4.10.0&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda update conda&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3&#xA;&#xA;  added / updated specs:&#xA;    - conda&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    backports.functools_lru_cache-1.6.3|     pyhd3eb1b0_0           9 KB&#xA;    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB&#xA;    libuv-1.40.0               |       he774522_0         255 KB&#xA;    ninja-1.10.2               |   py38h6d14046_0         247 KB&#xA;    ------------------------------------------------------------&#xA;                                           Total:         522 KB&#xA;&#xA;The following packages will be UPDATED:&#xA;&#xA;  backports.functoo~                             1.6.1-py_0 --&amp;gt; 1.6.3-pyhd3eb1b0_0&#xA;&#xA;The following packages will be SUPERSEDED by a higher-priority channel:&#xA;&#xA;  libuv                conda-forge::libuv-1.41.0-h8ffe710_0 --&amp;gt; pkgs/main::libuv-1.40.0-he774522_0&#xA;  ninja                conda-forge::ninja-1.10.2-h5362a0b_0 --&amp;gt; pkgs/main::ninja-1.10.2-py38h6d14046_0&#xA;&#xA;The following packages will be DOWNGRADED:&#xA;&#xA;  backports.tempfile                               1.0-py_1 --&amp;gt; 1.0-pyhd3eb1b0_1&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;ninja-1.10.2         | 247 KB    | ############################################################################ | 100%&#xA;libuv-1.40.0         | 255 KB    | ############################################################################ | 100%&#xA;backports.functools_ | 9 KB      | ############################################################################ | 100%&#xA;backports.tempfile-1 | 11 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: done&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --add channels conda-forge&#xA;Warning: 'conda-forge' already in 'channels' list, moving to the top&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda config --set channel_priority strict&#xA;&#xA;(base) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;conda install --channel=conda-forge physics-tenpy&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.&#xA;Collecting package metadata (repodata.json): done&#xA;Solving environment: failed with initial frozen solve. Retrying with flexible solve.&#xA;Solving environment: /&#xA;Found conflicts! Looking for incompatible packages.&#xA;This can take several minutes.  Press CTRL-C to abort.&#xA;Examining boto:   2%| &#xA;                                                                                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Y EL CODE SIGUE CON ALGUNOS CONFLICTOS QUE DICE QUE ENCUENTRA CON TRAS EJECUTAR conda install --channel=conda-forge physics-tenpy&#xA;Esto tomo unas horas pero no soluciono nada.&#xA;Osea que al final me tire por intentar lo del venv, realmente no entiendo porque tendria que funcionar pero... solo me queda probar&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(base) C:\Users\MIPC&amp;gt;conda activate tenpy&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;python&#xA;Python 3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 04:59:43) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&#xA;ModuleNotFoundError: No module named 'torch'&#xA;&amp;gt;&amp;gt;&amp;gt; exit()&#xA;&#xA;(tenpy) C:\Users\MIPC&amp;gt;conda install pytorch cudatoolkit -c pytorch&#xA;Collecting package metadata (current_repodata.json): done&#xA;Solving environment: done&#xA;&#xA;## Package Plan ##&#xA;&#xA;  environment location: C:\Users\MIPC\anaconda3\envs\tenpy&#xA;&#xA;  added / updated specs:&#xA;    - cudatoolkit&#xA;    - pytorch&#xA;&#xA;&#xA;The following packages will be downloaded:&#xA;&#xA;    package                    |            build&#xA;    ---------------------------|-----------------&#xA;    blas-2.108                 |              mkl          13 KB  conda-forge&#xA;    blas-devel-3.9.0           |            8_mkl          12 KB  conda-forge&#xA;    liblapacke-3.9.0           |            8_mkl         3.9 MB  conda-forge&#xA;    mkl-devel-2020.4           |     h57928b3_312         5.6 MB  conda-forge&#xA;    mkl-include-2020.4         |     hb70f87d_311         696 KB  conda-forge&#xA;    pytorch-1.8.1              |py3.9_cuda11.1_cudnn8_0        1.53 GB  pytorch&#xA;    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge&#xA;    ------------------------------------------------------------&#xA;                                           Total:        1.54 GB&#xA;&#xA;The following NEW packages will be INSTALLED:&#xA;&#xA;  blas               conda-forge/win-64::blas-2.108-mkl&#xA;  blas-devel         conda-forge/win-64::blas-devel-3.9.0-8_mkl&#xA;  cudatoolkit        conda-forge/win-64::cudatoolkit-11.1.1-heb2d755_7&#xA;  liblapacke         conda-forge/win-64::liblapacke-3.9.0-8_mkl&#xA;  libuv              conda-forge/win-64::libuv-1.41.0-h8ffe710_0&#xA;  mkl-devel          conda-forge/win-64::mkl-devel-2020.4-h57928b3_312&#xA;  mkl-include        conda-forge/win-64::mkl-include-2020.4-hb70f87d_311&#xA;  ninja              conda-forge/win-64::ninja-1.10.2-h5362a0b_0&#xA;  pytorch            pytorch/win-64::pytorch-1.8.1-py3.9_cuda11.1_cudnn8_0&#xA;  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0&#xA;&#xA;&#xA;Proceed ([y]/n)? y&#xA;&#xA;&#xA;Downloading and Extracting Packages&#xA;typing_extensions-3. | 25 KB     | ############################################################################ | 100%&#xA;mkl-devel-2020.4     | 5.6 MB    | ############################################################################ | 100%&#xA;mkl-include-2020.4   | 696 KB    | ############################################################################ | 100%&#xA;blas-devel-3.9.0     | 12 KB     | ############################################################################ | 100%&#xA;pytorch-1.8.1        | 1.53 GB   | ############################################################################ | 100%&#xA;liblapacke-3.9.0     | 3.9 MB    | ############################################################################ | 100%&#xA;blas-2.108           | 13 KB     | ############################################################################ | 100%&#xA;Preparing transaction: done&#xA;Verifying transaction: done&#xA;Executing transaction: - &amp;quot;By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html&amp;quot;&#xA;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;De todos modos tuve problemas al ejecutar el proyecto, instale algunas paqueterias necesarias, pero tira errores que sin un virtual enviroment no daba, como:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;(tenpy) C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend&amp;gt;python main.py&#xA;starting up on 127.0.0.1 port 65432&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master&#xA;Loading weights:  None&#xA;Using cache found in C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\main.py&amp;quot;, line 26, in &amp;lt;module&amp;gt;&#xA;    depth_estimator = DepthEstimator()&#xA;  File &amp;quot;C:\Users\MIPC\Desktop\MATI\Vtuber_HP\VtuberProject\Assets\TrackingBackend\utils\depth_estimator.py&amp;quot;, line 8, in __init__&#xA;    self.midas = torch.hub.load(&amp;quot;intel-isl/MiDaS&amp;quot;, &amp;quot;MiDaS&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\hubconf.py&amp;quot;, line 15, in MiDaS&#xA;    model = MidasNet()&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\midas_net.py&amp;quot;, line 30, in __init__&#xA;    self.pretrained, self.scratch = _make_encoder(backbone=&amp;quot;resnext101_wsl&amp;quot;, features=features, use_pretrained=use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 7, in _make_encoder&#xA;    pretrained = _make_pretrained_resnext101_wsl(use_pretrained)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\intel-isl_MiDaS_master\midas\blocks.py&amp;quot;, line 85, in _make_pretrained_resnext101_wsl&#xA;    resnet = torch.hub.load(&amp;quot;facebookresearch/WSL-Images&amp;quot;, &amp;quot;resnext101_32x8d_wsl&amp;quot;)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 339, in load&#xA;    model = _load_local(repo_or_dir, model, *args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC\anaconda3\envs\tenpy\lib\site-packages\torch\hub.py&amp;quot;, line 368, in _load_local&#xA;    model = entry(*args, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 39, in resnext101_32x8d_wsl&#xA;    return _resnext('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], True, progress, **kwargs)&#xA;  File &amp;quot;C:\Users\MIPC/.cache\torch\hub\facebookresearch_WSL-Images_master\hubconf.py&amp;quot;, line 23, in _resnext&#xA;    model = ResNet(block, layers, **kwargs)&#xA;TypeError: __init__() got an unexpected keyword argument 'groups'&#xA;[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-wvn_it83\opencv\modules\videoio\src\cap_msmf.cpp (434) `anonymous-namespace'::SourceReaderCB::~SourceReaderCB terminating async callback&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Osea que supongo que tampoco es una solucion viable.&lt;/p&gt;&#xA;&lt;p&gt;Lo que me queda pensar es lo del tema que quizas CUDA Toolkit, cuDNN o/y pyTorch con GPU, no son complatibles con mi Nvidia 730 GT&lt;/p&gt;&#xA;&lt;p&gt;Cheque aqui y como se ve en la imagen encontre mi placa de video en uno de los apartados, aunque no entiendo bien que significa (?)&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/cuda-gpus#compute&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://developer.nvidia.com/cuda-gpus#compute&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/97oBl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/97oBl.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Lo que no entiendo es como segun el Compute Capability asociado a la placa puedo saber si es o no compatible y cual version debo descargar, quizas estube probando todo este tiempo con la 11.1 pero enrealidad necesito otra o no se la verdad...&lt;/p&gt;&#xA;&lt;p&gt;Como hay 2 de las 730 GT mando foto de la caja de la mia, no se cual es realmente, aunque dice 2GB RAM DDR3:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/EuWbj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EuWbj.png&quot; alt=&quot;introducir la descripcin de la imagen aqu&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Que version deberia usar? Hay alguna compatible?&#xA;Espero puedan ayudarme&lt;/p&gt;&#xA;"" OwnerUserId=""77969"" LastEditorUserId=""77969"" LastEditDate=""2021-04-04T17:13:49.847"" LastActivityDate=""2021-08-24T20:44:50.740"" Title=""No puedo usar pytorch 11.1 con GPU, usando una NVIDIA 730 GT, que debo hacer"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;librera&gt;&lt;tensorflow&gt;&lt;cuda&gt;"" AnswerCount=""1"" CommentCount=""9"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""453508"" PostTypeId=""1"" CreationDate=""2021-05-20T11:59:01.210"" Score=""1"" ViewCount=""131"" Body=""&lt;p&gt;Entren varios modelos descargados del model zoo de Tensorflow 2, pero cuando los paso a formato .pb con exporter_main_v2.py del object detection de TensorFlow model garden, durante el proceso me alerta con varios Warnings que dicen:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;WARNING:tensorflow:Skipping full serialization of Keras layer &amp;lt;tensorflow.python.keras.layers.core.Lambda object at 0x000002B0CDD382C8&amp;gt;, because it is not built.&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;La versin de Tensorflow que uso es la 2.4.1&lt;/p&gt;&#xA;&lt;p&gt;Me falta instalar algo?&lt;/p&gt;&#xA;"" OwnerUserId=""229142"" LastEditorUserId=""108737"" LastEditDate=""2021-05-20T12:33:08.180"" LastActivityDate=""2021-05-20T12:33:08.180"" Title=""Problema al guardar modelo Tensorflow 2"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;inteligencia-artificial&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/es.stackoverflow.com,"  <row Id=""453508"" PostTypeId=""1"" CreationDate=""2021-05-20T11:59:01.210"" Score=""1"" ViewCount=""131"" Body=""&lt;p&gt;Entren varios modelos descargados del model zoo de Tensorflow 2, pero cuando los paso a formato .pb con exporter_main_v2.py del object detection de TensorFlow model garden, durante el proceso me alerta con varios Warnings que dicen:&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;WARNING:tensorflow:Skipping full serialization of Keras layer &amp;lt;tensorflow.python.keras.layers.core.Lambda object at 0x000002B0CDD382C8&amp;gt;, because it is not built.&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;La versin de Tensorflow que uso es la 2.4.1&lt;/p&gt;&#xA;&lt;p&gt;Me falta instalar algo?&lt;/p&gt;&#xA;"" OwnerUserId=""229142"" LastEditorUserId=""108737"" LastEditDate=""2021-05-20T12:33:08.180"" LastActivityDate=""2021-05-20T12:33:08.180"" Title=""Problema al guardar modelo Tensorflow 2"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;inteligencia-artificial&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""43168"" PostTypeId=""1"" CreationDate=""2018-04-11T14:10:53.787"" Score=""1"" ViewCount=""1950"" Body=""&lt;p&gt;Mac book TensorFlowTFlearnpython&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Shiraishi-Sadaaki-no-Macbook-ea:~ shiraishisadaaki$ python&#xA;Python 3.5.0 (default, Jun 14 2017, 14:38:36)&#xA;[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;/Users/shiraishisadaaki/.pyenv/3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.&#xA;  from ._conv import register_converters as _register_converters&#xA;WARNING:tensorflow:From /Users/shiraishisadaaki/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use the retry module or similar alternatives.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""28122"" LastEditorUserId=""19110"" LastEditDate=""2018-04-12T00:34:25.263"" LastActivityDate=""2018-08-20T14:01:47.337"" Title=""pythonTFlearn"" Tags=""&lt;python&gt;&lt;&gt;&lt;tensorflow&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""3"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""43168"" PostTypeId=""1"" CreationDate=""2018-04-11T14:10:53.787"" Score=""1"" ViewCount=""1950"" Body=""&lt;p&gt;Mac book TensorFlowTFlearnpython&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Shiraishi-Sadaaki-no-Macbook-ea:~ shiraishisadaaki$ python&#xA;Python 3.5.0 (default, Jun 14 2017, 14:38:36)&#xA;[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;/Users/shiraishisadaaki/.pyenv/3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.&#xA;  from ._conv import register_converters as _register_converters&#xA;WARNING:tensorflow:From /Users/shiraishisadaaki/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use the retry module or similar alternatives.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""28122"" LastEditorUserId=""19110"" LastEditDate=""2018-04-12T00:34:25.263"" LastActivityDate=""2018-08-20T14:01:47.337"" Title=""pythonTFlearn"" Tags=""&lt;python&gt;&lt;&gt;&lt;tensorflow&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""3"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""45375"" PostTypeId=""1"" CreationDate=""2018-07-07T00:47:11.640"" Score=""0"" ViewCount=""5735"" Body=""&lt;p&gt;.&lt;br&gt;&#xA;pythonTensorFlowFutureWarning.?&lt;a href=&quot;https://i.stack.imgur.com/RlLuj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RlLuj.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""29205"" LastEditorUserId=""19110"" LastEditDate=""2018-07-07T13:12:12.157"" LastActivityDate=""2018-10-16T01:00:19.563"" Title=""pythonFutureWarning"" Tags=""&lt;python&gt;&lt;python3&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""45375"" PostTypeId=""1"" CreationDate=""2018-07-07T00:47:11.640"" Score=""0"" ViewCount=""5735"" Body=""&lt;p&gt;.&lt;br&gt;&#xA;pythonTensorFlowFutureWarning.?&lt;a href=&quot;https://i.stack.imgur.com/RlLuj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RlLuj.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""29205"" LastEditorUserId=""19110"" LastEditDate=""2018-07-07T13:12:12.157"" LastActivityDate=""2018-10-16T01:00:19.563"" Title=""pythonFutureWarning"" Tags=""&lt;python&gt;&lt;python3&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""43168"" PostTypeId=""1"" CreationDate=""2018-04-11T14:10:53.787"" Score=""1"" ViewCount=""1950"" Body=""&lt;p&gt;Mac book TensorFlowTFlearnpython&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Shiraishi-Sadaaki-no-Macbook-ea:~ shiraishisadaaki$ python&#xA;Python 3.5.0 (default, Jun 14 2017, 14:38:36)&#xA;[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;/Users/shiraishisadaaki/.pyenv/3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.&#xA;  from ._conv import register_converters as _register_converters&#xA;WARNING:tensorflow:From /Users/shiraishisadaaki/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use the retry module or similar alternatives.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""28122"" LastEditorUserId=""19110"" LastEditDate=""2018-04-12T00:34:25.263"" LastActivityDate=""2018-08-20T14:01:47.337"" Title=""pythonTFlearn"" Tags=""&lt;python&gt;&lt;&gt;&lt;tensorflow&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""3"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""43168"" PostTypeId=""1"" CreationDate=""2018-04-11T14:10:53.787"" Score=""1"" ViewCount=""1950"" Body=""&lt;p&gt;Mac book TensorFlowTFlearnpython&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Shiraishi-Sadaaki-no-Macbook-ea:~ shiraishisadaaki$ python&#xA;Python 3.5.0 (default, Jun 14 2017, 14:38:36)&#xA;[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwin&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;/Users/shiraishisadaaki/.pyenv/3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.&#xA;  from ._conv import register_converters as _register_converters&#xA;WARNING:tensorflow:From /Users/shiraishisadaaki/.pyenv/versions/3.5.0/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use the retry module or similar alternatives.&#xA;&amp;gt;&amp;gt;&amp;gt; import tflearn&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""28122"" LastEditorUserId=""19110"" LastEditDate=""2018-04-12T00:34:25.263"" LastActivityDate=""2018-08-20T14:01:47.337"" Title=""pythonTFlearn"" Tags=""&lt;python&gt;&lt;&gt;&lt;tensorflow&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""3"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""45375"" PostTypeId=""1"" CreationDate=""2018-07-07T00:47:11.640"" Score=""0"" ViewCount=""5735"" Body=""&lt;p&gt;.&lt;br&gt;&#xA;pythonTensorFlowFutureWarning.?&lt;a href=&quot;https://i.stack.imgur.com/RlLuj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RlLuj.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""29205"" LastEditorUserId=""19110"" LastEditDate=""2018-07-07T13:12:12.157"" LastActivityDate=""2018-10-16T01:00:19.563"" Title=""pythonFutureWarning"" Tags=""&lt;python&gt;&lt;python3&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""45375"" PostTypeId=""1"" CreationDate=""2018-07-07T00:47:11.640"" Score=""0"" ViewCount=""5735"" Body=""&lt;p&gt;.&lt;br&gt;&#xA;pythonTensorFlowFutureWarning.?&lt;a href=&quot;https://i.stack.imgur.com/RlLuj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RlLuj.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""29205"" LastEditorUserId=""19110"" LastEditDate=""2018-07-07T13:12:12.157"" LastActivityDate=""2018-10-16T01:00:19.563"" Title=""pythonFutureWarning"" Tags=""&lt;python&gt;&lt;python3&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""62461"" PostTypeId=""1"" CreationDate=""2020-01-22T14:10:20.063"" Score=""0"" ViewCount=""1097"" Body=""&lt;p&gt;&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;code&gt;torch.autograd.set_detect_anomaly(True)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class UnNormfunc(nn.Module):&#xA;    def __init__(self):&#xA;        super(UnNormfunc, self).__init__()&#xA;&#xA;    def forward(self, x):&#xA;        tempx = x.clone()&#xA;        for i in range(3):&#xA;            tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;        return tempx&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sys:1: RuntimeWarning: Traceback of forward call that caused the error:&#xA;  File &quot;train.py&quot;, line 149, in &amp;lt;module&amp;gt;&#xA;    B_hat, B_hat_d1, B_hat_d2, B_hat_d3, B_hat_d4  = generator(torch.cat([ Norm(gamma_RF), Rmap, Norm(gamma_RF * Rmap) ], 1))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/models.py&quot;, line 133, in forward&#xA;    D4 = self.unNorm(self.final4(x4_0))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/util.py&quot;, line 30, in forward&#xA;    tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;train.py&quot;, line 190, in &amp;lt;module&amp;gt;&#xA;    loss_G.backward()&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py&quot;, line 102, in backward&#xA;    torch.autograd.backward(self, gradient, retain_graph, create_graph)&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py&quot;, line 90, in backward&#xA;    allow_unreachable=True)  # allow_unreachable flag&#xA;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br&gt;&#xA;URL&lt;a href=&quot;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""37552"" LastEditorUserId=""19110"" LastEditDate=""2020-01-23T22:38:26.590"" LastActivityDate=""2020-01-23T22:38:26.590"" Title=""pytorchRunTimeError"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""62461"" PostTypeId=""1"" CreationDate=""2020-01-22T14:10:20.063"" Score=""0"" ViewCount=""1097"" Body=""&lt;p&gt;&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;code&gt;torch.autograd.set_detect_anomaly(True)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class UnNormfunc(nn.Module):&#xA;    def __init__(self):&#xA;        super(UnNormfunc, self).__init__()&#xA;&#xA;    def forward(self, x):&#xA;        tempx = x.clone()&#xA;        for i in range(3):&#xA;            tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;        return tempx&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sys:1: RuntimeWarning: Traceback of forward call that caused the error:&#xA;  File &quot;train.py&quot;, line 149, in &amp;lt;module&amp;gt;&#xA;    B_hat, B_hat_d1, B_hat_d2, B_hat_d3, B_hat_d4  = generator(torch.cat([ Norm(gamma_RF), Rmap, Norm(gamma_RF * Rmap) ], 1))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/models.py&quot;, line 133, in forward&#xA;    D4 = self.unNorm(self.final4(x4_0))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/util.py&quot;, line 30, in forward&#xA;    tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;train.py&quot;, line 190, in &amp;lt;module&amp;gt;&#xA;    loss_G.backward()&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py&quot;, line 102, in backward&#xA;    torch.autograd.backward(self, gradient, retain_graph, create_graph)&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py&quot;, line 90, in backward&#xA;    allow_unreachable=True)  # allow_unreachable flag&#xA;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br&gt;&#xA;URL&lt;a href=&quot;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""37552"" LastEditorUserId=""19110"" LastEditDate=""2020-01-23T22:38:26.590"" LastActivityDate=""2020-01-23T22:38:26.590"" Title=""pytorchRunTimeError"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""62461"" PostTypeId=""1"" CreationDate=""2020-01-22T14:10:20.063"" Score=""0"" ViewCount=""1097"" Body=""&lt;p&gt;&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;code&gt;torch.autograd.set_detect_anomaly(True)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class UnNormfunc(nn.Module):&#xA;    def __init__(self):&#xA;        super(UnNormfunc, self).__init__()&#xA;&#xA;    def forward(self, x):&#xA;        tempx = x.clone()&#xA;        for i in range(3):&#xA;            tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;        return tempx&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sys:1: RuntimeWarning: Traceback of forward call that caused the error:&#xA;  File &quot;train.py&quot;, line 149, in &amp;lt;module&amp;gt;&#xA;    B_hat, B_hat_d1, B_hat_d2, B_hat_d3, B_hat_d4  = generator(torch.cat([ Norm(gamma_RF), Rmap, Norm(gamma_RF * Rmap) ], 1))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/models.py&quot;, line 133, in forward&#xA;    D4 = self.unNorm(self.final4(x4_0))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/util.py&quot;, line 30, in forward&#xA;    tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;train.py&quot;, line 190, in &amp;lt;module&amp;gt;&#xA;    loss_G.backward()&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py&quot;, line 102, in backward&#xA;    torch.autograd.backward(self, gradient, retain_graph, create_graph)&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py&quot;, line 90, in backward&#xA;    allow_unreachable=True)  # allow_unreachable flag&#xA;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br&gt;&#xA;URL&lt;a href=&quot;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""37552"" LastEditorUserId=""19110"" LastEditDate=""2020-01-23T22:38:26.590"" LastActivityDate=""2020-01-23T22:38:26.590"" Title=""pytorchRunTimeError"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""62461"" PostTypeId=""1"" CreationDate=""2020-01-22T14:10:20.063"" Score=""0"" ViewCount=""1097"" Body=""&lt;p&gt;&lt;br&gt;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;code&gt;torch.autograd.set_detect_anomaly(True)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class UnNormfunc(nn.Module):&#xA;    def __init__(self):&#xA;        super(UnNormfunc, self).__init__()&#xA;&#xA;    def forward(self, x):&#xA;        tempx = x.clone()&#xA;        for i in range(3):&#xA;            tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;        return tempx&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sys:1: RuntimeWarning: Traceback of forward call that caused the error:&#xA;  File &quot;train.py&quot;, line 149, in &amp;lt;module&amp;gt;&#xA;    B_hat, B_hat_d1, B_hat_d2, B_hat_d3, B_hat_d4  = generator(torch.cat([ Norm(gamma_RF), Rmap, Norm(gamma_RF * Rmap) ], 1))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/models.py&quot;, line 133, in forward&#xA;    D4 = self.unNorm(self.final4(x4_0))&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 489, in __call__&#xA;    result = self.forward(*input, **kwargs)&#xA;  File &quot;~/hoge/util.py&quot;, line 30, in forward&#xA;    tempx[:,i,:,:] = tempx[:,i,:,:] * std[i] + mean[i]&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;train.py&quot;, line 190, in &amp;lt;module&amp;gt;&#xA;    loss_G.backward()&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py&quot;, line 102, in backward&#xA;    torch.autograd.backward(self, gradient, retain_graph, create_graph)&#xA;  File &quot;~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py&quot;, line 90, in backward&#xA;    allow_unreachable=True)  # allow_unreachable flag&#xA;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br&gt;&#xA;URL&lt;a href=&quot;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.yongfengli.tk/2018/04/13/inplace-operation-in-pytorch.html&lt;/a&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""37552"" LastEditorUserId=""19110"" LastEditDate=""2020-01-23T22:38:26.590"" LastActivityDate=""2020-01-23T22:38:26.590"" Title=""pytorchRunTimeError"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85999"" PostTypeId=""1"" CreationDate=""2022-01-29T08:42:17.437"" Score=""0"" ViewCount=""188"" Body=""&lt;p&gt;Anaconda Navigator Jupyter Labtensorflow-gpu&lt;/p&gt;&#xA;&lt;p&gt;&lt;br /&gt;&#xA;Windows 10 RTX2060Super)&lt;br /&gt;&#xA;Microsoft Visual Studio 2019&lt;br /&gt;&#xA;Nvidia CUDA 11.6&lt;br /&gt;&#xA;Anaconda3.9&lt;/p&gt;&#xA;&lt;p&gt;Anaconda&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; conda create -n twne&#xA;&amp;gt; conda activate twne&#xA;(twne)&amp;gt; conda install python=3.6.10&#xA;(twne)&amp;gt; pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html&#xA;(twne)&amp;gt; conda install tensorflow-gpu=1.14&#xA;(twne)&amp;gt; nvcc -V&#xA;nvcc: NVIDIA (R) Cuda compiler driver&#xA;Copyright (c) 2005-2021 NVIDIA Corporation&#xA;Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021&#xA;Cuda compilation tools, release 11.6, V11.6.55&#xA;Build cuda_11.6.r11.6/compiler.30794723_0&#xA;(twne)&amp;gt;nvidia-smi&#xA;Sat Jan 29 17:04:27 2022&#xA;+-----------------------------------------------------------------------------+&#xA;| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |&#xA;|-------------------------------+----------------------+----------------------+&#xA;| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |&#xA;| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |&#xA;|===============================+======================+======================|&#xA;|   0  GeForce RTX 206... WDDM  | 00000000:08:00.0  On |                  N/A |&#xA;| 29%   30C    P8    19W / 175W |   1502MiB /  8192MiB |     11%      Default |&#xA;+-------------------------------+----------------------+----------------------+&#xA;&#xA;+-----------------------------------------------------------------------------+&#xA;| Processes:                                                                  |&#xA;|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |&#xA;|        ID   ID                                                   Usage      |&#xA;|=============================================================================|&#xA;|    0   N/A  N/A      1424    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A      2208    C+G   ...8wekyb3d8bbwe\Cortana.exe    N/A      |&#xA;|    0   N/A  N/A      7796    C+G   C:\Windows\explorer.exe         N/A      |&#xA;|    0   N/A  N/A      8740    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |&#xA;|    0   N/A  N/A     10004    C+G   ...i\Application\vivaldi.exe    N/A      |&#xA;|    0   N/A  N/A     10256    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |&#xA;|    0   N/A  N/A     10872    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A     11724    C+G   ...nputApp\TextInputHost.exe    N/A      |&#xA;|    0   N/A  N/A     12424    C+G   ...lPanel\SystemSettings.exe    N/A      |&#xA;|    0   N/A  N/A     14316    C+G   ...llpaper\RainWallpaper.exe    N/A      |&#xA;|    0   N/A  N/A     14568    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     14576    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     15348    C+G   ...ram Files\LGHUB\lghub.exe    N/A      |&#xA;|    0   N/A  N/A     16256    C+G   ...aming\Spotify\Spotify.exe    N/A      |&#xA;|    0   N/A  N/A     16908    C+G   ...4.0.3.0\GoogleDriveFS.exe    N/A      |&#xA;|    0   N/A  N/A     18548    C+G   ...t\GoogleIMEJaRenderer.exe    N/A      |&#xA;|    0   N/A  N/A     19144    C+G   ...ions\Sancan210\Sancan.exe    N/A      |&#xA;|    0   N/A  N/A     20204    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |&#xA;+-----------------------------------------------------------------------------+&#xA;(twne)&amp;gt;python&#xA;Python 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; from tensorflow.python.client import device_lib&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; print(device_lib.list_local_devices())&#xA;2022-01-29 17:12:54.466889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2&#xA;2022-01-29 17:12:54.469356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll&#xA;2022-01-29 17:12:54.493073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:&#xA;name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.65&#xA;pciBusID: 0000:08:00.0&#xA;2022-01-29 17:12:54.493153: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.&#xA;2022-01-29 17:12:54.493217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0&#xA;2022-01-29 17:12:54.880700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:&#xA;2022-01-29 17:12:54.880765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0&#xA;2022-01-29 17:12:54.881017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N&#xA;2022-01-29 17:12:54.881165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 6734 MB memory) -&amp;gt; physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)&#xA;[name: &amp;quot;/device:CPU:0&amp;quot;&#xA;device_type: &amp;quot;CPU&amp;quot;&#xA;memory_limit: 268435456&#xA;locality {&#xA;}&#xA;incarnation: 11278547066770496530&#xA;, name: &amp;quot;/device:GPU:0&amp;quot;&#xA;device_type: &amp;quot;GPU&amp;quot;&#xA;memory_limit: 7061500724&#xA;locality {&#xA;  bus_id: 1&#xA;  links {&#xA;  }&#xA;}&#xA;incarnation: 16847822037550859714&#xA;physical_device_desc: &amp;quot;device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5&amp;quot;&#xA;]&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;(twne)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Anaconda Navigator Jupyter Lab&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# &#xA;%cd .\stylegan2&#xA;from function import *&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\hoge\painter\stylegan2&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;Setting up TensorFlow plugin &amp;quot;fused_bias_act.cu&amp;quot;: Preprocessing... Failed!&#xA;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-1-5a9fc02cd5e9&amp;gt; in &amp;lt;module&amp;gt;&#xA;      1 # &#xA;      2 get_ipython().run_line_magic('cd', '.\\stylegan2')&#xA;----&amp;gt; 3 from function import *&#xA;&#xA;~\painter\stylegan2\function.py in &amp;lt;module&amp;gt;&#xA;      8 &#xA;      9 tflib.init_tf()&#xA;---&amp;gt; 10 _G, _D, Gs = pickle.load(open(&amp;quot;../network-tadne.pkl&amp;quot;, &amp;quot;rb&amp;quot;))&#xA;     11 # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.&#xA;     12 # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in __setstate__(self, state)&#xA;    295 &#xA;    296         # Init TensorFlow graph.&#xA;--&amp;gt; 297         self._init_graph()&#xA;    298         self.reset_own_vars()&#xA;    299         tfutil.set_vars({self.find_var(name): value for name, value in state[&amp;quot;variables&amp;quot;]})&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in _init_graph(self)&#xA;    152             with tf.control_dependencies(None):  # ignore surrounding control dependencies&#xA;    153                 self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]&#xA;--&amp;gt; 154                 out_expr = self._build_func(*self.input_templates, **build_kwargs)&#xA;    155 &#xA;    156         # Collect outputs.&#xA;&#xA;&amp;lt;string&amp;gt; in G_synthesis_stylegan2(dlatents_in, dlatent_size, num_channels, resolution, fmap_base, fmap_decay, fmap_min, fmap_max, randomize_noise, architecture, nonlinearity, dtype, resample_kernel, fused_modconv, **_kwargs)&#xA;&#xA;&amp;lt;string&amp;gt; in layer(x, layer_idx, fmaps, kernel, up)&#xA;&#xA;&amp;lt;string&amp;gt; in modulated_conv2d_layer(x, y, fmaps, kernel, up, down, demodulate, resample_kernel, gain, use_wscale, lrmul, fused_modconv, weight_var, mod_weight_var, mod_bias_var)&#xA;&#xA;&amp;lt;string&amp;gt; in apply_bias_act(x, act, alpha, gain, lrmul, bias_var)&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in fused_bias_act(x, b, axis, act, alpha, gain, impl)&#xA;     66         'cuda': _fused_bias_act_cuda,&#xA;     67     }&#xA;---&amp;gt; 68     return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain)&#xA;     69 &#xA;     70 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _fused_bias_act_cuda(x, b, axis, act, alpha, gain)&#xA;    120 &#xA;    121     # CUDA kernel.&#xA;--&amp;gt; 122     cuda_kernel = _get_plugin().fused_bias_act&#xA;    123     cuda_kwargs = dict(axis=axis, act=act_spec.cuda_idx, alpha=alpha, gain=gain)&#xA;    124 &#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _get_plugin()&#xA;     14 &#xA;     15 def _get_plugin():&#xA;---&amp;gt; 16     return custom_ops.get_plugin(os.path.splitext(__file__)[0] + '.cu')&#xA;     17 &#xA;     18 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in get_plugin(cuda_file)&#xA;    109             with tempfile.TemporaryDirectory() as tmp_dir:&#xA;    110                 tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + cuda_file_ext)&#xA;--&amp;gt; 111                 _run_cmd(_prepare_nvcc_cli('&amp;quot;%s&amp;quot; --preprocess -o &amp;quot;%s&amp;quot; --keep --keep-dir &amp;quot;%s&amp;quot;' % (cuda_file, tmp_file, tmp_dir)))&#xA;    112                 with open(tmp_file, 'rb') as f:&#xA;    113                     bad_file_str = ('&amp;quot;' + cuda_file.replace('\\', '/') + '&amp;quot;').encode('utf-8') # __FILE__ in error check macros&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in _run_cmd(cmd)&#xA;     59         status = pipe.close()&#xA;     60     if status is not None:&#xA;---&amp;gt; 61         raise RuntimeError('NVCC returned an error. See below for full command line and output log:\n\n%s\n\n%s' % (cmd, output))&#xA;     62 &#xA;     63 def _prepare_nvcc_cli(opts):&#xA;&#xA;RuntimeError: NVCC returned an error. See below for full command line and output log:&#xA;&#xA;nvcc &amp;quot;C:\Users\hoge\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.cu&amp;quot; --preprocess -o &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u\fused_bias_act_tmp.cu&amp;quot; --keep --keep-dir &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u&amp;quot; --disable-warnings --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\protobuf_archive\src&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\com_google_absl&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\eigen_archive&amp;quot; --compiler-bindir &amp;quot;C:/Program Files (x86)/Microsoft Visual Studio 14.0/vc/bin&amp;quot; 2&amp;gt;&amp;amp;1&#xA;&#xA;nvcc fatal   : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions between 2017 and 2019 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Visual Studio 2019tensorflowGPU&lt;br /&gt;&#xA;&lt;/p&gt;&#xA;"" OwnerUserId=""51106"" LastActivityDate=""2022-01-30T07:15:38.397"" Title=""Windows 10  Anaconda tensorflow-gpu1.14"" Tags=""&lt;python3&gt;&lt;visual-studio&gt;&lt;tensorflow&gt;&lt;cuda&gt;&lt;jupyter-lab&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85999"" PostTypeId=""1"" CreationDate=""2022-01-29T08:42:17.437"" Score=""0"" ViewCount=""188"" Body=""&lt;p&gt;Anaconda Navigator Jupyter Labtensorflow-gpu&lt;/p&gt;&#xA;&lt;p&gt;&lt;br /&gt;&#xA;Windows 10 RTX2060Super)&lt;br /&gt;&#xA;Microsoft Visual Studio 2019&lt;br /&gt;&#xA;Nvidia CUDA 11.6&lt;br /&gt;&#xA;Anaconda3.9&lt;/p&gt;&#xA;&lt;p&gt;Anaconda&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; conda create -n twne&#xA;&amp;gt; conda activate twne&#xA;(twne)&amp;gt; conda install python=3.6.10&#xA;(twne)&amp;gt; pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html&#xA;(twne)&amp;gt; conda install tensorflow-gpu=1.14&#xA;(twne)&amp;gt; nvcc -V&#xA;nvcc: NVIDIA (R) Cuda compiler driver&#xA;Copyright (c) 2005-2021 NVIDIA Corporation&#xA;Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021&#xA;Cuda compilation tools, release 11.6, V11.6.55&#xA;Build cuda_11.6.r11.6/compiler.30794723_0&#xA;(twne)&amp;gt;nvidia-smi&#xA;Sat Jan 29 17:04:27 2022&#xA;+-----------------------------------------------------------------------------+&#xA;| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |&#xA;|-------------------------------+----------------------+----------------------+&#xA;| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |&#xA;| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |&#xA;|===============================+======================+======================|&#xA;|   0  GeForce RTX 206... WDDM  | 00000000:08:00.0  On |                  N/A |&#xA;| 29%   30C    P8    19W / 175W |   1502MiB /  8192MiB |     11%      Default |&#xA;+-------------------------------+----------------------+----------------------+&#xA;&#xA;+-----------------------------------------------------------------------------+&#xA;| Processes:                                                                  |&#xA;|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |&#xA;|        ID   ID                                                   Usage      |&#xA;|=============================================================================|&#xA;|    0   N/A  N/A      1424    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A      2208    C+G   ...8wekyb3d8bbwe\Cortana.exe    N/A      |&#xA;|    0   N/A  N/A      7796    C+G   C:\Windows\explorer.exe         N/A      |&#xA;|    0   N/A  N/A      8740    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |&#xA;|    0   N/A  N/A     10004    C+G   ...i\Application\vivaldi.exe    N/A      |&#xA;|    0   N/A  N/A     10256    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |&#xA;|    0   N/A  N/A     10872    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A     11724    C+G   ...nputApp\TextInputHost.exe    N/A      |&#xA;|    0   N/A  N/A     12424    C+G   ...lPanel\SystemSettings.exe    N/A      |&#xA;|    0   N/A  N/A     14316    C+G   ...llpaper\RainWallpaper.exe    N/A      |&#xA;|    0   N/A  N/A     14568    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     14576    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     15348    C+G   ...ram Files\LGHUB\lghub.exe    N/A      |&#xA;|    0   N/A  N/A     16256    C+G   ...aming\Spotify\Spotify.exe    N/A      |&#xA;|    0   N/A  N/A     16908    C+G   ...4.0.3.0\GoogleDriveFS.exe    N/A      |&#xA;|    0   N/A  N/A     18548    C+G   ...t\GoogleIMEJaRenderer.exe    N/A      |&#xA;|    0   N/A  N/A     19144    C+G   ...ions\Sancan210\Sancan.exe    N/A      |&#xA;|    0   N/A  N/A     20204    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |&#xA;+-----------------------------------------------------------------------------+&#xA;(twne)&amp;gt;python&#xA;Python 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; from tensorflow.python.client import device_lib&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; print(device_lib.list_local_devices())&#xA;2022-01-29 17:12:54.466889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2&#xA;2022-01-29 17:12:54.469356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll&#xA;2022-01-29 17:12:54.493073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:&#xA;name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.65&#xA;pciBusID: 0000:08:00.0&#xA;2022-01-29 17:12:54.493153: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.&#xA;2022-01-29 17:12:54.493217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0&#xA;2022-01-29 17:12:54.880700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:&#xA;2022-01-29 17:12:54.880765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0&#xA;2022-01-29 17:12:54.881017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N&#xA;2022-01-29 17:12:54.881165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 6734 MB memory) -&amp;gt; physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)&#xA;[name: &amp;quot;/device:CPU:0&amp;quot;&#xA;device_type: &amp;quot;CPU&amp;quot;&#xA;memory_limit: 268435456&#xA;locality {&#xA;}&#xA;incarnation: 11278547066770496530&#xA;, name: &amp;quot;/device:GPU:0&amp;quot;&#xA;device_type: &amp;quot;GPU&amp;quot;&#xA;memory_limit: 7061500724&#xA;locality {&#xA;  bus_id: 1&#xA;  links {&#xA;  }&#xA;}&#xA;incarnation: 16847822037550859714&#xA;physical_device_desc: &amp;quot;device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5&amp;quot;&#xA;]&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;(twne)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Anaconda Navigator Jupyter Lab&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# &#xA;%cd .\stylegan2&#xA;from function import *&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\hoge\painter\stylegan2&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;Setting up TensorFlow plugin &amp;quot;fused_bias_act.cu&amp;quot;: Preprocessing... Failed!&#xA;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-1-5a9fc02cd5e9&amp;gt; in &amp;lt;module&amp;gt;&#xA;      1 # &#xA;      2 get_ipython().run_line_magic('cd', '.\\stylegan2')&#xA;----&amp;gt; 3 from function import *&#xA;&#xA;~\painter\stylegan2\function.py in &amp;lt;module&amp;gt;&#xA;      8 &#xA;      9 tflib.init_tf()&#xA;---&amp;gt; 10 _G, _D, Gs = pickle.load(open(&amp;quot;../network-tadne.pkl&amp;quot;, &amp;quot;rb&amp;quot;))&#xA;     11 # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.&#xA;     12 # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in __setstate__(self, state)&#xA;    295 &#xA;    296         # Init TensorFlow graph.&#xA;--&amp;gt; 297         self._init_graph()&#xA;    298         self.reset_own_vars()&#xA;    299         tfutil.set_vars({self.find_var(name): value for name, value in state[&amp;quot;variables&amp;quot;]})&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in _init_graph(self)&#xA;    152             with tf.control_dependencies(None):  # ignore surrounding control dependencies&#xA;    153                 self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]&#xA;--&amp;gt; 154                 out_expr = self._build_func(*self.input_templates, **build_kwargs)&#xA;    155 &#xA;    156         # Collect outputs.&#xA;&#xA;&amp;lt;string&amp;gt; in G_synthesis_stylegan2(dlatents_in, dlatent_size, num_channels, resolution, fmap_base, fmap_decay, fmap_min, fmap_max, randomize_noise, architecture, nonlinearity, dtype, resample_kernel, fused_modconv, **_kwargs)&#xA;&#xA;&amp;lt;string&amp;gt; in layer(x, layer_idx, fmaps, kernel, up)&#xA;&#xA;&amp;lt;string&amp;gt; in modulated_conv2d_layer(x, y, fmaps, kernel, up, down, demodulate, resample_kernel, gain, use_wscale, lrmul, fused_modconv, weight_var, mod_weight_var, mod_bias_var)&#xA;&#xA;&amp;lt;string&amp;gt; in apply_bias_act(x, act, alpha, gain, lrmul, bias_var)&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in fused_bias_act(x, b, axis, act, alpha, gain, impl)&#xA;     66         'cuda': _fused_bias_act_cuda,&#xA;     67     }&#xA;---&amp;gt; 68     return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain)&#xA;     69 &#xA;     70 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _fused_bias_act_cuda(x, b, axis, act, alpha, gain)&#xA;    120 &#xA;    121     # CUDA kernel.&#xA;--&amp;gt; 122     cuda_kernel = _get_plugin().fused_bias_act&#xA;    123     cuda_kwargs = dict(axis=axis, act=act_spec.cuda_idx, alpha=alpha, gain=gain)&#xA;    124 &#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _get_plugin()&#xA;     14 &#xA;     15 def _get_plugin():&#xA;---&amp;gt; 16     return custom_ops.get_plugin(os.path.splitext(__file__)[0] + '.cu')&#xA;     17 &#xA;     18 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in get_plugin(cuda_file)&#xA;    109             with tempfile.TemporaryDirectory() as tmp_dir:&#xA;    110                 tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + cuda_file_ext)&#xA;--&amp;gt; 111                 _run_cmd(_prepare_nvcc_cli('&amp;quot;%s&amp;quot; --preprocess -o &amp;quot;%s&amp;quot; --keep --keep-dir &amp;quot;%s&amp;quot;' % (cuda_file, tmp_file, tmp_dir)))&#xA;    112                 with open(tmp_file, 'rb') as f:&#xA;    113                     bad_file_str = ('&amp;quot;' + cuda_file.replace('\\', '/') + '&amp;quot;').encode('utf-8') # __FILE__ in error check macros&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in _run_cmd(cmd)&#xA;     59         status = pipe.close()&#xA;     60     if status is not None:&#xA;---&amp;gt; 61         raise RuntimeError('NVCC returned an error. See below for full command line and output log:\n\n%s\n\n%s' % (cmd, output))&#xA;     62 &#xA;     63 def _prepare_nvcc_cli(opts):&#xA;&#xA;RuntimeError: NVCC returned an error. See below for full command line and output log:&#xA;&#xA;nvcc &amp;quot;C:\Users\hoge\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.cu&amp;quot; --preprocess -o &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u\fused_bias_act_tmp.cu&amp;quot; --keep --keep-dir &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u&amp;quot; --disable-warnings --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\protobuf_archive\src&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\com_google_absl&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\eigen_archive&amp;quot; --compiler-bindir &amp;quot;C:/Program Files (x86)/Microsoft Visual Studio 14.0/vc/bin&amp;quot; 2&amp;gt;&amp;amp;1&#xA;&#xA;nvcc fatal   : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions between 2017 and 2019 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Visual Studio 2019tensorflowGPU&lt;br /&gt;&#xA;&lt;/p&gt;&#xA;"" OwnerUserId=""51106"" LastActivityDate=""2022-01-30T07:15:38.397"" Title=""Windows 10  Anaconda tensorflow-gpu1.14"" Tags=""&lt;python3&gt;&lt;visual-studio&gt;&lt;tensorflow&gt;&lt;cuda&gt;&lt;jupyter-lab&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85999"" PostTypeId=""1"" CreationDate=""2022-01-29T08:42:17.437"" Score=""0"" ViewCount=""188"" Body=""&lt;p&gt;Anaconda Navigator Jupyter Labtensorflow-gpu&lt;/p&gt;&#xA;&lt;p&gt;&lt;br /&gt;&#xA;Windows 10 RTX2060Super)&lt;br /&gt;&#xA;Microsoft Visual Studio 2019&lt;br /&gt;&#xA;Nvidia CUDA 11.6&lt;br /&gt;&#xA;Anaconda3.9&lt;/p&gt;&#xA;&lt;p&gt;Anaconda&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; conda create -n twne&#xA;&amp;gt; conda activate twne&#xA;(twne)&amp;gt; conda install python=3.6.10&#xA;(twne)&amp;gt; pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html&#xA;(twne)&amp;gt; conda install tensorflow-gpu=1.14&#xA;(twne)&amp;gt; nvcc -V&#xA;nvcc: NVIDIA (R) Cuda compiler driver&#xA;Copyright (c) 2005-2021 NVIDIA Corporation&#xA;Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021&#xA;Cuda compilation tools, release 11.6, V11.6.55&#xA;Build cuda_11.6.r11.6/compiler.30794723_0&#xA;(twne)&amp;gt;nvidia-smi&#xA;Sat Jan 29 17:04:27 2022&#xA;+-----------------------------------------------------------------------------+&#xA;| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |&#xA;|-------------------------------+----------------------+----------------------+&#xA;| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |&#xA;| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |&#xA;|===============================+======================+======================|&#xA;|   0  GeForce RTX 206... WDDM  | 00000000:08:00.0  On |                  N/A |&#xA;| 29%   30C    P8    19W / 175W |   1502MiB /  8192MiB |     11%      Default |&#xA;+-------------------------------+----------------------+----------------------+&#xA;&#xA;+-----------------------------------------------------------------------------+&#xA;| Processes:                                                                  |&#xA;|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |&#xA;|        ID   ID                                                   Usage      |&#xA;|=============================================================================|&#xA;|    0   N/A  N/A      1424    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A      2208    C+G   ...8wekyb3d8bbwe\Cortana.exe    N/A      |&#xA;|    0   N/A  N/A      7796    C+G   C:\Windows\explorer.exe         N/A      |&#xA;|    0   N/A  N/A      8740    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |&#xA;|    0   N/A  N/A     10004    C+G   ...i\Application\vivaldi.exe    N/A      |&#xA;|    0   N/A  N/A     10256    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |&#xA;|    0   N/A  N/A     10872    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A     11724    C+G   ...nputApp\TextInputHost.exe    N/A      |&#xA;|    0   N/A  N/A     12424    C+G   ...lPanel\SystemSettings.exe    N/A      |&#xA;|    0   N/A  N/A     14316    C+G   ...llpaper\RainWallpaper.exe    N/A      |&#xA;|    0   N/A  N/A     14568    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     14576    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     15348    C+G   ...ram Files\LGHUB\lghub.exe    N/A      |&#xA;|    0   N/A  N/A     16256    C+G   ...aming\Spotify\Spotify.exe    N/A      |&#xA;|    0   N/A  N/A     16908    C+G   ...4.0.3.0\GoogleDriveFS.exe    N/A      |&#xA;|    0   N/A  N/A     18548    C+G   ...t\GoogleIMEJaRenderer.exe    N/A      |&#xA;|    0   N/A  N/A     19144    C+G   ...ions\Sancan210\Sancan.exe    N/A      |&#xA;|    0   N/A  N/A     20204    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |&#xA;+-----------------------------------------------------------------------------+&#xA;(twne)&amp;gt;python&#xA;Python 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; from tensorflow.python.client import device_lib&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; print(device_lib.list_local_devices())&#xA;2022-01-29 17:12:54.466889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2&#xA;2022-01-29 17:12:54.469356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll&#xA;2022-01-29 17:12:54.493073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:&#xA;name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.65&#xA;pciBusID: 0000:08:00.0&#xA;2022-01-29 17:12:54.493153: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.&#xA;2022-01-29 17:12:54.493217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0&#xA;2022-01-29 17:12:54.880700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:&#xA;2022-01-29 17:12:54.880765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0&#xA;2022-01-29 17:12:54.881017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N&#xA;2022-01-29 17:12:54.881165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 6734 MB memory) -&amp;gt; physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)&#xA;[name: &amp;quot;/device:CPU:0&amp;quot;&#xA;device_type: &amp;quot;CPU&amp;quot;&#xA;memory_limit: 268435456&#xA;locality {&#xA;}&#xA;incarnation: 11278547066770496530&#xA;, name: &amp;quot;/device:GPU:0&amp;quot;&#xA;device_type: &amp;quot;GPU&amp;quot;&#xA;memory_limit: 7061500724&#xA;locality {&#xA;  bus_id: 1&#xA;  links {&#xA;  }&#xA;}&#xA;incarnation: 16847822037550859714&#xA;physical_device_desc: &amp;quot;device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5&amp;quot;&#xA;]&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;(twne)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Anaconda Navigator Jupyter Lab&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# &#xA;%cd .\stylegan2&#xA;from function import *&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\hoge\painter\stylegan2&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;Setting up TensorFlow plugin &amp;quot;fused_bias_act.cu&amp;quot;: Preprocessing... Failed!&#xA;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-1-5a9fc02cd5e9&amp;gt; in &amp;lt;module&amp;gt;&#xA;      1 # &#xA;      2 get_ipython().run_line_magic('cd', '.\\stylegan2')&#xA;----&amp;gt; 3 from function import *&#xA;&#xA;~\painter\stylegan2\function.py in &amp;lt;module&amp;gt;&#xA;      8 &#xA;      9 tflib.init_tf()&#xA;---&amp;gt; 10 _G, _D, Gs = pickle.load(open(&amp;quot;../network-tadne.pkl&amp;quot;, &amp;quot;rb&amp;quot;))&#xA;     11 # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.&#xA;     12 # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in __setstate__(self, state)&#xA;    295 &#xA;    296         # Init TensorFlow graph.&#xA;--&amp;gt; 297         self._init_graph()&#xA;    298         self.reset_own_vars()&#xA;    299         tfutil.set_vars({self.find_var(name): value for name, value in state[&amp;quot;variables&amp;quot;]})&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in _init_graph(self)&#xA;    152             with tf.control_dependencies(None):  # ignore surrounding control dependencies&#xA;    153                 self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]&#xA;--&amp;gt; 154                 out_expr = self._build_func(*self.input_templates, **build_kwargs)&#xA;    155 &#xA;    156         # Collect outputs.&#xA;&#xA;&amp;lt;string&amp;gt; in G_synthesis_stylegan2(dlatents_in, dlatent_size, num_channels, resolution, fmap_base, fmap_decay, fmap_min, fmap_max, randomize_noise, architecture, nonlinearity, dtype, resample_kernel, fused_modconv, **_kwargs)&#xA;&#xA;&amp;lt;string&amp;gt; in layer(x, layer_idx, fmaps, kernel, up)&#xA;&#xA;&amp;lt;string&amp;gt; in modulated_conv2d_layer(x, y, fmaps, kernel, up, down, demodulate, resample_kernel, gain, use_wscale, lrmul, fused_modconv, weight_var, mod_weight_var, mod_bias_var)&#xA;&#xA;&amp;lt;string&amp;gt; in apply_bias_act(x, act, alpha, gain, lrmul, bias_var)&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in fused_bias_act(x, b, axis, act, alpha, gain, impl)&#xA;     66         'cuda': _fused_bias_act_cuda,&#xA;     67     }&#xA;---&amp;gt; 68     return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain)&#xA;     69 &#xA;     70 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _fused_bias_act_cuda(x, b, axis, act, alpha, gain)&#xA;    120 &#xA;    121     # CUDA kernel.&#xA;--&amp;gt; 122     cuda_kernel = _get_plugin().fused_bias_act&#xA;    123     cuda_kwargs = dict(axis=axis, act=act_spec.cuda_idx, alpha=alpha, gain=gain)&#xA;    124 &#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _get_plugin()&#xA;     14 &#xA;     15 def _get_plugin():&#xA;---&amp;gt; 16     return custom_ops.get_plugin(os.path.splitext(__file__)[0] + '.cu')&#xA;     17 &#xA;     18 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in get_plugin(cuda_file)&#xA;    109             with tempfile.TemporaryDirectory() as tmp_dir:&#xA;    110                 tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + cuda_file_ext)&#xA;--&amp;gt; 111                 _run_cmd(_prepare_nvcc_cli('&amp;quot;%s&amp;quot; --preprocess -o &amp;quot;%s&amp;quot; --keep --keep-dir &amp;quot;%s&amp;quot;' % (cuda_file, tmp_file, tmp_dir)))&#xA;    112                 with open(tmp_file, 'rb') as f:&#xA;    113                     bad_file_str = ('&amp;quot;' + cuda_file.replace('\\', '/') + '&amp;quot;').encode('utf-8') # __FILE__ in error check macros&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in _run_cmd(cmd)&#xA;     59         status = pipe.close()&#xA;     60     if status is not None:&#xA;---&amp;gt; 61         raise RuntimeError('NVCC returned an error. See below for full command line and output log:\n\n%s\n\n%s' % (cmd, output))&#xA;     62 &#xA;     63 def _prepare_nvcc_cli(opts):&#xA;&#xA;RuntimeError: NVCC returned an error. See below for full command line and output log:&#xA;&#xA;nvcc &amp;quot;C:\Users\hoge\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.cu&amp;quot; --preprocess -o &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u\fused_bias_act_tmp.cu&amp;quot; --keep --keep-dir &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u&amp;quot; --disable-warnings --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\protobuf_archive\src&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\com_google_absl&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\eigen_archive&amp;quot; --compiler-bindir &amp;quot;C:/Program Files (x86)/Microsoft Visual Studio 14.0/vc/bin&amp;quot; 2&amp;gt;&amp;amp;1&#xA;&#xA;nvcc fatal   : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions between 2017 and 2019 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Visual Studio 2019tensorflowGPU&lt;br /&gt;&#xA;&lt;/p&gt;&#xA;"" OwnerUserId=""51106"" LastActivityDate=""2022-01-30T07:15:38.397"" Title=""Windows 10  Anaconda tensorflow-gpu1.14"" Tags=""&lt;python3&gt;&lt;visual-studio&gt;&lt;tensorflow&gt;&lt;cuda&gt;&lt;jupyter-lab&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85203"" PostTypeId=""1"" CreationDate=""2021-12-17T13:16:34.390"" Score=""2"" ViewCount=""3902"" Body=""&lt;p&gt;&lt;/p&gt;&#xA;&lt;p&gt;pytorchtorchtorchvisiontorchvision&lt;br /&gt;&#xA;Warning&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;import torch.nn as nn&#xA;import torch.nn.functional as f&#xA;from torch.utils.data import DataLoader&#xA;import torchvision&#xA;import torchvision.transforms as transforms&#xA;import torch.optim as optim&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    print(torch.cuda.is_available())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\username\PycharmProjects\pukatorch5\venv\lib\site-packages\torchvision\io\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\Users\username\PycharmProjects\pukatorch5\venv\Lib\site-packages\torchvision\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.&lt;br /&gt;&#xA;warn(f&amp;quot;Failed to load image Python extension: {e}&amp;quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Could not find moduleimage.pyd&lt;/p&gt;&#xA;&lt;p&gt;Package           Version&lt;/p&gt;&#xA;&lt;hr /&gt;&#xA;&lt;p&gt;numpy             1.21.4&lt;br /&gt;&#xA;Pillow            8.4.0&lt;br /&gt;&#xA;pip               21.3.1&lt;br /&gt;&#xA;setuptools        40.8.0&lt;br /&gt;&#xA;torch             1.10.1+cu102&lt;br /&gt;&#xA;torchaudio        0.10.1+cu102&lt;br /&gt;&#xA;torchvision       0.11.2+cu102&lt;br /&gt;&#xA;typing_extensions 4.0.1&lt;/p&gt;&#xA;&lt;p&gt;Python3.8&lt;/p&gt;&#xA;"" OwnerUserId=""50550"" LastActivityDate=""2021-12-17T21:37:58.437"" Title=""torchvisionimage.pyd"" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;torch&gt;"" AnswerCount=""1"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ja.stackoverflow.com,"  <row Id=""85999"" PostTypeId=""1"" CreationDate=""2022-01-29T08:42:17.437"" Score=""0"" ViewCount=""188"" Body=""&lt;p&gt;Anaconda Navigator Jupyter Labtensorflow-gpu&lt;/p&gt;&#xA;&lt;p&gt;&lt;br /&gt;&#xA;Windows 10 RTX2060Super)&lt;br /&gt;&#xA;Microsoft Visual Studio 2019&lt;br /&gt;&#xA;Nvidia CUDA 11.6&lt;br /&gt;&#xA;Anaconda3.9&lt;/p&gt;&#xA;&lt;p&gt;Anaconda&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; conda create -n twne&#xA;&amp;gt; conda activate twne&#xA;(twne)&amp;gt; conda install python=3.6.10&#xA;(twne)&amp;gt; pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html&#xA;(twne)&amp;gt; conda install tensorflow-gpu=1.14&#xA;(twne)&amp;gt; nvcc -V&#xA;nvcc: NVIDIA (R) Cuda compiler driver&#xA;Copyright (c) 2005-2021 NVIDIA Corporation&#xA;Built on Fri_Dec_17_18:28:54_Pacific_Standard_Time_2021&#xA;Cuda compilation tools, release 11.6, V11.6.55&#xA;Build cuda_11.6.r11.6/compiler.30794723_0&#xA;(twne)&amp;gt;nvidia-smi&#xA;Sat Jan 29 17:04:27 2022&#xA;+-----------------------------------------------------------------------------+&#xA;| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |&#xA;|-------------------------------+----------------------+----------------------+&#xA;| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |&#xA;| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |&#xA;|===============================+======================+======================|&#xA;|   0  GeForce RTX 206... WDDM  | 00000000:08:00.0  On |                  N/A |&#xA;| 29%   30C    P8    19W / 175W |   1502MiB /  8192MiB |     11%      Default |&#xA;+-------------------------------+----------------------+----------------------+&#xA;&#xA;+-----------------------------------------------------------------------------+&#xA;| Processes:                                                                  |&#xA;|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |&#xA;|        ID   ID                                                   Usage      |&#xA;|=============================================================================|&#xA;|    0   N/A  N/A      1424    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A      2208    C+G   ...8wekyb3d8bbwe\Cortana.exe    N/A      |&#xA;|    0   N/A  N/A      7796    C+G   C:\Windows\explorer.exe         N/A      |&#xA;|    0   N/A  N/A      8740    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |&#xA;|    0   N/A  N/A     10004    C+G   ...i\Application\vivaldi.exe    N/A      |&#xA;|    0   N/A  N/A     10256    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |&#xA;|    0   N/A  N/A     10872    C+G   Insufficient Permissions        N/A      |&#xA;|    0   N/A  N/A     11724    C+G   ...nputApp\TextInputHost.exe    N/A      |&#xA;|    0   N/A  N/A     12424    C+G   ...lPanel\SystemSettings.exe    N/A      |&#xA;|    0   N/A  N/A     14316    C+G   ...llpaper\RainWallpaper.exe    N/A      |&#xA;|    0   N/A  N/A     14568    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     14576    C+G   ...inWallpaper\videocore.exe    N/A      |&#xA;|    0   N/A  N/A     15348    C+G   ...ram Files\LGHUB\lghub.exe    N/A      |&#xA;|    0   N/A  N/A     16256    C+G   ...aming\Spotify\Spotify.exe    N/A      |&#xA;|    0   N/A  N/A     16908    C+G   ...4.0.3.0\GoogleDriveFS.exe    N/A      |&#xA;|    0   N/A  N/A     18548    C+G   ...t\GoogleIMEJaRenderer.exe    N/A      |&#xA;|    0   N/A  N/A     19144    C+G   ...ions\Sancan210\Sancan.exe    N/A      |&#xA;|    0   N/A  N/A     20204    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |&#xA;+-----------------------------------------------------------------------------+&#xA;(twne)&amp;gt;python&#xA;Python 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)] on win32&#xA;Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.&#xA;&amp;gt;&amp;gt;&amp;gt; from tensorflow.python.client import device_lib&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;&amp;gt;&amp;gt;&amp;gt; print(device_lib.list_local_devices())&#xA;2022-01-29 17:12:54.466889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2&#xA;2022-01-29 17:12:54.469356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll&#xA;2022-01-29 17:12:54.493073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:&#xA;name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.65&#xA;pciBusID: 0000:08:00.0&#xA;2022-01-29 17:12:54.493153: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.&#xA;2022-01-29 17:12:54.493217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0&#xA;2022-01-29 17:12:54.880700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:&#xA;2022-01-29 17:12:54.880765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0&#xA;2022-01-29 17:12:54.881017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N&#xA;2022-01-29 17:12:54.881165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 6734 MB memory) -&amp;gt; physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5)&#xA;[name: &amp;quot;/device:CPU:0&amp;quot;&#xA;device_type: &amp;quot;CPU&amp;quot;&#xA;memory_limit: 268435456&#xA;locality {&#xA;}&#xA;incarnation: 11278547066770496530&#xA;, name: &amp;quot;/device:GPU:0&amp;quot;&#xA;device_type: &amp;quot;GPU&amp;quot;&#xA;memory_limit: 7061500724&#xA;locality {&#xA;  bus_id: 1&#xA;  links {&#xA;  }&#xA;}&#xA;incarnation: 16847822037550859714&#xA;physical_device_desc: &amp;quot;device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5&amp;quot;&#xA;]&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;(twne)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Anaconda Navigator Jupyter Lab&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# &#xA;%cd .\stylegan2&#xA;from function import *&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\hoge\painter\stylegan2&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&amp;quot;qint8&amp;quot;, np.int8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&amp;quot;quint8&amp;quot;, np.uint8, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&amp;quot;qint16&amp;quot;, np.int16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&amp;quot;quint16&amp;quot;, np.uint16, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&amp;quot;qint32&amp;quot;, np.int32, 1)])&#xA;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&amp;quot;resource&amp;quot;, np.ubyte, 1)])&#xA;Setting up TensorFlow plugin &amp;quot;fused_bias_act.cu&amp;quot;: Preprocessing... Failed!&#xA;---------------------------------------------------------------------------&#xA;RuntimeError                              Traceback (most recent call last)&#xA;&amp;lt;ipython-input-1-5a9fc02cd5e9&amp;gt; in &amp;lt;module&amp;gt;&#xA;      1 # &#xA;      2 get_ipython().run_line_magic('cd', '.\\stylegan2')&#xA;----&amp;gt; 3 from function import *&#xA;&#xA;~\painter\stylegan2\function.py in &amp;lt;module&amp;gt;&#xA;      8 &#xA;      9 tflib.init_tf()&#xA;---&amp;gt; 10 _G, _D, Gs = pickle.load(open(&amp;quot;../network-tadne.pkl&amp;quot;, &amp;quot;rb&amp;quot;))&#xA;     11 # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.&#xA;     12 # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in __setstate__(self, state)&#xA;    295 &#xA;    296         # Init TensorFlow graph.&#xA;--&amp;gt; 297         self._init_graph()&#xA;    298         self.reset_own_vars()&#xA;    299         tfutil.set_vars({self.find_var(name): value for name, value in state[&amp;quot;variables&amp;quot;]})&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\network.py in _init_graph(self)&#xA;    152             with tf.control_dependencies(None):  # ignore surrounding control dependencies&#xA;    153                 self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]&#xA;--&amp;gt; 154                 out_expr = self._build_func(*self.input_templates, **build_kwargs)&#xA;    155 &#xA;    156         # Collect outputs.&#xA;&#xA;&amp;lt;string&amp;gt; in G_synthesis_stylegan2(dlatents_in, dlatent_size, num_channels, resolution, fmap_base, fmap_decay, fmap_min, fmap_max, randomize_noise, architecture, nonlinearity, dtype, resample_kernel, fused_modconv, **_kwargs)&#xA;&#xA;&amp;lt;string&amp;gt; in layer(x, layer_idx, fmaps, kernel, up)&#xA;&#xA;&amp;lt;string&amp;gt; in modulated_conv2d_layer(x, y, fmaps, kernel, up, down, demodulate, resample_kernel, gain, use_wscale, lrmul, fused_modconv, weight_var, mod_weight_var, mod_bias_var)&#xA;&#xA;&amp;lt;string&amp;gt; in apply_bias_act(x, act, alpha, gain, lrmul, bias_var)&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in fused_bias_act(x, b, axis, act, alpha, gain, impl)&#xA;     66         'cuda': _fused_bias_act_cuda,&#xA;     67     }&#xA;---&amp;gt; 68     return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain)&#xA;     69 &#xA;     70 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _fused_bias_act_cuda(x, b, axis, act, alpha, gain)&#xA;    120 &#xA;    121     # CUDA kernel.&#xA;--&amp;gt; 122     cuda_kernel = _get_plugin().fused_bias_act&#xA;    123     cuda_kwargs = dict(axis=axis, act=act_spec.cuda_idx, alpha=alpha, gain=gain)&#xA;    124 &#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.py in _get_plugin()&#xA;     14 &#xA;     15 def _get_plugin():&#xA;---&amp;gt; 16     return custom_ops.get_plugin(os.path.splitext(__file__)[0] + '.cu')&#xA;     17 &#xA;     18 #----------------------------------------------------------------------------&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in get_plugin(cuda_file)&#xA;    109             with tempfile.TemporaryDirectory() as tmp_dir:&#xA;    110                 tmp_file = os.path.join(tmp_dir, cuda_file_name + '_tmp' + cuda_file_ext)&#xA;--&amp;gt; 111                 _run_cmd(_prepare_nvcc_cli('&amp;quot;%s&amp;quot; --preprocess -o &amp;quot;%s&amp;quot; --keep --keep-dir &amp;quot;%s&amp;quot;' % (cuda_file, tmp_file, tmp_dir)))&#xA;    112                 with open(tmp_file, 'rb') as f:&#xA;    113                     bad_file_str = ('&amp;quot;' + cuda_file.replace('\\', '/') + '&amp;quot;').encode('utf-8') # __FILE__ in error check macros&#xA;&#xA;~\painter\stylegan2\dnnlib\tflib\custom_ops.py in _run_cmd(cmd)&#xA;     59         status = pipe.close()&#xA;     60     if status is not None:&#xA;---&amp;gt; 61         raise RuntimeError('NVCC returned an error. See below for full command line and output log:\n\n%s\n\n%s' % (cmd, output))&#xA;     62 &#xA;     63 def _prepare_nvcc_cli(opts):&#xA;&#xA;RuntimeError: NVCC returned an error. See below for full command line and output log:&#xA;&#xA;nvcc &amp;quot;C:\Users\hoge\painter\stylegan2\dnnlib\tflib\ops\fused_bias_act.cu&amp;quot; --preprocess -o &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u\fused_bias_act_tmp.cu&amp;quot; --keep --keep-dir &amp;quot;C:\Users\hoge\AppData\Local\Temp\tmpgwzv8k1u&amp;quot; --disable-warnings --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\protobuf_archive\src&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\com_google_absl&amp;quot; --include-path &amp;quot;C:\Users\hoge\anaconda3\envs\twne\lib\site-packages\tensorflow\include\external\eigen_archive&amp;quot; --compiler-bindir &amp;quot;C:/Program Files (x86)/Microsoft Visual Studio 14.0/vc/bin&amp;quot; 2&amp;gt;&amp;amp;1&#xA;&#xA;nvcc fatal   : nvcc cannot find a supported version of Microsoft Visual Studio. Only the versions between 2017 and 2019 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Visual Studio 2019tensorflowGPU&lt;br /&gt;&#xA;&lt;/p&gt;&#xA;"" OwnerUserId=""51106"" LastActivityDate=""2022-01-30T07:15:38.397"" Title=""Windows 10  Anaconda tensorflow-gpu1.14"" Tags=""&lt;python3&gt;&lt;visual-studio&gt;&lt;tensorflow&gt;&lt;cuda&gt;&lt;jupyter-lab&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""276341"" PostTypeId=""1"" AcceptedAnswerId=""276367"" CreationDate=""2018-02-14T21:15:57.210"" Score=""0"" ViewCount=""1210"" Body=""&lt;p&gt;Eu estou seguindo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow depois de dois dias preparando o ambiente em Anaconda Eu finalmente consegui executar &lt;code&gt;premade_estimator.py&lt;/code&gt; usando o cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mas quando tento executar o mesmo cdigo no jupyter, recebo esse erro:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Tentei corrigi-lo sem sucesso com essas linhas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Qualquer idia ser apreciada! Obrigado!&lt;/p&gt;&#xA;"" OwnerUserId=""102709"" LastEditorUserId=""102709"" LastEditDate=""2018-02-14T23:59:53.290"" LastActivityDate=""2018-02-14T23:59:53.290"" Title=""Como corrigir ipykernel_launcher.py: error: unrecognized arguments no jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""276341"" PostTypeId=""1"" AcceptedAnswerId=""276367"" CreationDate=""2018-02-14T21:15:57.210"" Score=""0"" ViewCount=""1210"" Body=""&lt;p&gt;Eu estou seguindo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow depois de dois dias preparando o ambiente em Anaconda Eu finalmente consegui executar &lt;code&gt;premade_estimator.py&lt;/code&gt; usando o cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mas quando tento executar o mesmo cdigo no jupyter, recebo esse erro:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Tentei corrigi-lo sem sucesso com essas linhas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Qualquer idia ser apreciada! Obrigado!&lt;/p&gt;&#xA;"" OwnerUserId=""102709"" LastEditorUserId=""102709"" LastEditDate=""2018-02-14T23:59:53.290"" LastActivityDate=""2018-02-14T23:59:53.290"" Title=""Como corrigir ipykernel_launcher.py: error: unrecognized arguments no jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""276341"" PostTypeId=""1"" AcceptedAnswerId=""276367"" CreationDate=""2018-02-14T21:15:57.210"" Score=""0"" ViewCount=""1210"" Body=""&lt;p&gt;Eu estou seguindo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow depois de dois dias preparando o ambiente em Anaconda Eu finalmente consegui executar &lt;code&gt;premade_estimator.py&lt;/code&gt; usando o cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mas quando tento executar o mesmo cdigo no jupyter, recebo esse erro:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Tentei corrigi-lo sem sucesso com essas linhas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Qualquer idia ser apreciada! Obrigado!&lt;/p&gt;&#xA;"" OwnerUserId=""102709"" LastEditorUserId=""102709"" LastEditDate=""2018-02-14T23:59:53.290"" LastActivityDate=""2018-02-14T23:59:53.290"" Title=""Como corrigir ipykernel_launcher.py: error: unrecognized arguments no jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""276341"" PostTypeId=""1"" AcceptedAnswerId=""276367"" CreationDate=""2018-02-14T21:15:57.210"" Score=""0"" ViewCount=""1210"" Body=""&lt;p&gt;Eu estou seguindo este &lt;a href=&quot;https://www.tensorflow.org/get_started/get_started_for_beginners&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; de tensorflow depois de dois dias preparando o ambiente em Anaconda Eu finalmente consegui executar &lt;code&gt;premade_estimator.py&lt;/code&gt; usando o cmd&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DmlmX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DmlmX.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mas quando tento executar o mesmo cdigo no jupyter, recebo esse erro:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]&#xA;                             [--train_steps TRAIN_STEPS]&#xA;&#xA;ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;An exception has occurred, use %tb to see the full traceback.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SystemExit: 2&#xA;&#xA;C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: &#xA;UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Tentei corrigi-lo sem sucesso com essas linhas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip install --ignore-installed --upgrade jupyter&#xA;&#xA;pip install ipykernel&#xA;python -m ipykernel install&#xA;&#xA;conda install notebook ipykernel&#xA;ipython kernelspec install-self&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Qualquer idia ser apreciada! Obrigado!&lt;/p&gt;&#xA;"" OwnerUserId=""102709"" LastEditorUserId=""102709"" LastEditDate=""2018-02-14T23:59:53.290"" LastActivityDate=""2018-02-14T23:59:53.290"" Title=""Como corrigir ipykernel_launcher.py: error: unrecognized arguments no jupyter?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""412603"" PostTypeId=""1"" CreationDate=""2019-09-25T22:51:38.543"" Score=""1"" ViewCount=""114"" Body=""&lt;p&gt;&lt;strong&gt;Tenho o seguinte cdigo salvo como main.py&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import nltk&#xA;from nltk.stem.lancaster import LancasterStemmer&#xA;&#xA;stemmer = LancasterStemmer()&#xA;&#xA;import numpy&#xA;import tflearn&#xA;import tensorflow&#xA;import random&#xA;import json&#xA;import pickle&#xA;&#xA;try:&#xA;    nltk.download('punkt')&#xA;except:&#xA;    pass&#xA;&#xA;with open(&quot;intents.json&quot;) as file:&#xA;    data = json.load(file)&#xA;try:&#xA;    with open(&quot;data.pickle&quot;, &quot;rb&quot;) as f:&#xA;        words, labels, training, output = pickle.load(f)&#xA;except:&#xA;    words = []&#xA;    labels = []&#xA;    docs_x = []&#xA;    docs_y = []&#xA;&#xA;    for intent in data[&quot;intents&quot;]:&#xA;        for pattern in intent[&quot;patterns&quot;]:&#xA;            wrds = nltk.word_tokenize(pattern)&#xA;            words.extend(wrds)&#xA;            docs_x.append(wrds)&#xA;            docs_y.append(intent[&quot;tag&quot;])&#xA;&#xA;        if intent[&quot;tag&quot;] not in labels:&#xA;            labels.append(intent[&quot;tag&quot;])&#xA;&#xA;    words = [stemmer.stem(w.lower()) for w in words if w != &quot;?&quot;]&#xA;    words = sorted(list(set(words)))&#xA;&#xA;    labels = sorted(labels)&#xA;&#xA;    training = []&#xA;    output = []&#xA;&#xA;    out_empty = [0 for _ in range(len(labels))]&#xA;&#xA;    for x, doc in enumerate(docs_x):&#xA;        bag = []&#xA;        wrds = [stemmer.stem(w.lower()) for w in doc]&#xA;&#xA;        for w in words:&#xA;            if w in wrds:&#xA;                bag.append(1)&#xA;            else:&#xA;                bag.append(0)&#xA;&#xA;        output_row = out_empty[:]&#xA;        output_row[labels.index(docs_y[x])] = 1&#xA;&#xA;        training.append(bag)&#xA;        output.append(output_row)&#xA;&#xA;    training = numpy.array(training)&#xA;    output = numpy.array(output)&#xA;&#xA;    with open(&quot;data.pickle&quot;, &quot;wb&quot;) as f:&#xA;        pickle.dump((words, labels, training, output), f)&#xA;&#xA;tensorflow.reset_default_graph()&#xA;&#xA;net = tflearn.input_data(shape=[None, len(training[0])])&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, len(output[0]), activation=&quot;softmax&quot;)&#xA;net = tflearn.regression(net)&#xA;&#xA;model = tflearn.DNN(net)&#xA;try:&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;except:&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;    model.save(&quot;model.tflearn&quot;)&#xA;&#xA;&#xA;def bag_of_words(s, words):&#xA;    bag = [0 for _ in range(len(words))]&#xA;&#xA;    s_words = nltk.word_tokenize(s)&#xA;    s_words = [stemmer.stem(word.lower()) for word in s_words]&#xA;&#xA;    for se in s_words:&#xA;        for i, w in enumerate(words):&#xA;            if w == se:&#xA;                bag[i] = 1&#xA;    return numpy.array(bag)&#xA;&#xA;&#xA;def chat():&#xA;    print(&quot;Comece a falar com o Bot! (Escreva sair para parar o Bot)&quot;)&#xA;    while True:&#xA;        inp = input(&quot;Voc:&quot;)&#xA;        if inp.lower() == &quot;sair&quot;:&#xA;            break&#xA;        results = model.predict([bag_of_words(inp, words)])[0]&#xA;        results_index = numpy.argmax(results)&#xA;        tag = labels[results_index]&#xA;&#xA;        if results[results_index] &amp;gt; 0.7:&#xA;            for tg in data[&quot;intents&quot;]:&#xA;                if tg['tag'] == &quot;tag&quot;:&#xA;                    responses = tg['responses']&#xA;&#xA;            print(random.choice(responses))&#xA;        else:&#xA;            print(&quot;No entendi, tente repetir a pergunta ou me diga outra coisa.&quot;)&#xA;&#xA;&#xA;chat()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Quando executo ele recebo o seguinte erro&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Viana\AppData\Local\Programs\Python\Python37\python.exe C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;curses is not supported on this machine (please install/reinstall curses for an optimal experience)&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.&#xA;&#xA;Scipy not supported!&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.&#xA;&#xA;[nltk_data] Downloading package punkt to&#xA;[nltk_data]     C:\Users\Viana\AppData\Roaming\nltk_data...&#xA;[nltk_data]   Package punkt is already up-to-date!&#xA;WARNING:tensorflow:From C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py:71: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Call initializer instance with the dtype argument instead of passing it to the constructor&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;keep_dims is deprecated, use keepdims instead&#xA;2019-09-25 19:25:50.808762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use tf.where in 2.0, which has the same broadcast rule as np.where&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use standard file APIs to check for files with this prefix.&#xA;---------------------------------&#xA;Run id: QAZDGH&#xA;Log directory: /tmp/tflearn_logs/&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 81, in &amp;lt;module&amp;gt;&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 308, in load&#xA;    self.trainer.restore(model_file, weights_only, **optargs)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 490, in restore&#xA;    self.restorer.restore(self.session, model_file)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py&quot;, line 1278, in restore&#xA;    compat.as_text(save_path))&#xA;ValueError: The passed save_path is not a valid checkpoint: C:\Users\Viana\Desktop\Projetos\Curso\Python\Jessie\Codigo\model.tflearn&#xA;&#xA;During handling of the above exception, another exception occurred:&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 216, in fit&#xA;    callbacks=callbacks)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 339, in fit&#xA;    show_metric)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 816, in _train&#xA;    tflearn.is_training(True, session=self.session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py&quot;, line 95, in is_training&#xA;    tf.get_collection('is_training_ops')[0].eval(session=session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 731, in eval&#xA;    return _eval_using_default_session(self, feed_dict, self.graph, session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 5579, in _eval_using_default_session&#xA;    return session.run(tensors, feed_dict)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 950, in run&#xA;    run_metadata_ptr)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1096, in _run&#xA;---------------------------------&#xA;Training samples: 12&#xA;Validation samples: 0&#xA;--&#xA;    raise RuntimeError('Attempted to use a closed Session.')&#xA;RuntimeError: Attempted to use a closed Session.&#xA;&#xA;Process finished with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;J reinstalei todos os frameworks que utilizo no projeto e mesmo assim o erro continua. No sei o que est acontecendo e no possuo o conhecimento necessrio para resolver isso, gostaria que me ajudassem a resolver. Obrigado&lt;/p&gt;&#xA;"" OwnerUserId=""159239"" LastEditorUserId=""159239"" LastEditDate=""2019-09-25T23:13:33.927"" LastActivityDate=""2019-09-25T23:13:33.927"" Title=""Estou com um erro no Python"" Tags=""&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;nltk&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""412603"" PostTypeId=""1"" CreationDate=""2019-09-25T22:51:38.543"" Score=""1"" ViewCount=""114"" Body=""&lt;p&gt;&lt;strong&gt;Tenho o seguinte cdigo salvo como main.py&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import nltk&#xA;from nltk.stem.lancaster import LancasterStemmer&#xA;&#xA;stemmer = LancasterStemmer()&#xA;&#xA;import numpy&#xA;import tflearn&#xA;import tensorflow&#xA;import random&#xA;import json&#xA;import pickle&#xA;&#xA;try:&#xA;    nltk.download('punkt')&#xA;except:&#xA;    pass&#xA;&#xA;with open(&quot;intents.json&quot;) as file:&#xA;    data = json.load(file)&#xA;try:&#xA;    with open(&quot;data.pickle&quot;, &quot;rb&quot;) as f:&#xA;        words, labels, training, output = pickle.load(f)&#xA;except:&#xA;    words = []&#xA;    labels = []&#xA;    docs_x = []&#xA;    docs_y = []&#xA;&#xA;    for intent in data[&quot;intents&quot;]:&#xA;        for pattern in intent[&quot;patterns&quot;]:&#xA;            wrds = nltk.word_tokenize(pattern)&#xA;            words.extend(wrds)&#xA;            docs_x.append(wrds)&#xA;            docs_y.append(intent[&quot;tag&quot;])&#xA;&#xA;        if intent[&quot;tag&quot;] not in labels:&#xA;            labels.append(intent[&quot;tag&quot;])&#xA;&#xA;    words = [stemmer.stem(w.lower()) for w in words if w != &quot;?&quot;]&#xA;    words = sorted(list(set(words)))&#xA;&#xA;    labels = sorted(labels)&#xA;&#xA;    training = []&#xA;    output = []&#xA;&#xA;    out_empty = [0 for _ in range(len(labels))]&#xA;&#xA;    for x, doc in enumerate(docs_x):&#xA;        bag = []&#xA;        wrds = [stemmer.stem(w.lower()) for w in doc]&#xA;&#xA;        for w in words:&#xA;            if w in wrds:&#xA;                bag.append(1)&#xA;            else:&#xA;                bag.append(0)&#xA;&#xA;        output_row = out_empty[:]&#xA;        output_row[labels.index(docs_y[x])] = 1&#xA;&#xA;        training.append(bag)&#xA;        output.append(output_row)&#xA;&#xA;    training = numpy.array(training)&#xA;    output = numpy.array(output)&#xA;&#xA;    with open(&quot;data.pickle&quot;, &quot;wb&quot;) as f:&#xA;        pickle.dump((words, labels, training, output), f)&#xA;&#xA;tensorflow.reset_default_graph()&#xA;&#xA;net = tflearn.input_data(shape=[None, len(training[0])])&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, len(output[0]), activation=&quot;softmax&quot;)&#xA;net = tflearn.regression(net)&#xA;&#xA;model = tflearn.DNN(net)&#xA;try:&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;except:&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;    model.save(&quot;model.tflearn&quot;)&#xA;&#xA;&#xA;def bag_of_words(s, words):&#xA;    bag = [0 for _ in range(len(words))]&#xA;&#xA;    s_words = nltk.word_tokenize(s)&#xA;    s_words = [stemmer.stem(word.lower()) for word in s_words]&#xA;&#xA;    for se in s_words:&#xA;        for i, w in enumerate(words):&#xA;            if w == se:&#xA;                bag[i] = 1&#xA;    return numpy.array(bag)&#xA;&#xA;&#xA;def chat():&#xA;    print(&quot;Comece a falar com o Bot! (Escreva sair para parar o Bot)&quot;)&#xA;    while True:&#xA;        inp = input(&quot;Voc:&quot;)&#xA;        if inp.lower() == &quot;sair&quot;:&#xA;            break&#xA;        results = model.predict([bag_of_words(inp, words)])[0]&#xA;        results_index = numpy.argmax(results)&#xA;        tag = labels[results_index]&#xA;&#xA;        if results[results_index] &amp;gt; 0.7:&#xA;            for tg in data[&quot;intents&quot;]:&#xA;                if tg['tag'] == &quot;tag&quot;:&#xA;                    responses = tg['responses']&#xA;&#xA;            print(random.choice(responses))&#xA;        else:&#xA;            print(&quot;No entendi, tente repetir a pergunta ou me diga outra coisa.&quot;)&#xA;&#xA;&#xA;chat()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Quando executo ele recebo o seguinte erro&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Viana\AppData\Local\Programs\Python\Python37\python.exe C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;curses is not supported on this machine (please install/reinstall curses for an optimal experience)&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.&#xA;&#xA;Scipy not supported!&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.&#xA;&#xA;[nltk_data] Downloading package punkt to&#xA;[nltk_data]     C:\Users\Viana\AppData\Roaming\nltk_data...&#xA;[nltk_data]   Package punkt is already up-to-date!&#xA;WARNING:tensorflow:From C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py:71: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Call initializer instance with the dtype argument instead of passing it to the constructor&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;keep_dims is deprecated, use keepdims instead&#xA;2019-09-25 19:25:50.808762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use tf.where in 2.0, which has the same broadcast rule as np.where&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use standard file APIs to check for files with this prefix.&#xA;---------------------------------&#xA;Run id: QAZDGH&#xA;Log directory: /tmp/tflearn_logs/&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 81, in &amp;lt;module&amp;gt;&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 308, in load&#xA;    self.trainer.restore(model_file, weights_only, **optargs)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 490, in restore&#xA;    self.restorer.restore(self.session, model_file)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py&quot;, line 1278, in restore&#xA;    compat.as_text(save_path))&#xA;ValueError: The passed save_path is not a valid checkpoint: C:\Users\Viana\Desktop\Projetos\Curso\Python\Jessie\Codigo\model.tflearn&#xA;&#xA;During handling of the above exception, another exception occurred:&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 216, in fit&#xA;    callbacks=callbacks)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 339, in fit&#xA;    show_metric)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 816, in _train&#xA;    tflearn.is_training(True, session=self.session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py&quot;, line 95, in is_training&#xA;    tf.get_collection('is_training_ops')[0].eval(session=session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 731, in eval&#xA;    return _eval_using_default_session(self, feed_dict, self.graph, session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 5579, in _eval_using_default_session&#xA;    return session.run(tensors, feed_dict)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 950, in run&#xA;    run_metadata_ptr)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1096, in _run&#xA;---------------------------------&#xA;Training samples: 12&#xA;Validation samples: 0&#xA;--&#xA;    raise RuntimeError('Attempted to use a closed Session.')&#xA;RuntimeError: Attempted to use a closed Session.&#xA;&#xA;Process finished with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;J reinstalei todos os frameworks que utilizo no projeto e mesmo assim o erro continua. No sei o que est acontecendo e no possuo o conhecimento necessrio para resolver isso, gostaria que me ajudassem a resolver. Obrigado&lt;/p&gt;&#xA;"" OwnerUserId=""159239"" LastEditorUserId=""159239"" LastEditDate=""2019-09-25T23:13:33.927"" LastActivityDate=""2019-09-25T23:13:33.927"" Title=""Estou com um erro no Python"" Tags=""&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;nltk&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""412603"" PostTypeId=""1"" CreationDate=""2019-09-25T22:51:38.543"" Score=""1"" ViewCount=""114"" Body=""&lt;p&gt;&lt;strong&gt;Tenho o seguinte cdigo salvo como main.py&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import nltk&#xA;from nltk.stem.lancaster import LancasterStemmer&#xA;&#xA;stemmer = LancasterStemmer()&#xA;&#xA;import numpy&#xA;import tflearn&#xA;import tensorflow&#xA;import random&#xA;import json&#xA;import pickle&#xA;&#xA;try:&#xA;    nltk.download('punkt')&#xA;except:&#xA;    pass&#xA;&#xA;with open(&quot;intents.json&quot;) as file:&#xA;    data = json.load(file)&#xA;try:&#xA;    with open(&quot;data.pickle&quot;, &quot;rb&quot;) as f:&#xA;        words, labels, training, output = pickle.load(f)&#xA;except:&#xA;    words = []&#xA;    labels = []&#xA;    docs_x = []&#xA;    docs_y = []&#xA;&#xA;    for intent in data[&quot;intents&quot;]:&#xA;        for pattern in intent[&quot;patterns&quot;]:&#xA;            wrds = nltk.word_tokenize(pattern)&#xA;            words.extend(wrds)&#xA;            docs_x.append(wrds)&#xA;            docs_y.append(intent[&quot;tag&quot;])&#xA;&#xA;        if intent[&quot;tag&quot;] not in labels:&#xA;            labels.append(intent[&quot;tag&quot;])&#xA;&#xA;    words = [stemmer.stem(w.lower()) for w in words if w != &quot;?&quot;]&#xA;    words = sorted(list(set(words)))&#xA;&#xA;    labels = sorted(labels)&#xA;&#xA;    training = []&#xA;    output = []&#xA;&#xA;    out_empty = [0 for _ in range(len(labels))]&#xA;&#xA;    for x, doc in enumerate(docs_x):&#xA;        bag = []&#xA;        wrds = [stemmer.stem(w.lower()) for w in doc]&#xA;&#xA;        for w in words:&#xA;            if w in wrds:&#xA;                bag.append(1)&#xA;            else:&#xA;                bag.append(0)&#xA;&#xA;        output_row = out_empty[:]&#xA;        output_row[labels.index(docs_y[x])] = 1&#xA;&#xA;        training.append(bag)&#xA;        output.append(output_row)&#xA;&#xA;    training = numpy.array(training)&#xA;    output = numpy.array(output)&#xA;&#xA;    with open(&quot;data.pickle&quot;, &quot;wb&quot;) as f:&#xA;        pickle.dump((words, labels, training, output), f)&#xA;&#xA;tensorflow.reset_default_graph()&#xA;&#xA;net = tflearn.input_data(shape=[None, len(training[0])])&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, len(output[0]), activation=&quot;softmax&quot;)&#xA;net = tflearn.regression(net)&#xA;&#xA;model = tflearn.DNN(net)&#xA;try:&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;except:&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;    model.save(&quot;model.tflearn&quot;)&#xA;&#xA;&#xA;def bag_of_words(s, words):&#xA;    bag = [0 for _ in range(len(words))]&#xA;&#xA;    s_words = nltk.word_tokenize(s)&#xA;    s_words = [stemmer.stem(word.lower()) for word in s_words]&#xA;&#xA;    for se in s_words:&#xA;        for i, w in enumerate(words):&#xA;            if w == se:&#xA;                bag[i] = 1&#xA;    return numpy.array(bag)&#xA;&#xA;&#xA;def chat():&#xA;    print(&quot;Comece a falar com o Bot! (Escreva sair para parar o Bot)&quot;)&#xA;    while True:&#xA;        inp = input(&quot;Voc:&quot;)&#xA;        if inp.lower() == &quot;sair&quot;:&#xA;            break&#xA;        results = model.predict([bag_of_words(inp, words)])[0]&#xA;        results_index = numpy.argmax(results)&#xA;        tag = labels[results_index]&#xA;&#xA;        if results[results_index] &amp;gt; 0.7:&#xA;            for tg in data[&quot;intents&quot;]:&#xA;                if tg['tag'] == &quot;tag&quot;:&#xA;                    responses = tg['responses']&#xA;&#xA;            print(random.choice(responses))&#xA;        else:&#xA;            print(&quot;No entendi, tente repetir a pergunta ou me diga outra coisa.&quot;)&#xA;&#xA;&#xA;chat()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Quando executo ele recebo o seguinte erro&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Viana\AppData\Local\Programs\Python\Python37\python.exe C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;curses is not supported on this machine (please install/reinstall curses for an optimal experience)&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.&#xA;&#xA;Scipy not supported!&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.&#xA;&#xA;[nltk_data] Downloading package punkt to&#xA;[nltk_data]     C:\Users\Viana\AppData\Roaming\nltk_data...&#xA;[nltk_data]   Package punkt is already up-to-date!&#xA;WARNING:tensorflow:From C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py:71: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Call initializer instance with the dtype argument instead of passing it to the constructor&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;keep_dims is deprecated, use keepdims instead&#xA;2019-09-25 19:25:50.808762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use tf.where in 2.0, which has the same broadcast rule as np.where&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use standard file APIs to check for files with this prefix.&#xA;---------------------------------&#xA;Run id: QAZDGH&#xA;Log directory: /tmp/tflearn_logs/&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 81, in &amp;lt;module&amp;gt;&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 308, in load&#xA;    self.trainer.restore(model_file, weights_only, **optargs)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 490, in restore&#xA;    self.restorer.restore(self.session, model_file)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py&quot;, line 1278, in restore&#xA;    compat.as_text(save_path))&#xA;ValueError: The passed save_path is not a valid checkpoint: C:\Users\Viana\Desktop\Projetos\Curso\Python\Jessie\Codigo\model.tflearn&#xA;&#xA;During handling of the above exception, another exception occurred:&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 216, in fit&#xA;    callbacks=callbacks)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 339, in fit&#xA;    show_metric)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 816, in _train&#xA;    tflearn.is_training(True, session=self.session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py&quot;, line 95, in is_training&#xA;    tf.get_collection('is_training_ops')[0].eval(session=session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 731, in eval&#xA;    return _eval_using_default_session(self, feed_dict, self.graph, session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 5579, in _eval_using_default_session&#xA;    return session.run(tensors, feed_dict)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 950, in run&#xA;    run_metadata_ptr)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1096, in _run&#xA;---------------------------------&#xA;Training samples: 12&#xA;Validation samples: 0&#xA;--&#xA;    raise RuntimeError('Attempted to use a closed Session.')&#xA;RuntimeError: Attempted to use a closed Session.&#xA;&#xA;Process finished with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;J reinstalei todos os frameworks que utilizo no projeto e mesmo assim o erro continua. No sei o que est acontecendo e no possuo o conhecimento necessrio para resolver isso, gostaria que me ajudassem a resolver. Obrigado&lt;/p&gt;&#xA;"" OwnerUserId=""159239"" LastEditorUserId=""159239"" LastEditDate=""2019-09-25T23:13:33.927"" LastActivityDate=""2019-09-25T23:13:33.927"" Title=""Estou com um erro no Python"" Tags=""&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;nltk&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/pt.stackoverflow.com,"  <row Id=""412603"" PostTypeId=""1"" CreationDate=""2019-09-25T22:51:38.543"" Score=""1"" ViewCount=""114"" Body=""&lt;p&gt;&lt;strong&gt;Tenho o seguinte cdigo salvo como main.py&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import nltk&#xA;from nltk.stem.lancaster import LancasterStemmer&#xA;&#xA;stemmer = LancasterStemmer()&#xA;&#xA;import numpy&#xA;import tflearn&#xA;import tensorflow&#xA;import random&#xA;import json&#xA;import pickle&#xA;&#xA;try:&#xA;    nltk.download('punkt')&#xA;except:&#xA;    pass&#xA;&#xA;with open(&quot;intents.json&quot;) as file:&#xA;    data = json.load(file)&#xA;try:&#xA;    with open(&quot;data.pickle&quot;, &quot;rb&quot;) as f:&#xA;        words, labels, training, output = pickle.load(f)&#xA;except:&#xA;    words = []&#xA;    labels = []&#xA;    docs_x = []&#xA;    docs_y = []&#xA;&#xA;    for intent in data[&quot;intents&quot;]:&#xA;        for pattern in intent[&quot;patterns&quot;]:&#xA;            wrds = nltk.word_tokenize(pattern)&#xA;            words.extend(wrds)&#xA;            docs_x.append(wrds)&#xA;            docs_y.append(intent[&quot;tag&quot;])&#xA;&#xA;        if intent[&quot;tag&quot;] not in labels:&#xA;            labels.append(intent[&quot;tag&quot;])&#xA;&#xA;    words = [stemmer.stem(w.lower()) for w in words if w != &quot;?&quot;]&#xA;    words = sorted(list(set(words)))&#xA;&#xA;    labels = sorted(labels)&#xA;&#xA;    training = []&#xA;    output = []&#xA;&#xA;    out_empty = [0 for _ in range(len(labels))]&#xA;&#xA;    for x, doc in enumerate(docs_x):&#xA;        bag = []&#xA;        wrds = [stemmer.stem(w.lower()) for w in doc]&#xA;&#xA;        for w in words:&#xA;            if w in wrds:&#xA;                bag.append(1)&#xA;            else:&#xA;                bag.append(0)&#xA;&#xA;        output_row = out_empty[:]&#xA;        output_row[labels.index(docs_y[x])] = 1&#xA;&#xA;        training.append(bag)&#xA;        output.append(output_row)&#xA;&#xA;    training = numpy.array(training)&#xA;    output = numpy.array(output)&#xA;&#xA;    with open(&quot;data.pickle&quot;, &quot;wb&quot;) as f:&#xA;        pickle.dump((words, labels, training, output), f)&#xA;&#xA;tensorflow.reset_default_graph()&#xA;&#xA;net = tflearn.input_data(shape=[None, len(training[0])])&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, 8)&#xA;net = tflearn.fully_connected(net, len(output[0]), activation=&quot;softmax&quot;)&#xA;net = tflearn.regression(net)&#xA;&#xA;model = tflearn.DNN(net)&#xA;try:&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;except:&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;    model.save(&quot;model.tflearn&quot;)&#xA;&#xA;&#xA;def bag_of_words(s, words):&#xA;    bag = [0 for _ in range(len(words))]&#xA;&#xA;    s_words = nltk.word_tokenize(s)&#xA;    s_words = [stemmer.stem(word.lower()) for word in s_words]&#xA;&#xA;    for se in s_words:&#xA;        for i, w in enumerate(words):&#xA;            if w == se:&#xA;                bag[i] = 1&#xA;    return numpy.array(bag)&#xA;&#xA;&#xA;def chat():&#xA;    print(&quot;Comece a falar com o Bot! (Escreva sair para parar o Bot)&quot;)&#xA;    while True:&#xA;        inp = input(&quot;Voc:&quot;)&#xA;        if inp.lower() == &quot;sair&quot;:&#xA;            break&#xA;        results = model.predict([bag_of_words(inp, words)])[0]&#xA;        results_index = numpy.argmax(results)&#xA;        tag = labels[results_index]&#xA;&#xA;        if results[results_index] &amp;gt; 0.7:&#xA;            for tg in data[&quot;intents&quot;]:&#xA;                if tg['tag'] == &quot;tag&quot;:&#xA;                    responses = tg['responses']&#xA;&#xA;            print(random.choice(responses))&#xA;        else:&#xA;            print(&quot;No entendi, tente repetir a pergunta ou me diga outra coisa.&quot;)&#xA;&#xA;&#xA;chat()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Quando executo ele recebo o seguinte erro&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Viana\AppData\Local\Programs\Python\Python37\python.exe C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])&#xA;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.&#xA;  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])&#xA;curses is not supported on this machine (please install/reinstall curses for an optimal experience)&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.&#xA;&#xA;Scipy not supported!&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.&#xA;&#xA;[nltk_data] Downloading package punkt to&#xA;[nltk_data]     C:\Users\Viana\AppData\Roaming\nltk_data...&#xA;[nltk_data]   Package punkt is already up-to-date!&#xA;WARNING:tensorflow:From C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py:71: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Call initializer instance with the dtype argument instead of passing it to the constructor&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;keep_dims is deprecated, use keepdims instead&#xA;2019-09-25 19:25:50.808762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\ops\math_grad.py:1250: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use tf.where in 2.0, which has the same broadcast rule as np.where&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.&#xA;&#xA;WARNING:tensorflow:From C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use standard file APIs to check for files with this prefix.&#xA;---------------------------------&#xA;Run id: QAZDGH&#xA;Log directory: /tmp/tflearn_logs/&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 81, in &amp;lt;module&amp;gt;&#xA;    model.load(&quot;model.tflearn&quot;)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 308, in load&#xA;    self.trainer.restore(model_file, weights_only, **optargs)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 490, in restore&#xA;    self.restorer.restore(self.session, model_file)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\training\saver.py&quot;, line 1278, in restore&#xA;    compat.as_text(save_path))&#xA;ValueError: The passed save_path is not a valid checkpoint: C:\Users\Viana\Desktop\Projetos\Curso\Python\Jessie\Codigo\model.tflearn&#xA;&#xA;During handling of the above exception, another exception occurred:&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Users/Viana/Desktop/Projetos/Curso/Python/Jessie/Codigo/main.py&quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\models\dnn.py&quot;, line 216, in fit&#xA;    callbacks=callbacks)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 339, in fit&#xA;    show_metric)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\helpers\trainer.py&quot;, line 816, in _train&#xA;    tflearn.is_training(True, session=self.session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tflearn\config.py&quot;, line 95, in is_training&#xA;    tf.get_collection('is_training_ops')[0].eval(session=session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 731, in eval&#xA;    return _eval_using_default_session(self, feed_dict, self.graph, session)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 5579, in _eval_using_default_session&#xA;    return session.run(tensors, feed_dict)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 950, in run&#xA;    run_metadata_ptr)&#xA;  File &quot;C:\Users\Viana\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1096, in _run&#xA;---------------------------------&#xA;Training samples: 12&#xA;Validation samples: 0&#xA;--&#xA;    raise RuntimeError('Attempted to use a closed Session.')&#xA;RuntimeError: Attempted to use a closed Session.&#xA;&#xA;Process finished with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;J reinstalei todos os frameworks que utilizo no projeto e mesmo assim o erro continua. No sei o que est acontecendo e no possuo o conhecimento necessrio para resolver isso, gostaria que me ajudassem a resolver. Obrigado&lt;/p&gt;&#xA;"" OwnerUserId=""159239"" LastEditorUserId=""159239"" LastEditDate=""2019-09-25T23:13:33.927"" LastActivityDate=""2019-09-25T23:13:33.927"" Title=""Estou com um erro no Python"" Tags=""&lt;python-3.x&gt;&lt;tensorflow&gt;&lt;nltk&gt;"" AnswerCount=""0"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""715266"" PostTypeId=""1"" AcceptedAnswerId=""715267"" CreationDate=""2017-09-06T19:36:08.030"" Score=""0"" ViewCount=""313"" Body=""&lt;p&gt;   &lt;strong&gt;Tensor Flow&lt;/strong&gt;  &lt;strong&gt;python 2.7&lt;/strong&gt;      .    ,   .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659427: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def baseline_model():&#xA;        model = Sequential()&#xA;        model.add(Dense(8, input_dim=4, activation='relu'))&#xA;        model.add(Dense(3, activation='softmax'))&#xA;        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])&#xA;        return model&#xA;&#xA;&#xA;estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)&#xA;kfold = KFold(n_splits=10, shuffle=True, random_state=seed)&#xA;results = cross_val_score(estimator, X, dummy_y, cv=kfold)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""188116"" LastActivityDate=""2017-09-06T19:36:08.030"" Title=""Warnings   TensorFlow"" Tags=""&lt;python&gt;&lt;python-2.x&gt;&lt;-&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""715266"" PostTypeId=""1"" AcceptedAnswerId=""715267"" CreationDate=""2017-09-06T19:36:08.030"" Score=""0"" ViewCount=""313"" Body=""&lt;p&gt;   &lt;strong&gt;Tensor Flow&lt;/strong&gt;  &lt;strong&gt;python 2.7&lt;/strong&gt;      .    ,   .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659427: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def baseline_model():&#xA;        model = Sequential()&#xA;        model.add(Dense(8, input_dim=4, activation='relu'))&#xA;        model.add(Dense(3, activation='softmax'))&#xA;        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])&#xA;        return model&#xA;&#xA;&#xA;estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)&#xA;kfold = KFold(n_splits=10, shuffle=True, random_state=seed)&#xA;results = cross_val_score(estimator, X, dummy_y, cv=kfold)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""188116"" LastActivityDate=""2017-09-06T19:36:08.030"" Title=""Warnings   TensorFlow"" Tags=""&lt;python&gt;&lt;python-2.x&gt;&lt;-&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""715266"" PostTypeId=""1"" AcceptedAnswerId=""715267"" CreationDate=""2017-09-06T19:36:08.030"" Score=""0"" ViewCount=""313"" Body=""&lt;p&gt;   &lt;strong&gt;Tensor Flow&lt;/strong&gt;  &lt;strong&gt;python 2.7&lt;/strong&gt;      .    ,   .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659427: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def baseline_model():&#xA;        model = Sequential()&#xA;        model.add(Dense(8, input_dim=4, activation='relu'))&#xA;        model.add(Dense(3, activation='softmax'))&#xA;        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])&#xA;        return model&#xA;&#xA;&#xA;estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)&#xA;kfold = KFold(n_splits=10, shuffle=True, random_state=seed)&#xA;results = cross_val_score(estimator, X, dummy_y, cv=kfold)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""188116"" LastActivityDate=""2017-09-06T19:36:08.030"" Title=""Warnings   TensorFlow"" Tags=""&lt;python&gt;&lt;python-2.x&gt;&lt;-&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""715266"" PostTypeId=""1"" AcceptedAnswerId=""715267"" CreationDate=""2017-09-06T19:36:08.030"" Score=""0"" ViewCount=""313"" Body=""&lt;p&gt;   &lt;strong&gt;Tensor Flow&lt;/strong&gt;  &lt;strong&gt;python 2.7&lt;/strong&gt;      .    ,   .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;2017-09-06 22:28:21.659427: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def baseline_model():&#xA;        model = Sequential()&#xA;        model.add(Dense(8, input_dim=4, activation='relu'))&#xA;        model.add(Dense(3, activation='softmax'))&#xA;        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])&#xA;        return model&#xA;&#xA;&#xA;estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)&#xA;kfold = KFold(n_splits=10, shuffle=True, random_state=seed)&#xA;results = cross_val_score(estimator, X, dummy_y, cv=kfold)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""188116"" LastActivityDate=""2017-09-06T19:36:08.030"" Title=""Warnings   TensorFlow"" Tags=""&lt;python&gt;&lt;python-2.x&gt;&lt;-&gt;&lt;tensorflow&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 3.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""865268"" PostTypeId=""1"" CreationDate=""2018-08-06T23:21:49.520"" Score=""0"" ViewCount=""64"" Body=""&lt;p&gt;  &lt;a href=&quot;https://github.com/RasaHQ/rasa_nlu/blob/master/rasa_nlu/classifiers/sklearn_intent_classifier.py#L110&quot; rel=&quot;nofollow noreferrer&quot;&gt;rasa_nlu&lt;/a&gt;   &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GridSearchCV.fit()&lt;/code&gt;&lt;/a&gt;   &lt;code&gt;clf.fit()&lt;/code&gt;.    ,       ,  ,   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Fitting 2 folds for each of 6 candidates, totalling 12 fits&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   GridSearchCV:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cv_splits = self._num_cv_splits(y) #   ,    2,  ,  -    &#xA;&#xA;GridSearchCV(SVC(C=1,&#xA;                probability=True,&#xA;                class_weight='balanced'),&#xA;            param_grid=tuned_parameters,&#xA;            n_jobs=num_threads,&#xA;            cv=cv_splits,&#xA;            scoring='f1_weighted',&#xA;            verbose=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; &lt;code&gt;y&lt;/code&gt; -  &lt;code&gt;labels&lt;/code&gt;,     &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;y: [1 0 2 1 1 1 1 1 1 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2&#xA; 3 3]&#xA;&#xA;labels: ['greet', 'goodbye', 'inform', 'greet', 'greet', 'greet', 'greet', 'greet', 'greet', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'laughing', 'laughing']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;     ,     .&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;       ,         :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        y = self.transform_labels_str2num(labels)&#xA;        X = np.stack([example.get(&quot;text_features&quot;)&#xA;                      for example in training_data.intent_examples])&#xA;&#xA;        self.clf = self._create_classifier(num_threads, y)&#xA;        try:&#xA;            fit_result = self.clf.fit(X, y)&#xA;            y_pred = self.clf.predict(X)&#xA;            print(&quot;set(y)-set(y_pred):\n&quot;,set(y)-set(y_pred))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;       &lt;code&gt;set()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;.predict(X)&lt;/code&gt;?      &lt;code&gt;clf.fit()&lt;/code&gt;?&lt;/p&gt;&#xA;"" OwnerUserId=""284043"" LastEditorUserId=""284043"" LastEditDate=""2018-08-07T15:35:36.110"" LastActivityDate=""2018-08-07T15:35:36.110"" Title="" ,    sklearn.GridSearchCV.fit()?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;-&gt;&lt;warning&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""865268"" PostTypeId=""1"" CreationDate=""2018-08-06T23:21:49.520"" Score=""0"" ViewCount=""64"" Body=""&lt;p&gt;  &lt;a href=&quot;https://github.com/RasaHQ/rasa_nlu/blob/master/rasa_nlu/classifiers/sklearn_intent_classifier.py#L110&quot; rel=&quot;nofollow noreferrer&quot;&gt;rasa_nlu&lt;/a&gt;   &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GridSearchCV.fit()&lt;/code&gt;&lt;/a&gt;   &lt;code&gt;clf.fit()&lt;/code&gt;.    ,       ,  ,   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Fitting 2 folds for each of 6 candidates, totalling 12 fits&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   GridSearchCV:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cv_splits = self._num_cv_splits(y) #   ,    2,  ,  -    &#xA;&#xA;GridSearchCV(SVC(C=1,&#xA;                probability=True,&#xA;                class_weight='balanced'),&#xA;            param_grid=tuned_parameters,&#xA;            n_jobs=num_threads,&#xA;            cv=cv_splits,&#xA;            scoring='f1_weighted',&#xA;            verbose=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; &lt;code&gt;y&lt;/code&gt; -  &lt;code&gt;labels&lt;/code&gt;,     &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;y: [1 0 2 1 1 1 1 1 1 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2&#xA; 3 3]&#xA;&#xA;labels: ['greet', 'goodbye', 'inform', 'greet', 'greet', 'greet', 'greet', 'greet', 'greet', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'laughing', 'laughing']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;     ,     .&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;       ,         :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        y = self.transform_labels_str2num(labels)&#xA;        X = np.stack([example.get(&quot;text_features&quot;)&#xA;                      for example in training_data.intent_examples])&#xA;&#xA;        self.clf = self._create_classifier(num_threads, y)&#xA;        try:&#xA;            fit_result = self.clf.fit(X, y)&#xA;            y_pred = self.clf.predict(X)&#xA;            print(&quot;set(y)-set(y_pred):\n&quot;,set(y)-set(y_pred))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;       &lt;code&gt;set()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;.predict(X)&lt;/code&gt;?      &lt;code&gt;clf.fit()&lt;/code&gt;?&lt;/p&gt;&#xA;"" OwnerUserId=""284043"" LastEditorUserId=""284043"" LastEditDate=""2018-08-07T15:35:36.110"" LastActivityDate=""2018-08-07T15:35:36.110"" Title="" ,    sklearn.GridSearchCV.fit()?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;-&gt;&lt;warning&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""865268"" PostTypeId=""1"" CreationDate=""2018-08-06T23:21:49.520"" Score=""0"" ViewCount=""64"" Body=""&lt;p&gt;  &lt;a href=&quot;https://github.com/RasaHQ/rasa_nlu/blob/master/rasa_nlu/classifiers/sklearn_intent_classifier.py#L110&quot; rel=&quot;nofollow noreferrer&quot;&gt;rasa_nlu&lt;/a&gt;   &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GridSearchCV.fit()&lt;/code&gt;&lt;/a&gt;   &lt;code&gt;clf.fit()&lt;/code&gt;.    ,       ,  ,   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Fitting 2 folds for each of 6 candidates, totalling 12 fits&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   GridSearchCV:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cv_splits = self._num_cv_splits(y) #   ,    2,  ,  -    &#xA;&#xA;GridSearchCV(SVC(C=1,&#xA;                probability=True,&#xA;                class_weight='balanced'),&#xA;            param_grid=tuned_parameters,&#xA;            n_jobs=num_threads,&#xA;            cv=cv_splits,&#xA;            scoring='f1_weighted',&#xA;            verbose=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; &lt;code&gt;y&lt;/code&gt; -  &lt;code&gt;labels&lt;/code&gt;,     &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;y: [1 0 2 1 1 1 1 1 1 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2&#xA; 3 3]&#xA;&#xA;labels: ['greet', 'goodbye', 'inform', 'greet', 'greet', 'greet', 'greet', 'greet', 'greet', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'laughing', 'laughing']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;     ,     .&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;       ,         :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        y = self.transform_labels_str2num(labels)&#xA;        X = np.stack([example.get(&quot;text_features&quot;)&#xA;                      for example in training_data.intent_examples])&#xA;&#xA;        self.clf = self._create_classifier(num_threads, y)&#xA;        try:&#xA;            fit_result = self.clf.fit(X, y)&#xA;            y_pred = self.clf.predict(X)&#xA;            print(&quot;set(y)-set(y_pred):\n&quot;,set(y)-set(y_pred))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;       &lt;code&gt;set()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;.predict(X)&lt;/code&gt;?      &lt;code&gt;clf.fit()&lt;/code&gt;?&lt;/p&gt;&#xA;"" OwnerUserId=""284043"" LastEditorUserId=""284043"" LastEditDate=""2018-08-07T15:35:36.110"" LastActivityDate=""2018-08-07T15:35:36.110"" Title="" ,    sklearn.GridSearchCV.fit()?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;-&gt;&lt;warning&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""865268"" PostTypeId=""1"" CreationDate=""2018-08-06T23:21:49.520"" Score=""0"" ViewCount=""64"" Body=""&lt;p&gt;  &lt;a href=&quot;https://github.com/RasaHQ/rasa_nlu/blob/master/rasa_nlu/classifiers/sklearn_intent_classifier.py#L110&quot; rel=&quot;nofollow noreferrer&quot;&gt;rasa_nlu&lt;/a&gt;   &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GridSearchCV.fit()&lt;/code&gt;&lt;/a&gt;   &lt;code&gt;clf.fit()&lt;/code&gt;.    ,       ,  ,   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Fitting 2 folds for each of 6 candidates, totalling 12 fits&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/home/mike/Programming/Rasa/myflaskapp/rasaenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   GridSearchCV:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cv_splits = self._num_cv_splits(y) #   ,    2,  ,  -    &#xA;&#xA;GridSearchCV(SVC(C=1,&#xA;                probability=True,&#xA;                class_weight='balanced'),&#xA;            param_grid=tuned_parameters,&#xA;            n_jobs=num_threads,&#xA;            cv=cv_splits,&#xA;            scoring='f1_weighted',&#xA;            verbose=1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; &lt;code&gt;y&lt;/code&gt; -  &lt;code&gt;labels&lt;/code&gt;,     &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;y: [1 0 2 1 1 1 1 1 1 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2&#xA; 3 3]&#xA;&#xA;labels: ['greet', 'goodbye', 'inform', 'greet', 'greet', 'greet', 'greet', 'greet', 'greet', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'inform', 'laughing', 'laughing']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;     ,     .&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;       ,         :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        y = self.transform_labels_str2num(labels)&#xA;        X = np.stack([example.get(&quot;text_features&quot;)&#xA;                      for example in training_data.intent_examples])&#xA;&#xA;        self.clf = self._create_classifier(num_threads, y)&#xA;        try:&#xA;            fit_result = self.clf.fit(X, y)&#xA;            y_pred = self.clf.predict(X)&#xA;            print(&quot;set(y)-set(y_pred):\n&quot;,set(y)-set(y_pred))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;       &lt;code&gt;set()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;.predict(X)&lt;/code&gt;?      &lt;code&gt;clf.fit()&lt;/code&gt;?&lt;/p&gt;&#xA;"" OwnerUserId=""284043"" LastEditorUserId=""284043"" LastEditDate=""2018-08-07T15:35:36.110"" LastActivityDate=""2018-08-07T15:35:36.110"" Title="" ,    sklearn.GridSearchCV.fit()?"" Tags=""&lt;python&gt;&lt;python-3.x&gt;&lt;-&gt;&lt;warning&gt;&lt;scikit-learn&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1082249"" PostTypeId=""1"" CreationDate=""2020-02-13T17:09:30.477"" Score=""1"" ViewCount=""80"" Body=""&lt;p&gt;      GridSearchCV.        f1.         precision  recall. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;  : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import accuracy_score&#xA;from sklearn.metrics import precision_score&#xA;from sklearn.metrics import recall_score&#xA;from sklearn.metrics import f1_score&#xA;from sklearn.metrics import make_scorer&#xA;&#xA;param_grid = {&#xA;    'num_leaves':[5,15,45],&#xA;    'learning_rate': [0.005, 0.01, 0.1],&#xA;    'n_estimators': [100,200,300]&#xA;}&#xA;&#xA;scoring = {&#xA;    'accuracy': make_scorer(accuracy_score),&#xA;    'precision': make_scorer(precision_score),&#xA;    'recall': make_scorer(recall_score),&#xA;    'f1': make_scorer(f1_score),    &#xA;}&#xA;&#xA;grid = GridSearchCV(clf, param_grid = param_grid, cv = 4, verbose = 5, scoring = scoring, refit = 'f1')&#xA;&#xA;grid.fit(X_train_t, y_train_t)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   &lt;code&gt;UndefinedMetricWarning&lt;/code&gt;     &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;recall&lt;/code&gt;  &lt;code&gt;f1&lt;/code&gt;  0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.&#xA;  'precision', 'predicted', average, warn_for)&#xA;[CV]  learning_rate=0.005, n_estimators=100, num_leaves=5, accuracy=0.5469064074675771, precision=0.0, recall=0.0, f1=0.0, total=   5.4s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; , sklearn -     (&lt;code&gt;UndefinedMetricWarning&lt;/code&gt;)     ,     0.    ,    . &#xA;  ,      &lt;code&gt;grid.best_estimator_&lt;/code&gt;     ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;"" OwnerUserId=""336188"" LastEditorUserId=""211923"" LastEditDate=""2020-02-14T06:56:28.163"" LastActivityDate=""2020-02-16T19:50:01.057"" Title=""   UndefinedMetricWarning   GridSearchCV?"" Tags=""&lt;python&gt;&lt;-&gt;&lt;scikit-learn&gt;&lt;&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1098445"" PostTypeId=""1"" CreationDate=""2020-03-23T09:10:52.057"" Score=""0"" ViewCount=""92"" Body=""&lt;p&gt;   :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pandas as pd &#xA;import numpy as np&#xA;import seaborn as sns&#xA;%matplotlib inline&#xA;import matplotlib.pyplot as plt&#xA;Porphs_data = pd.read_excel('I:\\Porphyrins\\26_4-Descs_varI_22-3-20.xlsx', index_col=0)&#xA;y = Porphs_data.LogFi&#xA;X = Porphs_data.drop(['LogFi'], axis=1)&#xA;# Select upper triangle of correlation matrix&#xA;upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))&#xA;# Find index of feature columns with correlation greater than 0.95&#xA;to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.95)]&#xA;# Drop features &#xA;X = X.drop(Porphs_data[to_drop], axis=1)&#xA;X_train = X.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;y_train = y.drop([&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;], axis=0)&#xA;X_test = X.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,15-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;y_test = y.loc[[&quot;(p-Br)4-TPP&quot;, &quot;5,10-NO2-etioporphyrin I&quot;, &quot;Deuteroporphyrin-IX-DME&quot;, &#xA;                  &quot;N-CH3-Octaethylporphyrin&quot;, &quot;Porphine&quot;]]&#xA;from sklearn import linear_model&#xA;lm = linear_model.LinearRegression()&#xA;lm.fit(X_train, y_train)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.metrics import r2_score&#xA;y_pred = lm.predict(X_train)&#xA;r2_score(y_train, y_pred)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;   q2  leave-one-out  :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import LeaveOneOut&#xA;from sklearn.linear_model import LinearRegression&#xA;loo_lm = LinearRegression(lm, LeaveOneOut())&#xA;loo_lm.fit(X_train, y_train)&#xA;loo_lm.score(X_train, y_train)                         &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;            .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;   q2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.model_selection import cross_val_score&#xA;cvs=cross_val_score(lm, X_train, y_train, cv=21)&#xA;cvs&#xA;mean_cross_val_score = cvs.mean()&#xA;mean_cross_val_score&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; : UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;,   q2  leave-one-out  !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS  X  y.  X:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Eig02_EA(dm)    MATS1e  HOMO-LUMO_Gap   SpMax4_Bh(p)    SAdon&#xA;compound                    &#xA;(m-Cl)4-TPP 0.00    -0.040  2.74    3.86    33.6&#xA;(o-Cl)4-TPP 0.00    -0.040  2.80    3.86    33.6&#xA;(o-F)4-TPP  0.00    -0.026  2.75    3.84    33.6&#xA;(p-Cl)4-TPP 0.00    -0.040  2.69    3.86    33.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt; y:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compound&#xA;(m-Cl)4-TPP                                  1.908485&#xA;(o-Cl)4-TPP                                  1.924279&#xA;(o-F)4-TPP                                   1.851258&#xA;(p-Cl)4-TPP                                  1.851258&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""373289"" LastEditorUserId=""373289"" LastEditDate=""2020-03-25T10:12:06.477"" LastActivityDate=""2020-03-27T12:29:30.800"" Title=""   q2  Leave-One-Out    ?"" Tags=""&lt;python&gt;&lt;numpy&gt;&lt;scikit-learn&gt;"" AnswerCount=""3"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1217415"" PostTypeId=""1"" CreationDate=""2020-12-11T15:05:45.050"" Score=""0"" ViewCount=""41"" Body=""&lt;p&gt; torch-    &lt;code&gt;(False, True)&lt;/code&gt;.   ,   &lt;code&gt;True&lt;/code&gt;,       &lt;code&gt;True&lt;/code&gt;.    ,       .&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def is_contain_only_one_true(tensor: torch.Tensor) -&amp;gt; bool:&#xA;    return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&amp;gt;&amp;gt;&amp;gt; a = torch.tensor([True, False, False, True])&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(a) # 2 True&#xA;False&#xA;&amp;gt;&amp;gt;&amp;gt; b = torch.tensor([True, False, False]) #   True&#xA;&amp;gt;&amp;gt;&amp;gt; is_contain_only_one_true(b)&#xA;True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     ,   :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ipython-input-4-a3e0f370a666&amp;gt;:2: UserWarning: This overload of nonzero is deprecated:&#xA;    nonzero()&#xA;Consider using one of the following signatures instead:&#xA;    nonzero(*, bool as_tuple) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:766.)&#xA;  return tensor.nonzero().numel() == 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  -       ?&lt;/p&gt;&#xA;"" OwnerUserId=""336531"" LastActivityDate=""2020-12-11T15:47:36.267"" Title=""    ?"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1232088"" PostTypeId=""1"" CreationDate=""2021-01-16T04:44:50.257"" Score=""0"" ViewCount=""131"" Body=""&lt;p&gt; &amp;quot;Hello world&amp;quot;   ,    Iris.          ,   80%    ,   20%- .   6  .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    # Load libraries    &#xA;    import pandas&#xA;    from pandas.plotting import scatter_matrix&#xA;    import matplotlib.pyplot as plt&#xA;    from sklearn import model_selection&#xA;    from sklearn.metrics import classification_report&#xA;    from sklearn.metrics import confusion_matrix&#xA;    from sklearn.metrics import accuracy_score&#xA;    from sklearn.linear_model import LogisticRegression&#xA;    from sklearn.tree import DecisionTreeClassifier&#xA;    from sklearn.neighbors import KNeighborsClassifier&#xA;    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis&#xA;    from sklearn.naive_bayes import GaussianNB&#xA;    from sklearn.svm import SVC&#xA;    &#xA;    # Load dataset&#xA;    url = &amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv&amp;quot;&#xA;    names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']&#xA;    dataset = pandas.read_csv(url, names=names)&#xA;    &#xA;    # Split-out validation dataset&#xA;    array = dataset.values&#xA;    X = array[:,0:4]&#xA;    Y = array[:,4]&#xA;    validation_size = 0.20&#xA;    seed = 7&#xA;    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)&#xA;    &#xA;    # Test options and evaluation metric&#xA;    seed = 7&#xA;    scoring = 'accuracy'&#xA;    &#xA;    # Spot Check Algorithms&#xA;    models = []&#xA;    models.append(('LR', LogisticRegression()))&#xA;    models.append(('LDA', LinearDiscriminantAnalysis()))&#xA;    models.append(('KNN', KNeighborsClassifier()))&#xA;    models.append(('CART', DecisionTreeClassifier()))&#xA;    models.append(('NB', GaussianNB()))&#xA;    models.append(('SVM', SVC()))&#xA;    # evaluate each model in turn&#xA;    results = []&#xA;    names = []&#xA;    for name, model in models:&#xA;        kfold = model_selection.KFold(n_splits=10, random_state=seed)&#xA;        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)&#xA;        results.append(cv_results)&#xA;        names.append(name)&#xA;        msg = &amp;quot;%s: %f (%f)&amp;quot; % (name, cv_results.mean(), cv_results.std())&#xA;        print(msg)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):&#xA;STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.&#xA;&#xA;Increase the number of iterations (max_iter) or scale the data as shown in:&#xA;    https://scikit-learn.org/stable/modules/preprocessing.html&#xA;Please also refer to the documentation for alternative solver options:&#xA;    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression&#xA;  n_iter_i = _check_optimize_result(&#xA;LR: 0.983333 (0.033333)&#xA;LDA: 0.975000 (0.038188)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;KNN: 0.983333 (0.033333)&#xA;CART: 0.975000 (0.038188)&#xA;NB: 0.975000 (0.053359)&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;C:\Users\Dexp\anaconda3\lib\site-packages\sklearn\model_selection\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.&#xA;  warnings.warn(&#xA;    SVM: 0.991667 (0.025000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  - ??&lt;/p&gt;&#xA;"" OwnerUserId=""425510"" LastActivityDate=""2021-01-16T04:44:50.257"" Title=""FutureWarning: Setting a random_state has no effect since shuffle is False"" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;scikit-learn&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1277670"" PostTypeId=""1"" CreationDate=""2021-05-03T13:19:30.727"" Score=""0"" ViewCount=""891"" Body=""&lt;p&gt;   PyTorch.  GAN,       .                  (ImageFolder).    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class ImageDataset(Dataset):&#xA;    def __init__(self, root, transforms_=None, image_size=image_size, mask_size=mask_size, mode=&amp;quot;train&amp;quot;):&#xA;        self.transform = transforms.Compose(transforms_)&#xA;        self.image_size = image_size&#xA;        self.mask_size = mask_size&#xA;        self.mode = mode&#xA;        self.files = sorted(glob.glob(&amp;quot;%s/**/*.jpg&amp;quot; % root, recursive=True))&#xA;        self.files = self.files[-4000:] if mode == &amp;quot;train&amp;quot; else self.files[:-4000]&#xA;...    &#xA;    def __len__(self):&#xA;        return len(self.files)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    transforms_ = [&#xA;        transforms.Grayscale(1),&#xA;        transforms.Resize((image_size, image_size), Image.BICUBIC),&#xA;        transforms.ToTensor(),&#xA;        transforms.Normalize((0.5, ), (0.5, )),&#xA;    ]&#xA;    img_dset = ImageDataset(path, transforms_=transforms_)&#xA;    print(len(img_dset))&#xA;    dataloader = DataLoader(&#xA;        img_dset,&#xA;        batch_size=batch_size,&#xA;        shuffle=True,&#xA;        num_workers=workers,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;dataloader = DataLoader(...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torchvision\transforms\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.&#xA;warnings.warn(&#xA;Traceback (most recent call last):&#xA;File &amp;quot;C:/Users//PycharmProjects/myFirstGAN/context_train.py&amp;quot;, line 82, in &#xA;test_dataloader = DataLoader(&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\dataloader.py&amp;quot;, line 266, in &lt;strong&gt;init&lt;/strong&gt;&#xA;sampler = RandomSampler(dataset, generator=generator)  # type: ignore&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\sampler.py&amp;quot;, line 103, in &lt;strong&gt;init&lt;/strong&gt;&#xA;raise ValueError(&amp;quot;num_samples should be a positive integer &amp;quot;&#xA;ValueError: num_samples should be a positive integer value, but got num_samples=0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;             , :&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;    .&lt;/li&gt;&#xA;&lt;li&gt;     len.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;    ,   . len  1044 (   ),      .  ,          .&lt;/p&gt;&#xA;"" OwnerUserId=""364669"" LastActivityDate=""2023-10-14T11:00:30.070"" Title=""PyTorch. ValueError: num_samples should be a positive integer value, but got num_samples=0"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1277670"" PostTypeId=""1"" CreationDate=""2021-05-03T13:19:30.727"" Score=""0"" ViewCount=""891"" Body=""&lt;p&gt;   PyTorch.  GAN,       .                  (ImageFolder).    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class ImageDataset(Dataset):&#xA;    def __init__(self, root, transforms_=None, image_size=image_size, mask_size=mask_size, mode=&amp;quot;train&amp;quot;):&#xA;        self.transform = transforms.Compose(transforms_)&#xA;        self.image_size = image_size&#xA;        self.mask_size = mask_size&#xA;        self.mode = mode&#xA;        self.files = sorted(glob.glob(&amp;quot;%s/**/*.jpg&amp;quot; % root, recursive=True))&#xA;        self.files = self.files[-4000:] if mode == &amp;quot;train&amp;quot; else self.files[:-4000]&#xA;...    &#xA;    def __len__(self):&#xA;        return len(self.files)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    transforms_ = [&#xA;        transforms.Grayscale(1),&#xA;        transforms.Resize((image_size, image_size), Image.BICUBIC),&#xA;        transforms.ToTensor(),&#xA;        transforms.Normalize((0.5, ), (0.5, )),&#xA;    ]&#xA;    img_dset = ImageDataset(path, transforms_=transforms_)&#xA;    print(len(img_dset))&#xA;    dataloader = DataLoader(&#xA;        img_dset,&#xA;        batch_size=batch_size,&#xA;        shuffle=True,&#xA;        num_workers=workers,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;dataloader = DataLoader(...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torchvision\transforms\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.&#xA;warnings.warn(&#xA;Traceback (most recent call last):&#xA;File &amp;quot;C:/Users//PycharmProjects/myFirstGAN/context_train.py&amp;quot;, line 82, in &#xA;test_dataloader = DataLoader(&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\dataloader.py&amp;quot;, line 266, in &lt;strong&gt;init&lt;/strong&gt;&#xA;sampler = RandomSampler(dataset, generator=generator)  # type: ignore&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\sampler.py&amp;quot;, line 103, in &lt;strong&gt;init&lt;/strong&gt;&#xA;raise ValueError(&amp;quot;num_samples should be a positive integer &amp;quot;&#xA;ValueError: num_samples should be a positive integer value, but got num_samples=0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;             , :&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;    .&lt;/li&gt;&#xA;&lt;li&gt;     len.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;    ,   . len  1044 (   ),      .  ,          .&lt;/p&gt;&#xA;"" OwnerUserId=""364669"" LastActivityDate=""2023-10-14T11:00:30.070"" Title=""PyTorch. ValueError: num_samples should be a positive integer value, but got num_samples=0"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1277670"" PostTypeId=""1"" CreationDate=""2021-05-03T13:19:30.727"" Score=""0"" ViewCount=""891"" Body=""&lt;p&gt;   PyTorch.  GAN,       .                  (ImageFolder).    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class ImageDataset(Dataset):&#xA;    def __init__(self, root, transforms_=None, image_size=image_size, mask_size=mask_size, mode=&amp;quot;train&amp;quot;):&#xA;        self.transform = transforms.Compose(transforms_)&#xA;        self.image_size = image_size&#xA;        self.mask_size = mask_size&#xA;        self.mode = mode&#xA;        self.files = sorted(glob.glob(&amp;quot;%s/**/*.jpg&amp;quot; % root, recursive=True))&#xA;        self.files = self.files[-4000:] if mode == &amp;quot;train&amp;quot; else self.files[:-4000]&#xA;...    &#xA;    def __len__(self):&#xA;        return len(self.files)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    transforms_ = [&#xA;        transforms.Grayscale(1),&#xA;        transforms.Resize((image_size, image_size), Image.BICUBIC),&#xA;        transforms.ToTensor(),&#xA;        transforms.Normalize((0.5, ), (0.5, )),&#xA;    ]&#xA;    img_dset = ImageDataset(path, transforms_=transforms_)&#xA;    print(len(img_dset))&#xA;    dataloader = DataLoader(&#xA;        img_dset,&#xA;        batch_size=batch_size,&#xA;        shuffle=True,&#xA;        num_workers=workers,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;dataloader = DataLoader(...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torchvision\transforms\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.&#xA;warnings.warn(&#xA;Traceback (most recent call last):&#xA;File &amp;quot;C:/Users//PycharmProjects/myFirstGAN/context_train.py&amp;quot;, line 82, in &#xA;test_dataloader = DataLoader(&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\dataloader.py&amp;quot;, line 266, in &lt;strong&gt;init&lt;/strong&gt;&#xA;sampler = RandomSampler(dataset, generator=generator)  # type: ignore&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\sampler.py&amp;quot;, line 103, in &lt;strong&gt;init&lt;/strong&gt;&#xA;raise ValueError(&amp;quot;num_samples should be a positive integer &amp;quot;&#xA;ValueError: num_samples should be a positive integer value, but got num_samples=0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;             , :&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;    .&lt;/li&gt;&#xA;&lt;li&gt;     len.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;    ,   . len  1044 (   ),      .  ,          .&lt;/p&gt;&#xA;"" OwnerUserId=""364669"" LastActivityDate=""2023-10-14T11:00:30.070"" Title=""PyTorch. ValueError: num_samples should be a positive integer value, but got num_samples=0"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1277670"" PostTypeId=""1"" CreationDate=""2021-05-03T13:19:30.727"" Score=""0"" ViewCount=""891"" Body=""&lt;p&gt;   PyTorch.  GAN,       .                  (ImageFolder).    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class ImageDataset(Dataset):&#xA;    def __init__(self, root, transforms_=None, image_size=image_size, mask_size=mask_size, mode=&amp;quot;train&amp;quot;):&#xA;        self.transform = transforms.Compose(transforms_)&#xA;        self.image_size = image_size&#xA;        self.mask_size = mask_size&#xA;        self.mode = mode&#xA;        self.files = sorted(glob.glob(&amp;quot;%s/**/*.jpg&amp;quot; % root, recursive=True))&#xA;        self.files = self.files[-4000:] if mode == &amp;quot;train&amp;quot; else self.files[:-4000]&#xA;...    &#xA;    def __len__(self):&#xA;        return len(self.files)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    transforms_ = [&#xA;        transforms.Grayscale(1),&#xA;        transforms.Resize((image_size, image_size), Image.BICUBIC),&#xA;        transforms.ToTensor(),&#xA;        transforms.Normalize((0.5, ), (0.5, )),&#xA;    ]&#xA;    img_dset = ImageDataset(path, transforms_=transforms_)&#xA;    print(len(img_dset))&#xA;    dataloader = DataLoader(&#xA;        img_dset,&#xA;        batch_size=batch_size,&#xA;        shuffle=True,&#xA;        num_workers=workers,&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;dataloader = DataLoader(...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torchvision\transforms\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.&#xA;warnings.warn(&#xA;Traceback (most recent call last):&#xA;File &amp;quot;C:/Users//PycharmProjects/myFirstGAN/context_train.py&amp;quot;, line 82, in &#xA;test_dataloader = DataLoader(&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\dataloader.py&amp;quot;, line 266, in &lt;strong&gt;init&lt;/strong&gt;&#xA;sampler = RandomSampler(dataset, generator=generator)  # type: ignore&#xA;File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\data\sampler.py&amp;quot;, line 103, in &lt;strong&gt;init&lt;/strong&gt;&#xA;raise ValueError(&amp;quot;num_samples should be a positive integer &amp;quot;&#xA;ValueError: num_samples should be a positive integer value, but got num_samples=0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;             , :&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;    .&lt;/li&gt;&#xA;&lt;li&gt;     len.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;    ,   . len  1044 (   ),      .  ,          .&lt;/p&gt;&#xA;"" OwnerUserId=""364669"" LastActivityDate=""2023-10-14T11:00:30.070"" Title=""PyTorch. ValueError: num_samples should be a positive integer value, but got num_samples=0"" Tags=""&lt;python&gt;&lt;pytorch&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1355227"" PostTypeId=""1"" CreationDate=""2021-11-29T10:45:54.327"" Score=""0"" ViewCount=""118"" Body=""&lt;pre&gt;&lt;code&gt;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 11:53:29.606911: E tensorflow/stream_executor/cuda/cuda_driver.cc:1018] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607332: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607544: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607703: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607898: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.608111: E tensorflow/stream_executor/cuda/cuda_driver.cc:1163] failed to enqueue async memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated; host dst: 0xf6f6f6bc90; GPU src: (nil); size: 8=0x8&#xA;2021-11-29 11:53:29.608404: F tensorflow/stream_executor/cuda/cuda_dnn.cc:213] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.&#xA;PS C:\Users\kamil&amp;gt; &amp;amp; C:/Users/kamil/.conda/envs/tensorflow/python.exe d:/dataset/recognition.py&#xA;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;2021-11-29 12:08:49.870032: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;2021-11-29 12:08:50.244496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1402 MB memory:  -&amp;gt; device: 0, name: GeForce GT 710, pci bus id: 0000:08:00.0, compute capability: 3.5&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 12:08:52.154918: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100&#xA;2021-11-29 12:08:53.124547: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller &#xA;indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:54.167153: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1018.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.166410: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 729.89MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.479574: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.879992: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 360.53MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.880466: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 532.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.507983: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 790.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.650998: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 289.45MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:57.183400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 866.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:09:01.663207: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA; 8/95 [=&amp;gt;............................] - ETA: 6:02 - loss: 2.7465 - accuracy: 0.35692021-11-29 12:09:47.664194: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 321.75MiB (rounded to 337383424)requested by op gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA;If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.&#xA;Current allocation summary follows.&#xA;Current allocation summary follows.&#xA;2021-11-29 12:09:47.665180: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc&#xA;2021-11-29 12:09:47.665411: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256):  Total Chunks: 51, Chunks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1.7KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665686: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512):  Total Chunks: 5, Chunks in use: 3. 3.2KiB allocated for chunks. 2.2KiB in use in bin. 2.2KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665923: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024):         Total Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 3.5KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666184: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666454: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096):         Total Chunks: 4, Chunks in use: 4. 18.0KiB allocated for chunks. 18.0KiB in use &#xA;in bin. 18.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666713: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384):        Total Chunks: 3, Chunks in use: 3. 67.5KiB allocated for chunks. 67.5KiB in use &#xA;in bin. 54.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.667116: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667285: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667447: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667631: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667802: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667976: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668154: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668496: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668893: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432):     Total Chunks: 4, Chunks in use: 3. 153.09MiB allocated for chunks. 114.84MiB in &#xA;use in bin. 114.84MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864):     Total Chunks: 2, Chunks in use: 2. 243.00MiB allocated for chunks. 243.00MiB in &#xA;use in bin. 201.94MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669305: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728):    Total Chunks: 2, Chunks in use: 1. 390.41MiB allocated for chunks. 151.33MiB in &#xA;use in bin. 80.44MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669493: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456):    Total Chunks: 2, Chunks in use: 1. 615.49MiB allocated for chunks. 321.75MiB in &#xA;use in bin. 321.75MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669666: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 321.75MiB was 256.00MiB, Chunk State:&#xA;2021-11-29 12:09:47.669765: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 293.74MiB | Requested Size: 158.64MiB | in_use: 0 | bin_num: 20, prev:   Size: 321.75MiB | Requested Size: 321.75MiB | in_use: 1 | bin_num: -1, next:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1&#xA;2021-11-29 12:09:47.669964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1470208256&#xA;2021-11-29 12:09:47.670074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0000 of size 256 next 1&#xA;2021-11-29 12:09:47.670195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0100 of size 1280 next 2&#xA;2021-11-29 12:09:47.670307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0600 of size 256 next 3&#xA;2021-11-29 12:09:47.670422: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0700 of size 256 next 4&#xA;2021-11-29 12:09:47.670530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0800 of size 256 next 5&#xA;2021-11-29 12:09:47.670631: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0900 of size 256 next 6&#xA;2021-11-29 12:09:47.670739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0a00 of size 256 next 9&#xA;2021-11-29 12:09:47.670827: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0b00 of size 256 next 10&#xA;2021-11-29 12:09:47.670910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0c00 of size 256 next 11&#xA;2021-11-29 12:09:47.670992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0d00 of size 256 next 14&#xA;2021-11-29 12:09:47.671075: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0e00 of size 256 next 15&#xA;2021-11-29 12:09:47.671186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0f00 of size 256 next 16&#xA;2021-11-29 12:09:47.671293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1000 of size 256 next 7&#xA;2021-11-29 12:09:47.671395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1100 of size 1024 next 8&#xA;2021-11-29 12:09:47.671511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1500 of size 256 next 19&#xA;2021-11-29 12:09:47.671628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1600 of size 256 next 20&#xA;2021-11-29 12:09:47.671738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1700 of size 256 next 23&#xA;2021-11-29 12:09:47.671900: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1800 of size 256 next 24&#xA;2021-11-29 12:09:47.672014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1900 of size 256 next 25&#xA;2021-11-29 12:09:47.672130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1a00 of size 256 next 26&#xA;2021-11-29 12:09:47.672231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1b00 of size 256 next 29&#xA;2021-11-29 12:09:47.672337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1c00 of size 256 next 30&#xA;2021-11-29 12:09:47.672445: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1d00 of size 256 next 31&#xA;2021-11-29 12:09:47.672556: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1e00 of size 256 next 32&#xA;2021-11-29 12:09:47.672670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1f00 of size 256 next 27&#xA;2021-11-29 12:09:47.672771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2000 of size 768 next 28&#xA;2021-11-29 12:09:47.672875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2300 of size 256 next 33&#xA;2021-11-29 12:09:47.672970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2400 of size 256 next 34&#xA;2021-11-29 12:09:47.673066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2500 of size 256 next 35&#xA;2021-11-29 12:09:47.673177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2600 of size 1024 next 36&#xA;2021-11-29 12:09:47.673286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2a00 of size 256 next 37&#xA;2021-11-29 12:09:47.673399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2b00 of size 256 next 39&#xA;2021-11-29 12:09:47.673498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2c00 of size 256 next 40&#xA;2021-11-29 12:09:47.673600: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2d00 of size 256 next 42&#xA;2021-11-29 12:09:47.673697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2e00 of size 768 next 43&#xA;2021-11-29 12:09:47.673796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3100 of size 256 next 44&#xA;2021-11-29 12:09:47.673899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3200 of size 1792 next 13&#xA;2021-11-29 12:09:47.674002: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3900 of size 4608 next 12&#xA;2021-11-29 12:09:47.674102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac4b00 of size 4608 next 38&#xA;2021-11-29 12:09:47.674230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac5d00 of size 32256 next 18&#xA;2021-11-29 12:09:47.674351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00acdb00 of size 18432 next 17&#xA;2021-11-29 12:09:47.674461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ad2300 of size 40140800 next 41&#xA;2021-11-29 12:09:47.674565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a300 of size 256 next 45&#xA;2021-11-29 12:09:47.674671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a400 of size 4608 next 46&#xA;2021-11-29 12:09:47.674779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b600 of size 256 next 47&#xA;2021-11-29 12:09:47.674890: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b700 of size 18432 next 48&#xA;2021-11-29 12:09:47.674996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311ff00 of size 256 next 49&#xA;2021-11-29 12:09:47.675096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120000 of size 256 next 51&#xA;2021-11-29 12:09:47.675216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120100 of size 768 next 52&#xA;2021-11-29 12:09:47.675318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120400 of size 256 next 53&#xA;2021-11-29 12:09:47.675412: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120500 of size 256 next 54&#xA;2021-11-29 12:09:47.675512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120600 of size 256 next 55&#xA;2021-11-29 12:09:47.675613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120700 of size 256 next 56&#xA;2021-11-29 12:09:47.675720: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120800 of size 256 next 57&#xA;2021-11-29 12:09:47.675819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120900 of size 256 next 58&#xA;2021-11-29 12:09:47.675910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120a00 of size 256 next 59&#xA;2021-11-29 12:09:47.676003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120b00 of size 256 next 60&#xA;2021-11-29 12:09:47.676097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120c00 of size 256 next 72&#xA;2021-11-29 12:09:47.676219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120d00 of size 256 next 76&#xA;2021-11-29 12:09:47.676333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120e00 of size 256 next 64&#xA;2021-11-29 12:09:47.676446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120f00 of size 256 next 80&#xA;2021-11-29 12:09:47.676552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121000 of size 512 next 61&#xA;2021-11-29 12:09:47.676663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121200 of size 256 next 83&#xA;2021-11-29 12:09:47.676779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121300 of size 256 next 74&#xA;2021-11-29 12:09:47.676883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121400 of size 512 next 66&#xA;2021-11-29 12:09:47.676993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121600 of size 256 next 62&#xA;2021-11-29 12:09:47.677098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121700 of size 4608 next 63&#xA;2021-11-29 12:09:47.677215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03122900 of size 40106496 next 22&#xA;2021-11-29 12:09:47.677326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b05762300 of size 40140800 next 21&#xA;2021-11-29 12:09:47.677438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b07daa300 of size 40140800 next 50&#xA;2021-11-29 12:09:47.677551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0a3f2300 of size 127401984 next 68&#xA;2021-11-29 12:09:47.677668: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b11d72300 of size 127401984 next 75&#xA;2021-11-29 12:09:47.677765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b196f2300 of size 337383424 next 65&#xA;2021-11-29 12:09:47.677880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b2d8b3300 of size 308004864 next 73&#xA;2021-11-29 12:09:47.677998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b3fe6fb00 of size 256 next 79&#xA;2021-11-29 12:09:47.678105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b3fe6fc00 of size 250694656 next 70&#xA;2021-11-29 12:09:47.678221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b4ed84800 of size 158683392 next 18446744073709551615&#xA;2021-11-29 12:09:47.678337: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:&#xA;2021-11-29 12:09:47.678464: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 51 Chunks of size 256 totalling 12.8KiB&#xA;2021-11-29 12:09:47.678579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 768 totalling 2.2KiB&#xA;2021-11-29 12:09:47.678682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1024 totalling 2.0KiB&#xA;2021-11-29 12:09:47.678784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB&#xA;2021-11-29 12:09:47.678903: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB&#xA;2021-11-29 12:09:47.679003: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4608 totalling 18.0KiB&#xA;2021-11-29 12:09:47.679110: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 18432 totalling 36.0KiB&#xA;2021-11-29 12:09:47.679223: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB&#xA;2021-11-29 12:09:47.679344: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 40140800 totalling 114.84MiB&#xA;2021-11-29 12:09:47.679508: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 127401984 totalling 243.00MiB&#xA;2021-11-29 12:09:47.679620: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 158683392 totalling 151.33MiB&#xA;2021-11-29 12:09:47.679732: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 337383424 totalling 321.75MiB&#xA;2021-11-29 12:09:47.679848: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 831.03MiB&#xA;2021-11-29 12:09:47.679969: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1470208256 memory_limit_: 1470208411 available bytes: 155 curr_region_allocation_bytes_: 2940417024&#xA;2021-11-29 12:09:47.680128: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:&#xA;Limit:                      1470208411&#xA;InUse:                       871401216&#xA;MaxInUse:                   1470208000&#xA;NumAllocs:                         796&#xA;MaxAllocSize:                402397184&#xA;Reserved:                            0&#xA;PeakReserved:                        0&#xA;LargestFreeBlock:                    0&#xA;&#xA;2021-11-29 12:09:47.680737: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***__************xx*********************************____________________*________________******xxxxx &#xA;2021-11-29 12:09:47.680863: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at pooling_ops_common.cc:458 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;    history = model.fit(train_generator,&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler&#xA;    raise e.with_traceback(filtered_tb) from None&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\eager\execute.py&amp;quot;, line 58, in quick_execute&#xA;    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,&#xA;tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;         [[node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA; (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:464)&#xA;]]&#xA;Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.&#xA; [Op:__inference_train_function_909]&#xA;&#xA;Errors may have originated from an input operation.&#xA;Input Source operations connected to node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad:&#xA;In[0] sequential/conv2d/Relu (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\backend.py:4867)&#xA;In[1] sequential/max_pooling2d/MaxPool (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\layers\pooling.py:357)&#xA;In[2] gradient_tape/sequential/conv2d_1/Conv2D/Conv2DBackpropInput:&#xA;&#xA;Operation defined at: (most recent call last)&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;     modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;&amp;gt;&amp;gt;&amp;gt;     history = model.fit(train_generator,&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 64, in error_handler&#xA;&amp;gt;&amp;gt;&amp;gt;     return fn(*args, **kwargs)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 1216, in fit&#xA;&amp;gt;&amp;gt;&amp;gt;     tmp_logs = self.train_function(iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 878, in train_function&#xA;&amp;gt;&amp;gt;&amp;gt;     return step_function(self, iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 867, in step_function&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.distribute_strategy.run(run_step, args=(data,))&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 860, in run_step&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.train_step(data)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 816, in train_step&#xA;&amp;gt;&amp;gt;&amp;gt;     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 530, in minimize&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._compute_gradients(&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 583, in _compute_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 464, in _get_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads = tape.gradient(loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;2021-11-29 12:09:48.275244: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.&#xA;         [[{{node PyFunc}}]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; !&lt;/p&gt;&#xA;"" OwnerUserId=""448322"" LastEditorUserId=""448322"" LastEditDate=""2021-11-29T11:10:22.013"" LastActivityDate=""2021-11-29T11:10:22.013"" Title=""     "" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1355227"" PostTypeId=""1"" CreationDate=""2021-11-29T10:45:54.327"" Score=""0"" ViewCount=""118"" Body=""&lt;pre&gt;&lt;code&gt;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 11:53:29.606911: E tensorflow/stream_executor/cuda/cuda_driver.cc:1018] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607332: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607544: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607703: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607898: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.608111: E tensorflow/stream_executor/cuda/cuda_driver.cc:1163] failed to enqueue async memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated; host dst: 0xf6f6f6bc90; GPU src: (nil); size: 8=0x8&#xA;2021-11-29 11:53:29.608404: F tensorflow/stream_executor/cuda/cuda_dnn.cc:213] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.&#xA;PS C:\Users\kamil&amp;gt; &amp;amp; C:/Users/kamil/.conda/envs/tensorflow/python.exe d:/dataset/recognition.py&#xA;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;2021-11-29 12:08:49.870032: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;2021-11-29 12:08:50.244496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1402 MB memory:  -&amp;gt; device: 0, name: GeForce GT 710, pci bus id: 0000:08:00.0, compute capability: 3.5&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 12:08:52.154918: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100&#xA;2021-11-29 12:08:53.124547: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller &#xA;indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:54.167153: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1018.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.166410: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 729.89MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.479574: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.879992: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 360.53MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.880466: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 532.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.507983: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 790.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.650998: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 289.45MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:57.183400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 866.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:09:01.663207: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA; 8/95 [=&amp;gt;............................] - ETA: 6:02 - loss: 2.7465 - accuracy: 0.35692021-11-29 12:09:47.664194: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 321.75MiB (rounded to 337383424)requested by op gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA;If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.&#xA;Current allocation summary follows.&#xA;Current allocation summary follows.&#xA;2021-11-29 12:09:47.665180: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc&#xA;2021-11-29 12:09:47.665411: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256):  Total Chunks: 51, Chunks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1.7KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665686: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512):  Total Chunks: 5, Chunks in use: 3. 3.2KiB allocated for chunks. 2.2KiB in use in bin. 2.2KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665923: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024):         Total Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 3.5KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666184: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666454: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096):         Total Chunks: 4, Chunks in use: 4. 18.0KiB allocated for chunks. 18.0KiB in use &#xA;in bin. 18.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666713: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384):        Total Chunks: 3, Chunks in use: 3. 67.5KiB allocated for chunks. 67.5KiB in use &#xA;in bin. 54.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.667116: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667285: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667447: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667631: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667802: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667976: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668154: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668496: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668893: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432):     Total Chunks: 4, Chunks in use: 3. 153.09MiB allocated for chunks. 114.84MiB in &#xA;use in bin. 114.84MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864):     Total Chunks: 2, Chunks in use: 2. 243.00MiB allocated for chunks. 243.00MiB in &#xA;use in bin. 201.94MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669305: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728):    Total Chunks: 2, Chunks in use: 1. 390.41MiB allocated for chunks. 151.33MiB in &#xA;use in bin. 80.44MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669493: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456):    Total Chunks: 2, Chunks in use: 1. 615.49MiB allocated for chunks. 321.75MiB in &#xA;use in bin. 321.75MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669666: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 321.75MiB was 256.00MiB, Chunk State:&#xA;2021-11-29 12:09:47.669765: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 293.74MiB | Requested Size: 158.64MiB | in_use: 0 | bin_num: 20, prev:   Size: 321.75MiB | Requested Size: 321.75MiB | in_use: 1 | bin_num: -1, next:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1&#xA;2021-11-29 12:09:47.669964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1470208256&#xA;2021-11-29 12:09:47.670074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0000 of size 256 next 1&#xA;2021-11-29 12:09:47.670195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0100 of size 1280 next 2&#xA;2021-11-29 12:09:47.670307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0600 of size 256 next 3&#xA;2021-11-29 12:09:47.670422: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0700 of size 256 next 4&#xA;2021-11-29 12:09:47.670530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0800 of size 256 next 5&#xA;2021-11-29 12:09:47.670631: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0900 of size 256 next 6&#xA;2021-11-29 12:09:47.670739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0a00 of size 256 next 9&#xA;2021-11-29 12:09:47.670827: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0b00 of size 256 next 10&#xA;2021-11-29 12:09:47.670910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0c00 of size 256 next 11&#xA;2021-11-29 12:09:47.670992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0d00 of size 256 next 14&#xA;2021-11-29 12:09:47.671075: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0e00 of size 256 next 15&#xA;2021-11-29 12:09:47.671186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0f00 of size 256 next 16&#xA;2021-11-29 12:09:47.671293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1000 of size 256 next 7&#xA;2021-11-29 12:09:47.671395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1100 of size 1024 next 8&#xA;2021-11-29 12:09:47.671511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1500 of size 256 next 19&#xA;2021-11-29 12:09:47.671628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1600 of size 256 next 20&#xA;2021-11-29 12:09:47.671738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1700 of size 256 next 23&#xA;2021-11-29 12:09:47.671900: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1800 of size 256 next 24&#xA;2021-11-29 12:09:47.672014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1900 of size 256 next 25&#xA;2021-11-29 12:09:47.672130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1a00 of size 256 next 26&#xA;2021-11-29 12:09:47.672231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1b00 of size 256 next 29&#xA;2021-11-29 12:09:47.672337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1c00 of size 256 next 30&#xA;2021-11-29 12:09:47.672445: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1d00 of size 256 next 31&#xA;2021-11-29 12:09:47.672556: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1e00 of size 256 next 32&#xA;2021-11-29 12:09:47.672670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1f00 of size 256 next 27&#xA;2021-11-29 12:09:47.672771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2000 of size 768 next 28&#xA;2021-11-29 12:09:47.672875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2300 of size 256 next 33&#xA;2021-11-29 12:09:47.672970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2400 of size 256 next 34&#xA;2021-11-29 12:09:47.673066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2500 of size 256 next 35&#xA;2021-11-29 12:09:47.673177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2600 of size 1024 next 36&#xA;2021-11-29 12:09:47.673286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2a00 of size 256 next 37&#xA;2021-11-29 12:09:47.673399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2b00 of size 256 next 39&#xA;2021-11-29 12:09:47.673498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2c00 of size 256 next 40&#xA;2021-11-29 12:09:47.673600: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2d00 of size 256 next 42&#xA;2021-11-29 12:09:47.673697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2e00 of size 768 next 43&#xA;2021-11-29 12:09:47.673796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3100 of size 256 next 44&#xA;2021-11-29 12:09:47.673899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3200 of size 1792 next 13&#xA;2021-11-29 12:09:47.674002: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3900 of size 4608 next 12&#xA;2021-11-29 12:09:47.674102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac4b00 of size 4608 next 38&#xA;2021-11-29 12:09:47.674230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac5d00 of size 32256 next 18&#xA;2021-11-29 12:09:47.674351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00acdb00 of size 18432 next 17&#xA;2021-11-29 12:09:47.674461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ad2300 of size 40140800 next 41&#xA;2021-11-29 12:09:47.674565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a300 of size 256 next 45&#xA;2021-11-29 12:09:47.674671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a400 of size 4608 next 46&#xA;2021-11-29 12:09:47.674779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b600 of size 256 next 47&#xA;2021-11-29 12:09:47.674890: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b700 of size 18432 next 48&#xA;2021-11-29 12:09:47.674996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311ff00 of size 256 next 49&#xA;2021-11-29 12:09:47.675096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120000 of size 256 next 51&#xA;2021-11-29 12:09:47.675216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120100 of size 768 next 52&#xA;2021-11-29 12:09:47.675318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120400 of size 256 next 53&#xA;2021-11-29 12:09:47.675412: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120500 of size 256 next 54&#xA;2021-11-29 12:09:47.675512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120600 of size 256 next 55&#xA;2021-11-29 12:09:47.675613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120700 of size 256 next 56&#xA;2021-11-29 12:09:47.675720: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120800 of size 256 next 57&#xA;2021-11-29 12:09:47.675819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120900 of size 256 next 58&#xA;2021-11-29 12:09:47.675910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120a00 of size 256 next 59&#xA;2021-11-29 12:09:47.676003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120b00 of size 256 next 60&#xA;2021-11-29 12:09:47.676097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120c00 of size 256 next 72&#xA;2021-11-29 12:09:47.676219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120d00 of size 256 next 76&#xA;2021-11-29 12:09:47.676333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120e00 of size 256 next 64&#xA;2021-11-29 12:09:47.676446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120f00 of size 256 next 80&#xA;2021-11-29 12:09:47.676552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121000 of size 512 next 61&#xA;2021-11-29 12:09:47.676663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121200 of size 256 next 83&#xA;2021-11-29 12:09:47.676779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121300 of size 256 next 74&#xA;2021-11-29 12:09:47.676883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121400 of size 512 next 66&#xA;2021-11-29 12:09:47.676993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121600 of size 256 next 62&#xA;2021-11-29 12:09:47.677098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121700 of size 4608 next 63&#xA;2021-11-29 12:09:47.677215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03122900 of size 40106496 next 22&#xA;2021-11-29 12:09:47.677326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b05762300 of size 40140800 next 21&#xA;2021-11-29 12:09:47.677438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b07daa300 of size 40140800 next 50&#xA;2021-11-29 12:09:47.677551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0a3f2300 of size 127401984 next 68&#xA;2021-11-29 12:09:47.677668: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b11d72300 of size 127401984 next 75&#xA;2021-11-29 12:09:47.677765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b196f2300 of size 337383424 next 65&#xA;2021-11-29 12:09:47.677880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b2d8b3300 of size 308004864 next 73&#xA;2021-11-29 12:09:47.677998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b3fe6fb00 of size 256 next 79&#xA;2021-11-29 12:09:47.678105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b3fe6fc00 of size 250694656 next 70&#xA;2021-11-29 12:09:47.678221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b4ed84800 of size 158683392 next 18446744073709551615&#xA;2021-11-29 12:09:47.678337: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:&#xA;2021-11-29 12:09:47.678464: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 51 Chunks of size 256 totalling 12.8KiB&#xA;2021-11-29 12:09:47.678579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 768 totalling 2.2KiB&#xA;2021-11-29 12:09:47.678682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1024 totalling 2.0KiB&#xA;2021-11-29 12:09:47.678784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB&#xA;2021-11-29 12:09:47.678903: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB&#xA;2021-11-29 12:09:47.679003: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4608 totalling 18.0KiB&#xA;2021-11-29 12:09:47.679110: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 18432 totalling 36.0KiB&#xA;2021-11-29 12:09:47.679223: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB&#xA;2021-11-29 12:09:47.679344: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 40140800 totalling 114.84MiB&#xA;2021-11-29 12:09:47.679508: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 127401984 totalling 243.00MiB&#xA;2021-11-29 12:09:47.679620: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 158683392 totalling 151.33MiB&#xA;2021-11-29 12:09:47.679732: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 337383424 totalling 321.75MiB&#xA;2021-11-29 12:09:47.679848: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 831.03MiB&#xA;2021-11-29 12:09:47.679969: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1470208256 memory_limit_: 1470208411 available bytes: 155 curr_region_allocation_bytes_: 2940417024&#xA;2021-11-29 12:09:47.680128: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:&#xA;Limit:                      1470208411&#xA;InUse:                       871401216&#xA;MaxInUse:                   1470208000&#xA;NumAllocs:                         796&#xA;MaxAllocSize:                402397184&#xA;Reserved:                            0&#xA;PeakReserved:                        0&#xA;LargestFreeBlock:                    0&#xA;&#xA;2021-11-29 12:09:47.680737: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***__************xx*********************************____________________*________________******xxxxx &#xA;2021-11-29 12:09:47.680863: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at pooling_ops_common.cc:458 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;    history = model.fit(train_generator,&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler&#xA;    raise e.with_traceback(filtered_tb) from None&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\eager\execute.py&amp;quot;, line 58, in quick_execute&#xA;    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,&#xA;tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;         [[node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA; (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:464)&#xA;]]&#xA;Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.&#xA; [Op:__inference_train_function_909]&#xA;&#xA;Errors may have originated from an input operation.&#xA;Input Source operations connected to node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad:&#xA;In[0] sequential/conv2d/Relu (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\backend.py:4867)&#xA;In[1] sequential/max_pooling2d/MaxPool (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\layers\pooling.py:357)&#xA;In[2] gradient_tape/sequential/conv2d_1/Conv2D/Conv2DBackpropInput:&#xA;&#xA;Operation defined at: (most recent call last)&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;     modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;&amp;gt;&amp;gt;&amp;gt;     history = model.fit(train_generator,&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 64, in error_handler&#xA;&amp;gt;&amp;gt;&amp;gt;     return fn(*args, **kwargs)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 1216, in fit&#xA;&amp;gt;&amp;gt;&amp;gt;     tmp_logs = self.train_function(iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 878, in train_function&#xA;&amp;gt;&amp;gt;&amp;gt;     return step_function(self, iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 867, in step_function&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.distribute_strategy.run(run_step, args=(data,))&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 860, in run_step&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.train_step(data)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 816, in train_step&#xA;&amp;gt;&amp;gt;&amp;gt;     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 530, in minimize&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._compute_gradients(&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 583, in _compute_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 464, in _get_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads = tape.gradient(loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;2021-11-29 12:09:48.275244: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.&#xA;         [[{{node PyFunc}}]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; !&lt;/p&gt;&#xA;"" OwnerUserId=""448322"" LastEditorUserId=""448322"" LastEditDate=""2021-11-29T11:10:22.013"" LastActivityDate=""2021-11-29T11:10:22.013"" Title=""     "" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1355227"" PostTypeId=""1"" CreationDate=""2021-11-29T10:45:54.327"" Score=""0"" ViewCount=""118"" Body=""&lt;pre&gt;&lt;code&gt;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 11:53:29.606911: E tensorflow/stream_executor/cuda/cuda_driver.cc:1018] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607332: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607544: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607703: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607898: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.608111: E tensorflow/stream_executor/cuda/cuda_driver.cc:1163] failed to enqueue async memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated; host dst: 0xf6f6f6bc90; GPU src: (nil); size: 8=0x8&#xA;2021-11-29 11:53:29.608404: F tensorflow/stream_executor/cuda/cuda_dnn.cc:213] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.&#xA;PS C:\Users\kamil&amp;gt; &amp;amp; C:/Users/kamil/.conda/envs/tensorflow/python.exe d:/dataset/recognition.py&#xA;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;2021-11-29 12:08:49.870032: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;2021-11-29 12:08:50.244496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1402 MB memory:  -&amp;gt; device: 0, name: GeForce GT 710, pci bus id: 0000:08:00.0, compute capability: 3.5&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 12:08:52.154918: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100&#xA;2021-11-29 12:08:53.124547: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller &#xA;indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:54.167153: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1018.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.166410: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 729.89MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.479574: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.879992: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 360.53MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.880466: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 532.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.507983: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 790.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.650998: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 289.45MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:57.183400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 866.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:09:01.663207: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA; 8/95 [=&amp;gt;............................] - ETA: 6:02 - loss: 2.7465 - accuracy: 0.35692021-11-29 12:09:47.664194: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 321.75MiB (rounded to 337383424)requested by op gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA;If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.&#xA;Current allocation summary follows.&#xA;Current allocation summary follows.&#xA;2021-11-29 12:09:47.665180: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc&#xA;2021-11-29 12:09:47.665411: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256):  Total Chunks: 51, Chunks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1.7KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665686: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512):  Total Chunks: 5, Chunks in use: 3. 3.2KiB allocated for chunks. 2.2KiB in use in bin. 2.2KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665923: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024):         Total Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 3.5KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666184: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666454: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096):         Total Chunks: 4, Chunks in use: 4. 18.0KiB allocated for chunks. 18.0KiB in use &#xA;in bin. 18.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666713: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384):        Total Chunks: 3, Chunks in use: 3. 67.5KiB allocated for chunks. 67.5KiB in use &#xA;in bin. 54.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.667116: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667285: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667447: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667631: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667802: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667976: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668154: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668496: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668893: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432):     Total Chunks: 4, Chunks in use: 3. 153.09MiB allocated for chunks. 114.84MiB in &#xA;use in bin. 114.84MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864):     Total Chunks: 2, Chunks in use: 2. 243.00MiB allocated for chunks. 243.00MiB in &#xA;use in bin. 201.94MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669305: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728):    Total Chunks: 2, Chunks in use: 1. 390.41MiB allocated for chunks. 151.33MiB in &#xA;use in bin. 80.44MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669493: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456):    Total Chunks: 2, Chunks in use: 1. 615.49MiB allocated for chunks. 321.75MiB in &#xA;use in bin. 321.75MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669666: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 321.75MiB was 256.00MiB, Chunk State:&#xA;2021-11-29 12:09:47.669765: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 293.74MiB | Requested Size: 158.64MiB | in_use: 0 | bin_num: 20, prev:   Size: 321.75MiB | Requested Size: 321.75MiB | in_use: 1 | bin_num: -1, next:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1&#xA;2021-11-29 12:09:47.669964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1470208256&#xA;2021-11-29 12:09:47.670074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0000 of size 256 next 1&#xA;2021-11-29 12:09:47.670195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0100 of size 1280 next 2&#xA;2021-11-29 12:09:47.670307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0600 of size 256 next 3&#xA;2021-11-29 12:09:47.670422: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0700 of size 256 next 4&#xA;2021-11-29 12:09:47.670530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0800 of size 256 next 5&#xA;2021-11-29 12:09:47.670631: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0900 of size 256 next 6&#xA;2021-11-29 12:09:47.670739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0a00 of size 256 next 9&#xA;2021-11-29 12:09:47.670827: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0b00 of size 256 next 10&#xA;2021-11-29 12:09:47.670910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0c00 of size 256 next 11&#xA;2021-11-29 12:09:47.670992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0d00 of size 256 next 14&#xA;2021-11-29 12:09:47.671075: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0e00 of size 256 next 15&#xA;2021-11-29 12:09:47.671186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0f00 of size 256 next 16&#xA;2021-11-29 12:09:47.671293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1000 of size 256 next 7&#xA;2021-11-29 12:09:47.671395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1100 of size 1024 next 8&#xA;2021-11-29 12:09:47.671511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1500 of size 256 next 19&#xA;2021-11-29 12:09:47.671628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1600 of size 256 next 20&#xA;2021-11-29 12:09:47.671738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1700 of size 256 next 23&#xA;2021-11-29 12:09:47.671900: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1800 of size 256 next 24&#xA;2021-11-29 12:09:47.672014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1900 of size 256 next 25&#xA;2021-11-29 12:09:47.672130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1a00 of size 256 next 26&#xA;2021-11-29 12:09:47.672231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1b00 of size 256 next 29&#xA;2021-11-29 12:09:47.672337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1c00 of size 256 next 30&#xA;2021-11-29 12:09:47.672445: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1d00 of size 256 next 31&#xA;2021-11-29 12:09:47.672556: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1e00 of size 256 next 32&#xA;2021-11-29 12:09:47.672670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1f00 of size 256 next 27&#xA;2021-11-29 12:09:47.672771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2000 of size 768 next 28&#xA;2021-11-29 12:09:47.672875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2300 of size 256 next 33&#xA;2021-11-29 12:09:47.672970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2400 of size 256 next 34&#xA;2021-11-29 12:09:47.673066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2500 of size 256 next 35&#xA;2021-11-29 12:09:47.673177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2600 of size 1024 next 36&#xA;2021-11-29 12:09:47.673286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2a00 of size 256 next 37&#xA;2021-11-29 12:09:47.673399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2b00 of size 256 next 39&#xA;2021-11-29 12:09:47.673498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2c00 of size 256 next 40&#xA;2021-11-29 12:09:47.673600: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2d00 of size 256 next 42&#xA;2021-11-29 12:09:47.673697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2e00 of size 768 next 43&#xA;2021-11-29 12:09:47.673796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3100 of size 256 next 44&#xA;2021-11-29 12:09:47.673899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3200 of size 1792 next 13&#xA;2021-11-29 12:09:47.674002: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3900 of size 4608 next 12&#xA;2021-11-29 12:09:47.674102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac4b00 of size 4608 next 38&#xA;2021-11-29 12:09:47.674230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac5d00 of size 32256 next 18&#xA;2021-11-29 12:09:47.674351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00acdb00 of size 18432 next 17&#xA;2021-11-29 12:09:47.674461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ad2300 of size 40140800 next 41&#xA;2021-11-29 12:09:47.674565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a300 of size 256 next 45&#xA;2021-11-29 12:09:47.674671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a400 of size 4608 next 46&#xA;2021-11-29 12:09:47.674779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b600 of size 256 next 47&#xA;2021-11-29 12:09:47.674890: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b700 of size 18432 next 48&#xA;2021-11-29 12:09:47.674996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311ff00 of size 256 next 49&#xA;2021-11-29 12:09:47.675096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120000 of size 256 next 51&#xA;2021-11-29 12:09:47.675216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120100 of size 768 next 52&#xA;2021-11-29 12:09:47.675318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120400 of size 256 next 53&#xA;2021-11-29 12:09:47.675412: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120500 of size 256 next 54&#xA;2021-11-29 12:09:47.675512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120600 of size 256 next 55&#xA;2021-11-29 12:09:47.675613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120700 of size 256 next 56&#xA;2021-11-29 12:09:47.675720: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120800 of size 256 next 57&#xA;2021-11-29 12:09:47.675819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120900 of size 256 next 58&#xA;2021-11-29 12:09:47.675910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120a00 of size 256 next 59&#xA;2021-11-29 12:09:47.676003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120b00 of size 256 next 60&#xA;2021-11-29 12:09:47.676097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120c00 of size 256 next 72&#xA;2021-11-29 12:09:47.676219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120d00 of size 256 next 76&#xA;2021-11-29 12:09:47.676333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120e00 of size 256 next 64&#xA;2021-11-29 12:09:47.676446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120f00 of size 256 next 80&#xA;2021-11-29 12:09:47.676552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121000 of size 512 next 61&#xA;2021-11-29 12:09:47.676663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121200 of size 256 next 83&#xA;2021-11-29 12:09:47.676779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121300 of size 256 next 74&#xA;2021-11-29 12:09:47.676883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121400 of size 512 next 66&#xA;2021-11-29 12:09:47.676993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121600 of size 256 next 62&#xA;2021-11-29 12:09:47.677098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121700 of size 4608 next 63&#xA;2021-11-29 12:09:47.677215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03122900 of size 40106496 next 22&#xA;2021-11-29 12:09:47.677326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b05762300 of size 40140800 next 21&#xA;2021-11-29 12:09:47.677438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b07daa300 of size 40140800 next 50&#xA;2021-11-29 12:09:47.677551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0a3f2300 of size 127401984 next 68&#xA;2021-11-29 12:09:47.677668: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b11d72300 of size 127401984 next 75&#xA;2021-11-29 12:09:47.677765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b196f2300 of size 337383424 next 65&#xA;2021-11-29 12:09:47.677880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b2d8b3300 of size 308004864 next 73&#xA;2021-11-29 12:09:47.677998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b3fe6fb00 of size 256 next 79&#xA;2021-11-29 12:09:47.678105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b3fe6fc00 of size 250694656 next 70&#xA;2021-11-29 12:09:47.678221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b4ed84800 of size 158683392 next 18446744073709551615&#xA;2021-11-29 12:09:47.678337: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:&#xA;2021-11-29 12:09:47.678464: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 51 Chunks of size 256 totalling 12.8KiB&#xA;2021-11-29 12:09:47.678579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 768 totalling 2.2KiB&#xA;2021-11-29 12:09:47.678682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1024 totalling 2.0KiB&#xA;2021-11-29 12:09:47.678784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB&#xA;2021-11-29 12:09:47.678903: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB&#xA;2021-11-29 12:09:47.679003: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4608 totalling 18.0KiB&#xA;2021-11-29 12:09:47.679110: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 18432 totalling 36.0KiB&#xA;2021-11-29 12:09:47.679223: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB&#xA;2021-11-29 12:09:47.679344: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 40140800 totalling 114.84MiB&#xA;2021-11-29 12:09:47.679508: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 127401984 totalling 243.00MiB&#xA;2021-11-29 12:09:47.679620: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 158683392 totalling 151.33MiB&#xA;2021-11-29 12:09:47.679732: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 337383424 totalling 321.75MiB&#xA;2021-11-29 12:09:47.679848: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 831.03MiB&#xA;2021-11-29 12:09:47.679969: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1470208256 memory_limit_: 1470208411 available bytes: 155 curr_region_allocation_bytes_: 2940417024&#xA;2021-11-29 12:09:47.680128: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:&#xA;Limit:                      1470208411&#xA;InUse:                       871401216&#xA;MaxInUse:                   1470208000&#xA;NumAllocs:                         796&#xA;MaxAllocSize:                402397184&#xA;Reserved:                            0&#xA;PeakReserved:                        0&#xA;LargestFreeBlock:                    0&#xA;&#xA;2021-11-29 12:09:47.680737: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***__************xx*********************************____________________*________________******xxxxx &#xA;2021-11-29 12:09:47.680863: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at pooling_ops_common.cc:458 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;    history = model.fit(train_generator,&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler&#xA;    raise e.with_traceback(filtered_tb) from None&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\eager\execute.py&amp;quot;, line 58, in quick_execute&#xA;    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,&#xA;tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;         [[node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA; (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:464)&#xA;]]&#xA;Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.&#xA; [Op:__inference_train_function_909]&#xA;&#xA;Errors may have originated from an input operation.&#xA;Input Source operations connected to node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad:&#xA;In[0] sequential/conv2d/Relu (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\backend.py:4867)&#xA;In[1] sequential/max_pooling2d/MaxPool (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\layers\pooling.py:357)&#xA;In[2] gradient_tape/sequential/conv2d_1/Conv2D/Conv2DBackpropInput:&#xA;&#xA;Operation defined at: (most recent call last)&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;     modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;&amp;gt;&amp;gt;&amp;gt;     history = model.fit(train_generator,&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 64, in error_handler&#xA;&amp;gt;&amp;gt;&amp;gt;     return fn(*args, **kwargs)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 1216, in fit&#xA;&amp;gt;&amp;gt;&amp;gt;     tmp_logs = self.train_function(iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 878, in train_function&#xA;&amp;gt;&amp;gt;&amp;gt;     return step_function(self, iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 867, in step_function&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.distribute_strategy.run(run_step, args=(data,))&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 860, in run_step&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.train_step(data)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 816, in train_step&#xA;&amp;gt;&amp;gt;&amp;gt;     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 530, in minimize&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._compute_gradients(&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 583, in _compute_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 464, in _get_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads = tape.gradient(loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;2021-11-29 12:09:48.275244: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.&#xA;         [[{{node PyFunc}}]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; !&lt;/p&gt;&#xA;"" OwnerUserId=""448322"" LastEditorUserId=""448322"" LastEditDate=""2021-11-29T11:10:22.013"" LastActivityDate=""2021-11-29T11:10:22.013"" Title=""     "" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1355227"" PostTypeId=""1"" CreationDate=""2021-11-29T10:45:54.327"" Score=""0"" ViewCount=""118"" Body=""&lt;pre&gt;&lt;code&gt;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 11:53:29.606911: E tensorflow/stream_executor/cuda/cuda_driver.cc:1018] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607332: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607544: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] INTERNAL: Error destroying CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated &#xA;2021-11-29 11:53:29.607703: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.607898: E tensorflow/stream_executor/stream.cc:4476] INTERNAL: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated&#xA;2021-11-29 11:53:29.608111: E tensorflow/stream_executor/cuda/cuda_driver.cc:1163] failed to enqueue async memcpy from device to host: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated; host dst: 0xf6f6f6bc90; GPU src: (nil); size: 8=0x8&#xA;2021-11-29 11:53:29.608404: F tensorflow/stream_executor/cuda/cuda_dnn.cc:213] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.&#xA;PS C:\Users\kamil&amp;gt; &amp;amp; C:/Users/kamil/.conda/envs/tensorflow/python.exe d:/dataset/recognition.py&#xA;Found 575 images belonging to 3 classes.&#xA;Found 69 images belonging to 3 classes.&#xA;2021-11-29 12:08:49.870032: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;2021-11-29 12:08:50.244496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1402 MB memory:  -&amp;gt; device: 0, name: GeForce GT 710, pci bus id: 0000:08:00.0, compute capability: 3.5&#xA;Epoch 1/30&#xA;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:1096: UserWarning: &amp;quot;`categorical_crossentropy` received `from_logits=True`, but the `output` &#xA;argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&amp;quot;&#xA;  return dispatch_target(*args, **kwargs)&#xA;2021-11-29 12:08:52.154918: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100&#xA;2021-11-29 12:08:53.124547: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller &#xA;indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:54.167153: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1018.38MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.166410: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 729.89MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.479574: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.879992: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 360.53MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:55.880466: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 532.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.507983: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 790.02MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:56.650998: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 289.45MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:08:57.183400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 866.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA;2021-11-29 12:09:01.663207: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 562.77MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.&#xA; 8/95 [=&amp;gt;............................] - ETA: 6:02 - loss: 2.7465 - accuracy: 0.35692021-11-29 12:09:47.664194: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 321.75MiB (rounded to 337383424)requested by op gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA;If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.&#xA;Current allocation summary follows.&#xA;Current allocation summary follows.&#xA;2021-11-29 12:09:47.665180: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc&#xA;2021-11-29 12:09:47.665411: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256):  Total Chunks: 51, Chunks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1.7KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665686: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512):  Total Chunks: 5, Chunks in use: 3. 3.2KiB allocated for chunks. 2.2KiB in use in bin. 2.2KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.665923: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024):         Total Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 3.5KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666184: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666454: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096):         Total Chunks: 4, Chunks in use: 4. 18.0KiB allocated for chunks. 18.0KiB in use &#xA;in bin. 18.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.666713: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.666916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384):        Total Chunks: 3, Chunks in use: 3. 67.5KiB allocated for chunks. 67.5KiB in use &#xA;in bin. 54.0KiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.667116: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667285: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667447: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667631: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667802: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.667976: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668154: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668311: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668496: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.&#xA;2021-11-29 12:09:47.668893: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432):     Total Chunks: 4, Chunks in use: 3. 153.09MiB allocated for chunks. 114.84MiB in &#xA;use in bin. 114.84MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864):     Total Chunks: 2, Chunks in use: 2. 243.00MiB allocated for chunks. 243.00MiB in &#xA;use in bin. 201.94MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669305: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728):    Total Chunks: 2, Chunks in use: 1. 390.41MiB allocated for chunks. 151.33MiB in &#xA;use in bin. 80.44MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669493: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456):    Total Chunks: 2, Chunks in use: 1. 615.49MiB allocated for chunks. 321.75MiB in &#xA;use in bin. 321.75MiB client-requested in use in bin.&#xA;2021-11-29 12:09:47.669666: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 321.75MiB was 256.00MiB, Chunk State:&#xA;2021-11-29 12:09:47.669765: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 293.74MiB | Requested Size: 158.64MiB | in_use: 0 | bin_num: 20, prev:   Size: 321.75MiB | Requested Size: 321.75MiB | in_use: 1 | bin_num: -1, next:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1&#xA;2021-11-29 12:09:47.669964: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 1470208256&#xA;2021-11-29 12:09:47.670074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0000 of size 256 next 1&#xA;2021-11-29 12:09:47.670195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0100 of size 1280 next 2&#xA;2021-11-29 12:09:47.670307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0600 of size 256 next 3&#xA;2021-11-29 12:09:47.670422: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0700 of size 256 next 4&#xA;2021-11-29 12:09:47.670530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0800 of size 256 next 5&#xA;2021-11-29 12:09:47.670631: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0900 of size 256 next 6&#xA;2021-11-29 12:09:47.670739: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0a00 of size 256 next 9&#xA;2021-11-29 12:09:47.670827: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0b00 of size 256 next 10&#xA;2021-11-29 12:09:47.670910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0c00 of size 256 next 11&#xA;2021-11-29 12:09:47.670992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0d00 of size 256 next 14&#xA;2021-11-29 12:09:47.671075: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0e00 of size 256 next 15&#xA;2021-11-29 12:09:47.671186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac0f00 of size 256 next 16&#xA;2021-11-29 12:09:47.671293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1000 of size 256 next 7&#xA;2021-11-29 12:09:47.671395: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1100 of size 1024 next 8&#xA;2021-11-29 12:09:47.671511: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1500 of size 256 next 19&#xA;2021-11-29 12:09:47.671628: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1600 of size 256 next 20&#xA;2021-11-29 12:09:47.671738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1700 of size 256 next 23&#xA;2021-11-29 12:09:47.671900: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1800 of size 256 next 24&#xA;2021-11-29 12:09:47.672014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1900 of size 256 next 25&#xA;2021-11-29 12:09:47.672130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1a00 of size 256 next 26&#xA;2021-11-29 12:09:47.672231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1b00 of size 256 next 29&#xA;2021-11-29 12:09:47.672337: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1c00 of size 256 next 30&#xA;2021-11-29 12:09:47.672445: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1d00 of size 256 next 31&#xA;2021-11-29 12:09:47.672556: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1e00 of size 256 next 32&#xA;2021-11-29 12:09:47.672670: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac1f00 of size 256 next 27&#xA;2021-11-29 12:09:47.672771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2000 of size 768 next 28&#xA;2021-11-29 12:09:47.672875: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2300 of size 256 next 33&#xA;2021-11-29 12:09:47.672970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2400 of size 256 next 34&#xA;2021-11-29 12:09:47.673066: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2500 of size 256 next 35&#xA;2021-11-29 12:09:47.673177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2600 of size 1024 next 36&#xA;2021-11-29 12:09:47.673286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2a00 of size 256 next 37&#xA;2021-11-29 12:09:47.673399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2b00 of size 256 next 39&#xA;2021-11-29 12:09:47.673498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2c00 of size 256 next 40&#xA;2021-11-29 12:09:47.673600: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2d00 of size 256 next 42&#xA;2021-11-29 12:09:47.673697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac2e00 of size 768 next 43&#xA;2021-11-29 12:09:47.673796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3100 of size 256 next 44&#xA;2021-11-29 12:09:47.673899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3200 of size 1792 next 13&#xA;2021-11-29 12:09:47.674002: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac3900 of size 4608 next 12&#xA;2021-11-29 12:09:47.674102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac4b00 of size 4608 next 38&#xA;2021-11-29 12:09:47.674230: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ac5d00 of size 32256 next 18&#xA;2021-11-29 12:09:47.674351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00acdb00 of size 18432 next 17&#xA;2021-11-29 12:09:47.674461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b00ad2300 of size 40140800 next 41&#xA;2021-11-29 12:09:47.674565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a300 of size 256 next 45&#xA;2021-11-29 12:09:47.674671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311a400 of size 4608 next 46&#xA;2021-11-29 12:09:47.674779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b600 of size 256 next 47&#xA;2021-11-29 12:09:47.674890: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311b700 of size 18432 next 48&#xA;2021-11-29 12:09:47.674996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0311ff00 of size 256 next 49&#xA;2021-11-29 12:09:47.675096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120000 of size 256 next 51&#xA;2021-11-29 12:09:47.675216: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120100 of size 768 next 52&#xA;2021-11-29 12:09:47.675318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120400 of size 256 next 53&#xA;2021-11-29 12:09:47.675412: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120500 of size 256 next 54&#xA;2021-11-29 12:09:47.675512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120600 of size 256 next 55&#xA;2021-11-29 12:09:47.675613: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120700 of size 256 next 56&#xA;2021-11-29 12:09:47.675720: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120800 of size 256 next 57&#xA;2021-11-29 12:09:47.675819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120900 of size 256 next 58&#xA;2021-11-29 12:09:47.675910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120a00 of size 256 next 59&#xA;2021-11-29 12:09:47.676003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120b00 of size 256 next 60&#xA;2021-11-29 12:09:47.676097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120c00 of size 256 next 72&#xA;2021-11-29 12:09:47.676219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120d00 of size 256 next 76&#xA;2021-11-29 12:09:47.676333: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120e00 of size 256 next 64&#xA;2021-11-29 12:09:47.676446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03120f00 of size 256 next 80&#xA;2021-11-29 12:09:47.676552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121000 of size 512 next 61&#xA;2021-11-29 12:09:47.676663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121200 of size 256 next 83&#xA;2021-11-29 12:09:47.676779: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121300 of size 256 next 74&#xA;2021-11-29 12:09:47.676883: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03121400 of size 512 next 66&#xA;2021-11-29 12:09:47.676993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121600 of size 256 next 62&#xA;2021-11-29 12:09:47.677098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b03121700 of size 4608 next 63&#xA;2021-11-29 12:09:47.677215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b03122900 of size 40106496 next 22&#xA;2021-11-29 12:09:47.677326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b05762300 of size 40140800 next 21&#xA;2021-11-29 12:09:47.677438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b07daa300 of size 40140800 next 50&#xA;2021-11-29 12:09:47.677551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b0a3f2300 of size 127401984 next 68&#xA;2021-11-29 12:09:47.677668: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b11d72300 of size 127401984 next 75&#xA;2021-11-29 12:09:47.677765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b196f2300 of size 337383424 next 65&#xA;2021-11-29 12:09:47.677880: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b2d8b3300 of size 308004864 next 73&#xA;2021-11-29 12:09:47.677998: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b3fe6fb00 of size 256 next 79&#xA;2021-11-29 12:09:47.678105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at b3fe6fc00 of size 250694656 next 70&#xA;2021-11-29 12:09:47.678221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at b4ed84800 of size 158683392 next 18446744073709551615&#xA;2021-11-29 12:09:47.678337: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:&#xA;2021-11-29 12:09:47.678464: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 51 Chunks of size 256 totalling 12.8KiB&#xA;2021-11-29 12:09:47.678579: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 768 totalling 2.2KiB&#xA;2021-11-29 12:09:47.678682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1024 totalling 2.0KiB&#xA;2021-11-29 12:09:47.678784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB&#xA;2021-11-29 12:09:47.678903: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB&#xA;2021-11-29 12:09:47.679003: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4608 totalling 18.0KiB&#xA;2021-11-29 12:09:47.679110: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 18432 totalling 36.0KiB&#xA;2021-11-29 12:09:47.679223: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB&#xA;2021-11-29 12:09:47.679344: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 40140800 totalling 114.84MiB&#xA;2021-11-29 12:09:47.679508: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 127401984 totalling 243.00MiB&#xA;2021-11-29 12:09:47.679620: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 158683392 totalling 151.33MiB&#xA;2021-11-29 12:09:47.679732: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 337383424 totalling 321.75MiB&#xA;2021-11-29 12:09:47.679848: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 831.03MiB&#xA;2021-11-29 12:09:47.679969: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 1470208256 memory_limit_: 1470208411 available bytes: 155 curr_region_allocation_bytes_: 2940417024&#xA;2021-11-29 12:09:47.680128: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:&#xA;Limit:                      1470208411&#xA;InUse:                       871401216&#xA;MaxInUse:                   1470208000&#xA;NumAllocs:                         796&#xA;MaxAllocSize:                402397184&#xA;Reserved:                            0&#xA;PeakReserved:                        0&#xA;LargestFreeBlock:                    0&#xA;&#xA;2021-11-29 12:09:47.680737: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***__************xx*********************************____________________*________________******xxxxx &#xA;2021-11-29 12:09:47.680863: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at pooling_ops_common.cc:458 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;    modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;  File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;    history = model.fit(train_generator,&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 67, in error_handler&#xA;    raise e.with_traceback(filtered_tb) from None&#xA;  File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\tensorflow\python\eager\execute.py&amp;quot;, line 58, in quick_execute&#xA;    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,&#xA;tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[32,8,574,574] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc&#xA;         [[node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad&#xA; (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py:464)&#xA;]]&#xA;Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.&#xA; [Op:__inference_train_function_909]&#xA;&#xA;Errors may have originated from an input operation.&#xA;Input Source operations connected to node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad:&#xA;In[0] sequential/conv2d/Relu (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\backend.py:4867)&#xA;In[1] sequential/max_pooling2d/MaxPool (defined at C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\layers\pooling.py:357)&#xA;In[2] gradient_tape/sequential/conv2d_1/Conv2D/Conv2DBackpropInput:&#xA;&#xA;Operation defined at: (most recent call last)&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 83, in &amp;lt;module&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;     modelTrain(3, 'D:/dataset/training2/training/', 'D:/dataset/training2/validation/', &amp;quot;modelTest&amp;quot;, 0.0005, 30, 6, 192*mult, 192*mult, 3)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;d:\dataset\recognition.py&amp;quot;, line 72, in modelTrain&#xA;&amp;gt;&amp;gt;&amp;gt;     history = model.fit(train_generator,&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\utils\traceback_utils.py&amp;quot;, line 64, in error_handler&#xA;&amp;gt;&amp;gt;&amp;gt;     return fn(*args, **kwargs)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 1216, in fit&#xA;&amp;gt;&amp;gt;&amp;gt;     tmp_logs = self.train_function(iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 878, in train_function&#xA;&amp;gt;&amp;gt;&amp;gt;     return step_function(self, iterator)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 867, in step_function&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.distribute_strategy.run(run_step, args=(data,))&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 860, in run_step&#xA;&amp;gt;&amp;gt;&amp;gt;     outputs = model.train_step(data)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\engine\training.py&amp;quot;, line 816, in train_step&#xA;&amp;gt;&amp;gt;&amp;gt;     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 530, in minimize&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._compute_gradients(&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 583, in _compute_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt;   File &amp;quot;C:\Users\kamil\.conda\envs\tensorflow\lib\site-packages\keras\optimizer_v2\optimizer_v2.py&amp;quot;, line 464, in _get_gradients&#xA;&amp;gt;&amp;gt;&amp;gt;     grads = tape.gradient(loss, var_list, grad_loss)&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;2021-11-29 12:09:48.275244: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: FAILED_PRECONDITION: Python interpreter state is not initialized. The process may be terminated.&#xA;         [[{{node PyFunc}}]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; !&lt;/p&gt;&#xA;"" OwnerUserId=""448322"" LastEditorUserId=""448322"" LastEditDate=""2021-11-29T11:10:22.013"" LastActivityDate=""2021-11-29T11:10:22.013"" Title=""     "" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""5"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1365764"" PostTypeId=""1"" CreationDate=""2021-12-25T07:59:19.113"" Score=""1"" ViewCount=""224"" Body=""&lt;p&gt; ,  .         Pixellib and Tensorflow.&#xA;  ,   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import pixellib&#xA;from pixellib.instance import instance_segmentation&#xA;&#xA;segment_image = instance_segmentation()&#xA;segment_image.load_model(&amp;quot;mask_rcnn_coco.h5&amp;quot;)&#xA;segment_image.segmentImage(&amp;quot;self-driving-car.jpg&amp;quot;, show_bboxes=True, output_image_name=&amp;quot;new.jpg&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;   ,   .&#xA;&lt;a href=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    WARNING:tensorflow:From C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use fn_output_signature instead&#xA;2021-12-25 12:51:51.370102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found&#xA;2021-12-25 12:51:51.370277: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.&#xA;Skipping registering GPU devices...&#xA;2021-12-25 12:51:52.650356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.&#xA;  updates=self.state_updates,&#xA;Processed image saved successfully in your current working directory.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""467645"" LastEditorUserId=""467645"" LastEditDate=""2021-12-25T08:07:44.917"" LastActivityDate=""2021-12-25T08:07:44.917"" Title=""Pixellib and Tensorflow"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1365764"" PostTypeId=""1"" CreationDate=""2021-12-25T07:59:19.113"" Score=""1"" ViewCount=""224"" Body=""&lt;p&gt; ,  .         Pixellib and Tensorflow.&#xA;  ,   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import pixellib&#xA;from pixellib.instance import instance_segmentation&#xA;&#xA;segment_image = instance_segmentation()&#xA;segment_image.load_model(&amp;quot;mask_rcnn_coco.h5&amp;quot;)&#xA;segment_image.segmentImage(&amp;quot;self-driving-car.jpg&amp;quot;, show_bboxes=True, output_image_name=&amp;quot;new.jpg&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;   ,   .&#xA;&lt;a href=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    WARNING:tensorflow:From C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use fn_output_signature instead&#xA;2021-12-25 12:51:51.370102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found&#xA;2021-12-25 12:51:51.370277: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.&#xA;Skipping registering GPU devices...&#xA;2021-12-25 12:51:52.650356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.&#xA;  updates=self.state_updates,&#xA;Processed image saved successfully in your current working directory.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""467645"" LastEditorUserId=""467645"" LastEditDate=""2021-12-25T08:07:44.917"" LastActivityDate=""2021-12-25T08:07:44.917"" Title=""Pixellib and Tensorflow"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1365764"" PostTypeId=""1"" CreationDate=""2021-12-25T07:59:19.113"" Score=""1"" ViewCount=""224"" Body=""&lt;p&gt; ,  .         Pixellib and Tensorflow.&#xA;  ,   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import pixellib&#xA;from pixellib.instance import instance_segmentation&#xA;&#xA;segment_image = instance_segmentation()&#xA;segment_image.load_model(&amp;quot;mask_rcnn_coco.h5&amp;quot;)&#xA;segment_image.segmentImage(&amp;quot;self-driving-car.jpg&amp;quot;, show_bboxes=True, output_image_name=&amp;quot;new.jpg&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;   ,   .&#xA;&lt;a href=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    WARNING:tensorflow:From C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use fn_output_signature instead&#xA;2021-12-25 12:51:51.370102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found&#xA;2021-12-25 12:51:51.370277: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.&#xA;Skipping registering GPU devices...&#xA;2021-12-25 12:51:52.650356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.&#xA;  updates=self.state_updates,&#xA;Processed image saved successfully in your current working directory.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""467645"" LastEditorUserId=""467645"" LastEditDate=""2021-12-25T08:07:44.917"" LastActivityDate=""2021-12-25T08:07:44.917"" Title=""Pixellib and Tensorflow"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1365764"" PostTypeId=""1"" CreationDate=""2021-12-25T07:59:19.113"" Score=""1"" ViewCount=""224"" Body=""&lt;p&gt; ,  .         Pixellib and Tensorflow.&#xA;  ,   &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import pixellib&#xA;from pixellib.instance import instance_segmentation&#xA;&#xA;segment_image = instance_segmentation()&#xA;segment_image.load_model(&amp;quot;mask_rcnn_coco.h5&amp;quot;)&#xA;segment_image.segmentImage(&amp;quot;self-driving-car.jpg&amp;quot;, show_bboxes=True, output_image_name=&amp;quot;new.jpg&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;   ,   .&#xA;&lt;a href=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iYbBK.jpg&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    WARNING:tensorflow:From C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.&#xA;Instructions for updating:&#xA;Use fn_output_signature instead&#xA;2021-12-25 12:51:51.370102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found&#xA;2021-12-25 12:51:51.370277: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.&#xA;Skipping registering GPU devices...&#xA;2021-12-25 12:51:52.650356: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2&#xA;To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.&#xA;C:\Users\User\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.&#xA;  updates=self.state_updates,&#xA;Processed image saved successfully in your current working directory.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""467645"" LastEditorUserId=""467645"" LastEditDate=""2021-12-25T08:07:44.917"" LastActivityDate=""2021-12-25T08:07:44.917"" Title=""Pixellib and Tensorflow"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""2"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1392344"" PostTypeId=""1"" CreationDate=""2022-03-21T18:25:15.140"" Score=""0"" ViewCount=""291"" Body=""&lt;p&gt;     &lt;code&gt;ImageAI&lt;/code&gt;,  ,  &lt;code&gt;python 3.7.6&lt;/code&gt;,   &lt;code&gt;tensorflow 2.4.0&lt;/code&gt; pip ,    , ,  2.5.0,    ,     &lt;code&gt; from PIL import Image ModuleNotFoundError: No module named 'PIL'&lt;/code&gt;, Pillow &#xA;  ,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from imageai.Detection import ObjectDetection&#xA;import tensorflow as tf&#xA;import os&#xA;&#xA;session = tf.compat.v1.keras.backend.get_session()&#xA;&#xA;execution_path = os.getcwd()&#xA;&#xA;detector = ObjectDetection()&#xA;detector.setModelTypeAsYOLOv3()&#xA;detector.setModelPath( os.path.join(execution_path , &amp;quot;yolo.h5&amp;quot;))&#xA;detector.loadModel()&#xA;detections = detector.detectObjectsFromImage(&#xA;    input_image=os.path.join(execution_path , &amp;quot;object.jpg&amp;quot;),&#xA;    output_image_path=os.path.join(execution_path , &amp;quot;imagenew.jpg&amp;quot;),&#xA;    minimum_percentage_probability=30&#xA;)&#xA;&#xA;for eachObject in detections:&#xA;    print(eachObject[&amp;quot;name&amp;quot;] , &amp;quot; : &amp;quot;, eachObject[&amp;quot;percentage_probability&amp;quot;], &amp;quot; : &amp;quot;, eachObject[&amp;quot;box_points&amp;quot;] )&#xA;    print(&amp;quot;--------------------------------&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      - &lt;code&gt; pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0&lt;/code&gt;,    :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;    ERROR: Command errored out with exit status 1:&#xA;     command: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor&#xA;\AppData\Local\Temp\tmphtbvlvp9'&#xA;         cwd: C:\Users\Egor\AppData\Local\Temp\pip-install-m7a66eod\scipy_8b6bdb5b4a3c4b90a970a703e46181ff&#xA;    Complete output (195 lines):&#xA;    setup.py:418: UserWarning: Unrecognized setuptools command ('dist_info --egg-base C:\Users\Egor\AppData\Local\Temp\pip-modern-metadata-w3op0uy2'), proceeding with generating Cython so&#xA;urces and expanding templates&#xA;      warnings.warn(&amp;quot;Unrecognized setuptools command ('{}'), proceeding with &amp;quot;&#xA;    Running from scipy source directory.&#xA;    lapack_opt_info:&#xA;    lapack_mkl_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries mkl_rt not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;    get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'&#xA;    customize GnuFCompiler&#xA;    Could not locate executable g77&#xA;    Could not locate executable f77&#xA;    customize IntelVisualFCompiler&#xA;    Could not locate executable ifort&#xA;    Could not locate executable ifl&#xA;    customize AbsoftFCompiler&#xA;    Could not locate executable f90&#xA;    customize CompaqVisualFCompiler&#xA;    Could not locate executable DF&#xA;    customize IntelItaniumVisualFCompiler&#xA;    Could not locate executable efl&#xA;    customize Gnu95FCompiler&#xA;    Could not locate executable gfortran&#xA;    Could not locate executable f95&#xA;    customize G95FCompiler&#xA;    Could not locate executable g95&#xA;    customize IntelEM64VisualFCompiler&#xA;    customize IntelEM64TFCompiler&#xA;    Could not locate executable efort&#xA;    Could not locate executable efc&#xA;    customize PGroupFlangCompiler&#xA;    Could not locate executable flang&#xA;    don't know how to compile Fortran code on platform 'nt'&#xA;      NOT AVAILABLE&#xA;&#xA;    openblas_clapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries openblas,lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    flame_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries flame not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries tatlas,tatlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_3_10_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries satlas,satlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_3_10_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_threads_info:&#xA;    Setting PTATLAS=ATLAS&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries ptf77blas,ptcblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_threads_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    atlas_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\lib&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack_atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries f77blas,cblas,atlas not found in C:\Users\Egor\anaconda3\libs&#xA;    &amp;lt;class 'numpy.distutils.system_info.atlas_info'&amp;gt;&#xA;      NOT AVAILABLE&#xA;&#xA;    accelerate_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    lapack_info:&#xA;    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils&#xA;    customize MSVCCompiler&#xA;      libraries lapack not found in ['C:\\Users\\Egor\\anaconda3\\lib', 'C:\\', 'C:\\Users\\Egor\\anaconda3\\libs']&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) libraries not found.&#xA;        Directories to search for the libraries can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack]) or by setting&#xA;        the LAPACK environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;    lapack_src_info:&#xA;      NOT AVAILABLE&#xA;&#xA;    C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\system_info.py:1712: UserWarning:&#xA;        Lapack (http://www.netlib.org/lapack/) sources not found.&#xA;        Directories to search for the sources can be specified in the&#xA;        numpy/distutils/site.cfg file (section [lapack_src]) or by setting&#xA;        the LAPACK_SRC environment variable.&#xA;      if getattr(self, '_calc_info_{}'.format(lapack))():&#xA;      NOT AVAILABLE&#xA;&#xA;    Traceback (most recent call last):&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 349, in &amp;lt;module&amp;gt;&#xA;        main()&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 331, in main&#xA;        json_out['return_val'] = hook(**hook_input['kwargs'])&#xA;      File &amp;quot;C:\Users\Egor\anaconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&amp;quot;, line 151, in prepare_metadata_for_build_wheel&#xA;        return hook(metadata_directory, config_settings)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 188, in prepare_metadata_for_build_wheel&#xA;        self.run_setup()&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 281, in run_setup&#xA;        super(_BuildMetaLegacyBackend,&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\setuptools\build_meta.py&amp;quot;, line 174, in run_setup&#xA;        exec(compile(code, __file__, 'exec'), locals())&#xA;      File &amp;quot;setup.py&amp;quot;, line 540, in &amp;lt;module&amp;gt;&#xA;        setup_package()&#xA;      File &amp;quot;setup.py&amp;quot;, line 536, in setup_package&#xA;        setup(**metadata)&#xA;      File &amp;quot;C:\Users\Egor\AppData\Local\Temp\pip-build-env-lgaj2xno\overlay\Lib\site-packages\numpy\distutils\core.py&amp;quot;, line 137, in setup&#xA;        config = configuration()&#xA;      File &amp;quot;setup.py&amp;quot;, line 435, in configuration&#xA;        raise NotFoundError(msg)&#xA;    numpy.distutils.system_info.NotFoundError: No lapack/blas resources found.&#xA;    ----------------------------------------&#xA;WARNING: Discarding https://files.pythonhosted.org/packages/04/ab/e2eb3e3f90b9363040a3d885ccc5c79fe20c5b8a3caa8fe3bf47ff653260/scipy-1.4.1.tar.gz#sha256=dee1bbf3a6c8f73b6b218cb28eed8dd133&#xA;47ea2f87d572ce19b289d6fd3fbc59 (from https://pypi.org/simple/scipy/) (requires-python:&amp;gt;=3.5). Command errored out with exit status 1: 'C:\Users\Egor\anaconda3\python.exe' 'C:\Users\Egor\a&#xA;naconda3\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py' prepare_metadata_for_build_wheel 'C:\Users\Egor\AppData\Local\Temp\tmphtbvlvp9' Check the logs for full command ou&#xA;tput.&#xA;ERROR: Could not find a version that satisfies the requirement scipy==1.4.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0&#xA;.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2,&#xA; 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1&#xA;.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0)&#xA;ERROR: No matching distribution found for scipy==1.4.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""483187"" LastEditorUserId=""483187"" LastEditDate=""2022-03-21T19:32:09.990"" LastActivityDate=""2022-03-21T19:32:09.990"" Title="" ImageAI"" Tags=""&lt;python&gt;&lt;tensorflow&gt;"" AnswerCount=""0"" CommentCount=""3"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1430856"" PostTypeId=""1"" CreationDate=""2022-07-18T15:35:02.300"" Score=""0"" ViewCount=""133"" Body=""&lt;p&gt;        ,     .exe ,        torch&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;[7396] WARNING: file already exists but should not:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C.cp310-win_amd64.pyd&#xA;[7396] WARNING: file already exists but should not: C:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  pyinstaller:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;D:\coding\python\artix_assistant&amp;gt; pyinstaller -F  interface.py&#xA;375 INFO: PyInstaller: 5.2&#xA;375 INFO: Python: 3.10.5&#xA;383 INFO: Platform: Windows-10-10.0.22000-SP0&#xA;422 INFO: wrote D:\coding\python\artix_assistant\interface.spec&#xA;422 INFO: UPX is not available.&#xA;437 INFO: Extending PYTHONPATH with paths&#xA;['D:\\coding\\python\\artix_assistant']&#xA;1235 INFO: checking Analysis&#xA;1235 INFO: Building Analysis because Analysis-00.toc is non existent&#xA;1235 INFO: Initializing module dependency graph...&#xA;1235 INFO: Caching module graph hooks...&#xA;1250 WARNING: Several hooks defined for module 'numpy'. Please take care they do not conflict.&#xA;1266 INFO: Analyzing base_library.zip ...&#xA;7904 INFO: Processing pre-find module path hook distutils from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-distutils.py'.&#xA;7905 INFO: distutils: retargeting to non-venv dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib'&#xA;9263 INFO: Caching module dependency graph...&#xA;9453 INFO: running Analysis Analysis-00.toc&#xA;9475 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable&#xA;  required by C:\Users\artix\AppData\Local\Programs\Python\Python310\python.exe&#xA;9536 INFO: Analyzing D:\coding\python\artix_assistant\interface.py&#xA;9901 INFO: Processing pre-find module path hook site from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-site.py'.&#xA;9902 INFO: site: retargeting to fake-dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\fake-modules'&#xA;23781 INFO: Processing pre-safe import module hook gi from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-gi.py'.&#xA;27168 INFO: Processing pre-safe import module hook six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-six.moves.py'.&#xA;40396 INFO: Processing pre-safe import module hook urllib3.packages.six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-urllib3.packages.six.moves.py'.&#xA;42002 INFO: Processing module hooks...&#xA;42002 INFO: Loading module hook 'hook-certifi.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-pycparser.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-sounddevice.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-torch.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42737 INFO: Loading module hook 'hook-numpy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\numpy\\_pyinstaller'...&#xA;42862 INFO: Import to be excluded not found: 'f2py'&#xA;42893 INFO: Loading module hook 'hook-difflib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-encodings.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-heapq.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-lib2to3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43550 INFO: Loading module hook 'hook-matplotlib.backends.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43551 INFO: Matplotlib backend selection method: automatic discovery of used backends&#xA;43969 INFO: Trying determine the default backend as first importable candidate from the list: ['Qt5Agg', 'Gtk3Agg', 'TkAgg', 'WxAgg']&#xA;45876 INFO: Selected matplotlib backends: ['TkAgg']&#xA;45970 INFO: Loading module hook 'hook-matplotlib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-multiprocessing.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-numpy._pytesttester.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-packaging.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-pandas.io.formats.style.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...       &#xA;47830 INFO: Loading module hook 'hook-pandas.plotting.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;48096 INFO: Loading module hook 'hook-pandas.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49831 INFO: Loading module hook 'hook-pickle.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49847 INFO: Loading module hook 'hook-PIL.Image.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50550 INFO: Loading module hook 'hook-PIL.ImageFilter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50566 INFO: Loading module hook 'hook-PIL.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50581 INFO: Loading module hook 'hook-PIL.SpiderImagePlugin.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50597 INFO: Loading module hook 'hook-pkg_resources.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;125 WARNING: Failed to collect submodules for 'pkg_resources._vendor.pyparsing.diagram' because importing 'pkg_resources._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;51863 INFO: Processing pre-safe import module hook win32com from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\pre_safe_import_module\\hook-win32com.py'.&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.py2_warn&amp;quot; not found!&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.markers&amp;quot; not found!&#xA;52643 INFO: Loading module hook 'hook-platform.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52663 INFO: Loading module hook 'hook-PyQt6.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52878 INFO: Loading module hook 'hook-PyQt6.QtCore.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53050 INFO: Loading module hook 'hook-PyQt6.QtGui.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53284 INFO: Loading module hook 'hook-PyQt6.QtWidgets.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53534 INFO: Loading module hook 'hook-pytz.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53769 INFO: Loading module hook 'hook-scipy.io.matlab.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.linalg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.sparse.csgraph.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.spatial.transform.rotation.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ellip_harm_2.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ufuncs.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-scipy.stats._stats.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-setuptools.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;312 WARNING: Failed to collect submodules for 'setuptools._vendor.pyparsing.diagram' because importing 'setuptools._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;55879 INFO: Loading module hook 'hook-sqlite3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-sysconfig.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-win32ctypes.core.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.dom.domreg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.etree.cElementTree.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-_tkinter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57177 INFO: checking Tree&#xA;57177 INFO: Building Tree because Tree-00.toc is non existent&#xA;57177 INFO: Building Tree Tree-00.toc&#xA;57271 INFO: checking Tree&#xA;57271 INFO: Building Tree because Tree-01.toc is non existent&#xA;57271 INFO: Building Tree Tree-01.toc&#xA;57365 INFO: checking Tree&#xA;57365 INFO: Building Tree because Tree-02.toc is non existent&#xA;57365 INFO: Building Tree Tree-02.toc&#xA;57521 INFO: Loading module hook 'hook-setuptools.msvc.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57740 INFO: Looking for ctypes DLLs&#xA;57865 INFO: Analyzing run-time hooks ...&#xA;57880 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_subprocess.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_inspect.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgutil.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_multiprocessing.py'        &#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgres.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth__tkinter.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_mplconfig.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pyqt6.py'&#xA;57959 INFO: Looking for dynamic libraries&#xA;C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\building\build_main.py:156: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.&#xA;  __import__(package)&#xA;4174 WARNING: lib not found: api-ms-win-shcore-scaling-l1-1-1.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyQt6\Qt6\plugins\platforms\qwindows.dll&#xA;4252 INFO: Cannot get manifest resource from non-PE file C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING: Cannot get binary dependencies for file: C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING:   Reason: 'DOS Header magic not found.'&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C.cp310-win_amd64.pyd&#xA;64635 INFO: Looking for eggs&#xA;64635 INFO: Using Python library C:\Users\artix\AppData\Local\Programs\Python\Python310\python310.dll&#xA;64635 INFO: Found binding redirects:&#xA;[]&#xA;64666 INFO: Warnings written to D:\coding\python\artix_assistant\build\interface\warn-interface.txt&#xA;65057 INFO: Graph cross-reference written to D:\coding\python\artix_assistant\build\interface\xref-interface.html&#xA;65666 INFO: checking PYZ&#xA;65666 INFO: Building PYZ because PYZ-00.toc is non existent&#xA;65666 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz&#xA;68824 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz completed successfully.&#xA;68933 INFO: checking PKG&#xA;68933 INFO: Building PKG because PKG-00.toc is non existent&#xA;68933 INFO: Building PKG (CArchive) interface.pkg&#xA;200906 INFO: Building PKG (CArchive) interface.pkg completed successfully.&#xA;201328 INFO: Bootloader C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe&#xA;201328 INFO: checking EXE&#xA;201328 INFO: Building EXE because EXE-00.toc is non existent&#xA;201328 INFO: Building EXE from EXE-00.toc&#xA;201328 INFO: Copying bootloader EXE to D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201406 INFO: Copying icon to EXE&#xA;201406 INFO: Copying icons from ['C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\bootloader\\images\\icon-console.ico']&#xA;201469 INFO: Writing RT_GROUP_ICON 0 resource with 104 bytes&#xA;201469 INFO: Writing RT_ICON 1 resource with 3752 bytes&#xA;201469 INFO: Writing RT_ICON 2 resource with 2216 bytes&#xA;201469 INFO: Writing RT_ICON 3 resource with 1384 bytes&#xA;201469 INFO: Writing RT_ICON 4 resource with 37019 bytes&#xA;201469 INFO: Writing RT_ICON 5 resource with 9640 bytes&#xA;201469 INFO: Writing RT_ICON 6 resource with 4264 bytes&#xA;201469 INFO: Writing RT_ICON 7 resource with 1128 bytes&#xA;201484 INFO: Copying 0 resources to EXE&#xA;201484 INFO: Embedding manifest in EXE&#xA;201484 INFO: Updating manifest in D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201547 INFO: Updating resource type 24 name 1 language 0&#xA;201562 INFO: Appending PKG archive to EXE&#xA;201797 INFO: Fixing EXE headers&#xA;206831 INFO: Building EXE from EXE-00.toc completed successfully.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      pyinstaller     -    ?&lt;/p&gt;&#xA;"" OwnerUserId=""452211"" LastActivityDate=""2022-07-18T15:35:02.300"" Title="" .exe    .py   Pyinstaller"" Tags=""&lt;python&gt;&lt;pyinstaller&gt;&lt;pytorch&gt;&lt;pyqt6&gt;&lt;vosk&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1430856"" PostTypeId=""1"" CreationDate=""2022-07-18T15:35:02.300"" Score=""0"" ViewCount=""133"" Body=""&lt;p&gt;        ,     .exe ,        torch&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;[7396] WARNING: file already exists but should not:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C.cp310-win_amd64.pyd&#xA;[7396] WARNING: file already exists but should not: C:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  pyinstaller:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;D:\coding\python\artix_assistant&amp;gt; pyinstaller -F  interface.py&#xA;375 INFO: PyInstaller: 5.2&#xA;375 INFO: Python: 3.10.5&#xA;383 INFO: Platform: Windows-10-10.0.22000-SP0&#xA;422 INFO: wrote D:\coding\python\artix_assistant\interface.spec&#xA;422 INFO: UPX is not available.&#xA;437 INFO: Extending PYTHONPATH with paths&#xA;['D:\\coding\\python\\artix_assistant']&#xA;1235 INFO: checking Analysis&#xA;1235 INFO: Building Analysis because Analysis-00.toc is non existent&#xA;1235 INFO: Initializing module dependency graph...&#xA;1235 INFO: Caching module graph hooks...&#xA;1250 WARNING: Several hooks defined for module 'numpy'. Please take care they do not conflict.&#xA;1266 INFO: Analyzing base_library.zip ...&#xA;7904 INFO: Processing pre-find module path hook distutils from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-distutils.py'.&#xA;7905 INFO: distutils: retargeting to non-venv dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib'&#xA;9263 INFO: Caching module dependency graph...&#xA;9453 INFO: running Analysis Analysis-00.toc&#xA;9475 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable&#xA;  required by C:\Users\artix\AppData\Local\Programs\Python\Python310\python.exe&#xA;9536 INFO: Analyzing D:\coding\python\artix_assistant\interface.py&#xA;9901 INFO: Processing pre-find module path hook site from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-site.py'.&#xA;9902 INFO: site: retargeting to fake-dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\fake-modules'&#xA;23781 INFO: Processing pre-safe import module hook gi from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-gi.py'.&#xA;27168 INFO: Processing pre-safe import module hook six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-six.moves.py'.&#xA;40396 INFO: Processing pre-safe import module hook urllib3.packages.six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-urllib3.packages.six.moves.py'.&#xA;42002 INFO: Processing module hooks...&#xA;42002 INFO: Loading module hook 'hook-certifi.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-pycparser.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-sounddevice.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-torch.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42737 INFO: Loading module hook 'hook-numpy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\numpy\\_pyinstaller'...&#xA;42862 INFO: Import to be excluded not found: 'f2py'&#xA;42893 INFO: Loading module hook 'hook-difflib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-encodings.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-heapq.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-lib2to3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43550 INFO: Loading module hook 'hook-matplotlib.backends.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43551 INFO: Matplotlib backend selection method: automatic discovery of used backends&#xA;43969 INFO: Trying determine the default backend as first importable candidate from the list: ['Qt5Agg', 'Gtk3Agg', 'TkAgg', 'WxAgg']&#xA;45876 INFO: Selected matplotlib backends: ['TkAgg']&#xA;45970 INFO: Loading module hook 'hook-matplotlib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-multiprocessing.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-numpy._pytesttester.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-packaging.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-pandas.io.formats.style.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...       &#xA;47830 INFO: Loading module hook 'hook-pandas.plotting.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;48096 INFO: Loading module hook 'hook-pandas.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49831 INFO: Loading module hook 'hook-pickle.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49847 INFO: Loading module hook 'hook-PIL.Image.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50550 INFO: Loading module hook 'hook-PIL.ImageFilter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50566 INFO: Loading module hook 'hook-PIL.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50581 INFO: Loading module hook 'hook-PIL.SpiderImagePlugin.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50597 INFO: Loading module hook 'hook-pkg_resources.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;125 WARNING: Failed to collect submodules for 'pkg_resources._vendor.pyparsing.diagram' because importing 'pkg_resources._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;51863 INFO: Processing pre-safe import module hook win32com from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\pre_safe_import_module\\hook-win32com.py'.&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.py2_warn&amp;quot; not found!&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.markers&amp;quot; not found!&#xA;52643 INFO: Loading module hook 'hook-platform.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52663 INFO: Loading module hook 'hook-PyQt6.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52878 INFO: Loading module hook 'hook-PyQt6.QtCore.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53050 INFO: Loading module hook 'hook-PyQt6.QtGui.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53284 INFO: Loading module hook 'hook-PyQt6.QtWidgets.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53534 INFO: Loading module hook 'hook-pytz.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53769 INFO: Loading module hook 'hook-scipy.io.matlab.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.linalg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.sparse.csgraph.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.spatial.transform.rotation.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ellip_harm_2.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ufuncs.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-scipy.stats._stats.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-setuptools.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;312 WARNING: Failed to collect submodules for 'setuptools._vendor.pyparsing.diagram' because importing 'setuptools._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;55879 INFO: Loading module hook 'hook-sqlite3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-sysconfig.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-win32ctypes.core.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.dom.domreg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.etree.cElementTree.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-_tkinter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57177 INFO: checking Tree&#xA;57177 INFO: Building Tree because Tree-00.toc is non existent&#xA;57177 INFO: Building Tree Tree-00.toc&#xA;57271 INFO: checking Tree&#xA;57271 INFO: Building Tree because Tree-01.toc is non existent&#xA;57271 INFO: Building Tree Tree-01.toc&#xA;57365 INFO: checking Tree&#xA;57365 INFO: Building Tree because Tree-02.toc is non existent&#xA;57365 INFO: Building Tree Tree-02.toc&#xA;57521 INFO: Loading module hook 'hook-setuptools.msvc.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57740 INFO: Looking for ctypes DLLs&#xA;57865 INFO: Analyzing run-time hooks ...&#xA;57880 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_subprocess.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_inspect.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgutil.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_multiprocessing.py'        &#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgres.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth__tkinter.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_mplconfig.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pyqt6.py'&#xA;57959 INFO: Looking for dynamic libraries&#xA;C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\building\build_main.py:156: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.&#xA;  __import__(package)&#xA;4174 WARNING: lib not found: api-ms-win-shcore-scaling-l1-1-1.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyQt6\Qt6\plugins\platforms\qwindows.dll&#xA;4252 INFO: Cannot get manifest resource from non-PE file C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING: Cannot get binary dependencies for file: C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING:   Reason: 'DOS Header magic not found.'&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C.cp310-win_amd64.pyd&#xA;64635 INFO: Looking for eggs&#xA;64635 INFO: Using Python library C:\Users\artix\AppData\Local\Programs\Python\Python310\python310.dll&#xA;64635 INFO: Found binding redirects:&#xA;[]&#xA;64666 INFO: Warnings written to D:\coding\python\artix_assistant\build\interface\warn-interface.txt&#xA;65057 INFO: Graph cross-reference written to D:\coding\python\artix_assistant\build\interface\xref-interface.html&#xA;65666 INFO: checking PYZ&#xA;65666 INFO: Building PYZ because PYZ-00.toc is non existent&#xA;65666 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz&#xA;68824 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz completed successfully.&#xA;68933 INFO: checking PKG&#xA;68933 INFO: Building PKG because PKG-00.toc is non existent&#xA;68933 INFO: Building PKG (CArchive) interface.pkg&#xA;200906 INFO: Building PKG (CArchive) interface.pkg completed successfully.&#xA;201328 INFO: Bootloader C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe&#xA;201328 INFO: checking EXE&#xA;201328 INFO: Building EXE because EXE-00.toc is non existent&#xA;201328 INFO: Building EXE from EXE-00.toc&#xA;201328 INFO: Copying bootloader EXE to D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201406 INFO: Copying icon to EXE&#xA;201406 INFO: Copying icons from ['C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\bootloader\\images\\icon-console.ico']&#xA;201469 INFO: Writing RT_GROUP_ICON 0 resource with 104 bytes&#xA;201469 INFO: Writing RT_ICON 1 resource with 3752 bytes&#xA;201469 INFO: Writing RT_ICON 2 resource with 2216 bytes&#xA;201469 INFO: Writing RT_ICON 3 resource with 1384 bytes&#xA;201469 INFO: Writing RT_ICON 4 resource with 37019 bytes&#xA;201469 INFO: Writing RT_ICON 5 resource with 9640 bytes&#xA;201469 INFO: Writing RT_ICON 6 resource with 4264 bytes&#xA;201469 INFO: Writing RT_ICON 7 resource with 1128 bytes&#xA;201484 INFO: Copying 0 resources to EXE&#xA;201484 INFO: Embedding manifest in EXE&#xA;201484 INFO: Updating manifest in D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201547 INFO: Updating resource type 24 name 1 language 0&#xA;201562 INFO: Appending PKG archive to EXE&#xA;201797 INFO: Fixing EXE headers&#xA;206831 INFO: Building EXE from EXE-00.toc completed successfully.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      pyinstaller     -    ?&lt;/p&gt;&#xA;"" OwnerUserId=""452211"" LastActivityDate=""2022-07-18T15:35:02.300"" Title="" .exe    .py   Pyinstaller"" Tags=""&lt;python&gt;&lt;pyinstaller&gt;&lt;pytorch&gt;&lt;pyqt6&gt;&lt;vosk&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1430856"" PostTypeId=""1"" CreationDate=""2022-07-18T15:35:02.300"" Score=""0"" ViewCount=""133"" Body=""&lt;p&gt;        ,     .exe ,        torch&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;[7396] WARNING: file already exists but should not:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C.cp310-win_amd64.pyd&#xA;[7396] WARNING: file already exists but should not: C:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  pyinstaller:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;D:\coding\python\artix_assistant&amp;gt; pyinstaller -F  interface.py&#xA;375 INFO: PyInstaller: 5.2&#xA;375 INFO: Python: 3.10.5&#xA;383 INFO: Platform: Windows-10-10.0.22000-SP0&#xA;422 INFO: wrote D:\coding\python\artix_assistant\interface.spec&#xA;422 INFO: UPX is not available.&#xA;437 INFO: Extending PYTHONPATH with paths&#xA;['D:\\coding\\python\\artix_assistant']&#xA;1235 INFO: checking Analysis&#xA;1235 INFO: Building Analysis because Analysis-00.toc is non existent&#xA;1235 INFO: Initializing module dependency graph...&#xA;1235 INFO: Caching module graph hooks...&#xA;1250 WARNING: Several hooks defined for module 'numpy'. Please take care they do not conflict.&#xA;1266 INFO: Analyzing base_library.zip ...&#xA;7904 INFO: Processing pre-find module path hook distutils from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-distutils.py'.&#xA;7905 INFO: distutils: retargeting to non-venv dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib'&#xA;9263 INFO: Caching module dependency graph...&#xA;9453 INFO: running Analysis Analysis-00.toc&#xA;9475 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable&#xA;  required by C:\Users\artix\AppData\Local\Programs\Python\Python310\python.exe&#xA;9536 INFO: Analyzing D:\coding\python\artix_assistant\interface.py&#xA;9901 INFO: Processing pre-find module path hook site from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-site.py'.&#xA;9902 INFO: site: retargeting to fake-dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\fake-modules'&#xA;23781 INFO: Processing pre-safe import module hook gi from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-gi.py'.&#xA;27168 INFO: Processing pre-safe import module hook six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-six.moves.py'.&#xA;40396 INFO: Processing pre-safe import module hook urllib3.packages.six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-urllib3.packages.six.moves.py'.&#xA;42002 INFO: Processing module hooks...&#xA;42002 INFO: Loading module hook 'hook-certifi.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-pycparser.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-sounddevice.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-torch.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42737 INFO: Loading module hook 'hook-numpy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\numpy\\_pyinstaller'...&#xA;42862 INFO: Import to be excluded not found: 'f2py'&#xA;42893 INFO: Loading module hook 'hook-difflib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-encodings.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-heapq.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-lib2to3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43550 INFO: Loading module hook 'hook-matplotlib.backends.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43551 INFO: Matplotlib backend selection method: automatic discovery of used backends&#xA;43969 INFO: Trying determine the default backend as first importable candidate from the list: ['Qt5Agg', 'Gtk3Agg', 'TkAgg', 'WxAgg']&#xA;45876 INFO: Selected matplotlib backends: ['TkAgg']&#xA;45970 INFO: Loading module hook 'hook-matplotlib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-multiprocessing.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-numpy._pytesttester.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-packaging.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-pandas.io.formats.style.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...       &#xA;47830 INFO: Loading module hook 'hook-pandas.plotting.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;48096 INFO: Loading module hook 'hook-pandas.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49831 INFO: Loading module hook 'hook-pickle.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49847 INFO: Loading module hook 'hook-PIL.Image.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50550 INFO: Loading module hook 'hook-PIL.ImageFilter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50566 INFO: Loading module hook 'hook-PIL.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50581 INFO: Loading module hook 'hook-PIL.SpiderImagePlugin.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50597 INFO: Loading module hook 'hook-pkg_resources.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;125 WARNING: Failed to collect submodules for 'pkg_resources._vendor.pyparsing.diagram' because importing 'pkg_resources._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;51863 INFO: Processing pre-safe import module hook win32com from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\pre_safe_import_module\\hook-win32com.py'.&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.py2_warn&amp;quot; not found!&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.markers&amp;quot; not found!&#xA;52643 INFO: Loading module hook 'hook-platform.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52663 INFO: Loading module hook 'hook-PyQt6.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52878 INFO: Loading module hook 'hook-PyQt6.QtCore.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53050 INFO: Loading module hook 'hook-PyQt6.QtGui.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53284 INFO: Loading module hook 'hook-PyQt6.QtWidgets.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53534 INFO: Loading module hook 'hook-pytz.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53769 INFO: Loading module hook 'hook-scipy.io.matlab.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.linalg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.sparse.csgraph.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.spatial.transform.rotation.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ellip_harm_2.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ufuncs.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-scipy.stats._stats.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-setuptools.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;312 WARNING: Failed to collect submodules for 'setuptools._vendor.pyparsing.diagram' because importing 'setuptools._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;55879 INFO: Loading module hook 'hook-sqlite3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-sysconfig.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-win32ctypes.core.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.dom.domreg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.etree.cElementTree.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-_tkinter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57177 INFO: checking Tree&#xA;57177 INFO: Building Tree because Tree-00.toc is non existent&#xA;57177 INFO: Building Tree Tree-00.toc&#xA;57271 INFO: checking Tree&#xA;57271 INFO: Building Tree because Tree-01.toc is non existent&#xA;57271 INFO: Building Tree Tree-01.toc&#xA;57365 INFO: checking Tree&#xA;57365 INFO: Building Tree because Tree-02.toc is non existent&#xA;57365 INFO: Building Tree Tree-02.toc&#xA;57521 INFO: Loading module hook 'hook-setuptools.msvc.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57740 INFO: Looking for ctypes DLLs&#xA;57865 INFO: Analyzing run-time hooks ...&#xA;57880 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_subprocess.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_inspect.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgutil.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_multiprocessing.py'        &#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgres.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth__tkinter.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_mplconfig.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pyqt6.py'&#xA;57959 INFO: Looking for dynamic libraries&#xA;C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\building\build_main.py:156: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.&#xA;  __import__(package)&#xA;4174 WARNING: lib not found: api-ms-win-shcore-scaling-l1-1-1.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyQt6\Qt6\plugins\platforms\qwindows.dll&#xA;4252 INFO: Cannot get manifest resource from non-PE file C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING: Cannot get binary dependencies for file: C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING:   Reason: 'DOS Header magic not found.'&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C.cp310-win_amd64.pyd&#xA;64635 INFO: Looking for eggs&#xA;64635 INFO: Using Python library C:\Users\artix\AppData\Local\Programs\Python\Python310\python310.dll&#xA;64635 INFO: Found binding redirects:&#xA;[]&#xA;64666 INFO: Warnings written to D:\coding\python\artix_assistant\build\interface\warn-interface.txt&#xA;65057 INFO: Graph cross-reference written to D:\coding\python\artix_assistant\build\interface\xref-interface.html&#xA;65666 INFO: checking PYZ&#xA;65666 INFO: Building PYZ because PYZ-00.toc is non existent&#xA;65666 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz&#xA;68824 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz completed successfully.&#xA;68933 INFO: checking PKG&#xA;68933 INFO: Building PKG because PKG-00.toc is non existent&#xA;68933 INFO: Building PKG (CArchive) interface.pkg&#xA;200906 INFO: Building PKG (CArchive) interface.pkg completed successfully.&#xA;201328 INFO: Bootloader C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe&#xA;201328 INFO: checking EXE&#xA;201328 INFO: Building EXE because EXE-00.toc is non existent&#xA;201328 INFO: Building EXE from EXE-00.toc&#xA;201328 INFO: Copying bootloader EXE to D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201406 INFO: Copying icon to EXE&#xA;201406 INFO: Copying icons from ['C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\bootloader\\images\\icon-console.ico']&#xA;201469 INFO: Writing RT_GROUP_ICON 0 resource with 104 bytes&#xA;201469 INFO: Writing RT_ICON 1 resource with 3752 bytes&#xA;201469 INFO: Writing RT_ICON 2 resource with 2216 bytes&#xA;201469 INFO: Writing RT_ICON 3 resource with 1384 bytes&#xA;201469 INFO: Writing RT_ICON 4 resource with 37019 bytes&#xA;201469 INFO: Writing RT_ICON 5 resource with 9640 bytes&#xA;201469 INFO: Writing RT_ICON 6 resource with 4264 bytes&#xA;201469 INFO: Writing RT_ICON 7 resource with 1128 bytes&#xA;201484 INFO: Copying 0 resources to EXE&#xA;201484 INFO: Embedding manifest in EXE&#xA;201484 INFO: Updating manifest in D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201547 INFO: Updating resource type 24 name 1 language 0&#xA;201562 INFO: Appending PKG archive to EXE&#xA;201797 INFO: Fixing EXE headers&#xA;206831 INFO: Building EXE from EXE-00.toc completed successfully.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      pyinstaller     -    ?&lt;/p&gt;&#xA;"" OwnerUserId=""452211"" LastActivityDate=""2022-07-18T15:35:02.300"" Title="" .exe    .py   Pyinstaller"" Tags=""&lt;python&gt;&lt;pyinstaller&gt;&lt;pytorch&gt;&lt;pyqt6&gt;&lt;vosk&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1453876"" PostTypeId=""1"" AcceptedAnswerId=""1454186"" CreationDate=""2022-10-03T20:07:28.383"" Score=""1"" ViewCount=""72"" Body=""&lt;p&gt;    ,    &amp;quot;&amp;quot;    18   80,    &lt;code&gt;QPushButton&lt;/code&gt; (  )   &lt;code&gt;QRadioButton&lt;/code&gt; (    ).&lt;/p&gt;&#xA;&lt;p&gt;  -  ?      &lt;code&gt;LineEdit&lt;/code&gt;,   .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from PySide6 import QtWidgets, QtCore, QtGui&#xA;&#xA;class QTApp(QtWidgets.QWidget):&#xA;    def __init__(self):&#xA;        super(QTApp, self).__init__()&#xA;&#xA;        self.LE_sample_input_01 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_02 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_01.setPlaceholderText('')&#xA;        self.RadioButton = QtWidgets.QRadioButton('-')&#xA;        self.Button = QtWidgets.QPushButton('')&#xA;        self.Button.setStyleSheet(&amp;quot;QPushButton:pressed {background-color: #b3b3ba;}&amp;quot;)&#xA;&#xA;        layout = QtWidgets.QVBoxLayout(self)&#xA;        layout.addWidget(self.LE_sample_input_01)&#xA;        layout.addWidget(self.LE_sample_input_02)&#xA;        layout.addWidget(self.RadioButton)&#xA;        layout.addWidget(self.Button)&#xA;&#xA;        self.LE_sample_input_01.installEventFilter(self)&#xA;&#xA;    def eventFilter(self, obj, event):&#xA;        if event.type() == QtCore.QEvent.FocusOut and QtGui.QFocusEvent.reason(event) == QtCore.Qt.MouseFocusReason:&#xA;            if obj is self.LE_sample_input_01:&#xA;                try:&#xA;                    age = int(self.LE_sample_input_01.text())&#xA;                    if age &amp;lt; 18 or age &amp;gt; 80:&#xA;                        error = QtWidgets.QMessageBox()&#xA;                        error.setWindowTitle(' ')&#xA;                        error.setText('   ( 18  80 )')&#xA;                        error.setIcon(QtWidgets.QMessageBox.Warning)&#xA;                        error.addButton('',QtWidgets.QMessageBox.AcceptRole)&#xA;                        error.exec()&#xA;                        obj.setFocus()&#xA;                        obj.selectAll()&#xA;                        return True&#xA;                except: pass&#xA;        return False&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    app = QtWidgets.QApplication()&#xA;    qt_app = QTApp()&#xA;    qt_app.show()&#xA;    app.exec()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""466264"" LastEditorUserId=""217323"" LastEditDate=""2022-10-05T17:11:57.173"" LastActivityDate=""2022-10-05T17:11:57.173"" Title=""eventFilter  QLineEdit"" Tags=""&lt;python&gt;&lt;&gt;&lt;qlineedit&gt;&lt;pyside6&gt;&lt;eventfilter&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1430856"" PostTypeId=""1"" CreationDate=""2022-07-18T15:35:02.300"" Score=""0"" ViewCount=""133"" Body=""&lt;p&gt;        ,     .exe ,        torch&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;[7396] WARNING: file already exists but should not:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C.cp310-win_amd64.pyd&#xA;[7396] WARNING: file already exists but should not: C:\Users\artix\AppData\Local\Temp\_MEI73962\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  pyinstaller:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;D:\coding\python\artix_assistant&amp;gt; pyinstaller -F  interface.py&#xA;375 INFO: PyInstaller: 5.2&#xA;375 INFO: Python: 3.10.5&#xA;383 INFO: Platform: Windows-10-10.0.22000-SP0&#xA;422 INFO: wrote D:\coding\python\artix_assistant\interface.spec&#xA;422 INFO: UPX is not available.&#xA;437 INFO: Extending PYTHONPATH with paths&#xA;['D:\\coding\\python\\artix_assistant']&#xA;1235 INFO: checking Analysis&#xA;1235 INFO: Building Analysis because Analysis-00.toc is non existent&#xA;1235 INFO: Initializing module dependency graph...&#xA;1235 INFO: Caching module graph hooks...&#xA;1250 WARNING: Several hooks defined for module 'numpy'. Please take care they do not conflict.&#xA;1266 INFO: Analyzing base_library.zip ...&#xA;7904 INFO: Processing pre-find module path hook distutils from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-distutils.py'.&#xA;7905 INFO: distutils: retargeting to non-venv dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib'&#xA;9263 INFO: Caching module dependency graph...&#xA;9453 INFO: running Analysis Analysis-00.toc&#xA;9475 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable&#xA;  required by C:\Users\artix\AppData\Local\Programs\Python\Python310\python.exe&#xA;9536 INFO: Analyzing D:\coding\python\artix_assistant\interface.py&#xA;9901 INFO: Processing pre-find module path hook site from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_find_module_path\\hook-site.py'.&#xA;9902 INFO: site: retargeting to fake-dir 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\fake-modules'&#xA;23781 INFO: Processing pre-safe import module hook gi from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-gi.py'.&#xA;27168 INFO: Processing pre-safe import module hook six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-six.moves.py'.&#xA;40396 INFO: Processing pre-safe import module hook urllib3.packages.six.moves from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\pre_safe_import_module\\hook-urllib3.packages.six.moves.py'.&#xA;42002 INFO: Processing module hooks...&#xA;42002 INFO: Loading module hook 'hook-certifi.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-pycparser.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-sounddevice.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42002 INFO: Loading module hook 'hook-torch.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\stdhooks'...&#xA;42737 INFO: Loading module hook 'hook-numpy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\numpy\\_pyinstaller'...&#xA;42862 INFO: Import to be excluded not found: 'f2py'&#xA;42893 INFO: Loading module hook 'hook-difflib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-distutils.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;42909 INFO: Loading module hook 'hook-encodings.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-heapq.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43488 INFO: Loading module hook 'hook-lib2to3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43550 INFO: Loading module hook 'hook-matplotlib.backends.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;43551 INFO: Matplotlib backend selection method: automatic discovery of used backends&#xA;43969 INFO: Trying determine the default backend as first importable candidate from the list: ['Qt5Agg', 'Gtk3Agg', 'TkAgg', 'WxAgg']&#xA;45876 INFO: Selected matplotlib backends: ['TkAgg']&#xA;45970 INFO: Loading module hook 'hook-matplotlib.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-multiprocessing.util.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-numpy._pytesttester.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-packaging.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;46408 INFO: Loading module hook 'hook-pandas.io.formats.style.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...       &#xA;47830 INFO: Loading module hook 'hook-pandas.plotting.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;48096 INFO: Loading module hook 'hook-pandas.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49831 INFO: Loading module hook 'hook-pickle.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;49847 INFO: Loading module hook 'hook-PIL.Image.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50550 INFO: Loading module hook 'hook-PIL.ImageFilter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50566 INFO: Loading module hook 'hook-PIL.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50581 INFO: Loading module hook 'hook-PIL.SpiderImagePlugin.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;50597 INFO: Loading module hook 'hook-pkg_resources.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;125 WARNING: Failed to collect submodules for 'pkg_resources._vendor.pyparsing.diagram' because importing 'pkg_resources._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;51863 INFO: Processing pre-safe import module hook win32com from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_pyinstaller_hooks_contrib\\hooks\\pre_safe_import_module\\hook-win32com.py'.&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.py2_warn&amp;quot; not found!&#xA;52643 WARNING: Hidden import &amp;quot;pkg_resources.markers&amp;quot; not found!&#xA;52643 INFO: Loading module hook 'hook-platform.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52663 INFO: Loading module hook 'hook-PyQt6.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;52878 INFO: Loading module hook 'hook-PyQt6.QtCore.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53050 INFO: Loading module hook 'hook-PyQt6.QtGui.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53284 INFO: Loading module hook 'hook-PyQt6.QtWidgets.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53534 INFO: Loading module hook 'hook-pytz.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53769 INFO: Loading module hook 'hook-scipy.io.matlab.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.linalg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.sparse.csgraph.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53784 INFO: Loading module hook 'hook-scipy.spatial.transform.rotation.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ellip_harm_2.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53831 INFO: Loading module hook 'hook-scipy.special._ufuncs.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-scipy.stats._stats.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;53847 INFO: Loading module hook 'hook-setuptools.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;312 WARNING: Failed to collect submodules for 'setuptools._vendor.pyparsing.diagram' because importing 'setuptools._vendor.pyparsing.diagram' raised: ModuleNotFoundError: No module named 'railroad'&#xA;55879 INFO: Loading module hook 'hook-sqlite3.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-sysconfig.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;56349 INFO: Loading module hook 'hook-win32ctypes.core.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.dom.domreg.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.etree.cElementTree.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-xml.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57021 INFO: Loading module hook 'hook-_tkinter.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57177 INFO: checking Tree&#xA;57177 INFO: Building Tree because Tree-00.toc is non existent&#xA;57177 INFO: Building Tree Tree-00.toc&#xA;57271 INFO: checking Tree&#xA;57271 INFO: Building Tree because Tree-01.toc is non existent&#xA;57271 INFO: Building Tree Tree-01.toc&#xA;57365 INFO: checking Tree&#xA;57365 INFO: Building Tree because Tree-02.toc is non existent&#xA;57365 INFO: Building Tree Tree-02.toc&#xA;57521 INFO: Loading module hook 'hook-setuptools.msvc.py' from 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks'...&#xA;57740 INFO: Looking for ctypes DLLs&#xA;57865 INFO: Analyzing run-time hooks ...&#xA;57880 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_subprocess.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_inspect.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgutil.py'&#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_multiprocessing.py'        &#xA;57896 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pkgres.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth__tkinter.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_mplconfig.py'&#xA;57912 INFO: Including run-time hook 'C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\hooks\\rthooks\\pyi_rth_pyqt6.py'&#xA;57959 INFO: Looking for dynamic libraries&#xA;C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\building\build_main.py:156: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.&#xA;  __import__(package)&#xA;4174 WARNING: lib not found: api-ms-win-shcore-scaling-l1-1-1.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyQt6\Qt6\plugins\platforms\qwindows.dll&#xA;4252 INFO: Cannot get manifest resource from non-PE file C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING: Cannot get binary dependencies for file: C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\_sounddevice_data\portaudio-binaries\README.md&#xA;4252 WARNING:   Reason: 'DOS Header magic not found.'&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C_flatbuffer.cp310-win_amd64.pyd&#xA;5550 WARNING: lib not found: torch_python.dll dependency of C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_C.cp310-win_amd64.pyd&#xA;64635 INFO: Looking for eggs&#xA;64635 INFO: Using Python library C:\Users\artix\AppData\Local\Programs\Python\Python310\python310.dll&#xA;64635 INFO: Found binding redirects:&#xA;[]&#xA;64666 INFO: Warnings written to D:\coding\python\artix_assistant\build\interface\warn-interface.txt&#xA;65057 INFO: Graph cross-reference written to D:\coding\python\artix_assistant\build\interface\xref-interface.html&#xA;65666 INFO: checking PYZ&#xA;65666 INFO: Building PYZ because PYZ-00.toc is non existent&#xA;65666 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz&#xA;68824 INFO: Building PYZ (ZlibArchive) D:\coding\python\artix_assistant\build\interface\PYZ-00.pyz completed successfully.&#xA;68933 INFO: checking PKG&#xA;68933 INFO: Building PKG because PKG-00.toc is non existent&#xA;68933 INFO: Building PKG (CArchive) interface.pkg&#xA;200906 INFO: Building PKG (CArchive) interface.pkg completed successfully.&#xA;201328 INFO: Bootloader C:\Users\artix\AppData\Local\Programs\Python\Python310\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe&#xA;201328 INFO: checking EXE&#xA;201328 INFO: Building EXE because EXE-00.toc is non existent&#xA;201328 INFO: Building EXE from EXE-00.toc&#xA;201328 INFO: Copying bootloader EXE to D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201406 INFO: Copying icon to EXE&#xA;201406 INFO: Copying icons from ['C:\\Users\\artix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PyInstaller\\bootloader\\images\\icon-console.ico']&#xA;201469 INFO: Writing RT_GROUP_ICON 0 resource with 104 bytes&#xA;201469 INFO: Writing RT_ICON 1 resource with 3752 bytes&#xA;201469 INFO: Writing RT_ICON 2 resource with 2216 bytes&#xA;201469 INFO: Writing RT_ICON 3 resource with 1384 bytes&#xA;201469 INFO: Writing RT_ICON 4 resource with 37019 bytes&#xA;201469 INFO: Writing RT_ICON 5 resource with 9640 bytes&#xA;201469 INFO: Writing RT_ICON 6 resource with 4264 bytes&#xA;201469 INFO: Writing RT_ICON 7 resource with 1128 bytes&#xA;201484 INFO: Copying 0 resources to EXE&#xA;201484 INFO: Embedding manifest in EXE&#xA;201484 INFO: Updating manifest in D:\coding\python\artix_assistant\dist\interface.exe.notanexecutable&#xA;201547 INFO: Updating resource type 24 name 1 language 0&#xA;201562 INFO: Appending PKG archive to EXE&#xA;201797 INFO: Fixing EXE headers&#xA;206831 INFO: Building EXE from EXE-00.toc completed successfully.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;      pyinstaller     -    ?&lt;/p&gt;&#xA;"" OwnerUserId=""452211"" LastActivityDate=""2022-07-18T15:35:02.300"" Title="" .exe    .py   Pyinstaller"" Tags=""&lt;python&gt;&lt;pyinstaller&gt;&lt;pytorch&gt;&lt;pyqt6&gt;&lt;vosk&gt;"" AnswerCount=""0"" CommentCount=""7"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1453876"" PostTypeId=""1"" AcceptedAnswerId=""1454186"" CreationDate=""2022-10-03T20:07:28.383"" Score=""1"" ViewCount=""72"" Body=""&lt;p&gt;    ,    &amp;quot;&amp;quot;    18   80,    &lt;code&gt;QPushButton&lt;/code&gt; (  )   &lt;code&gt;QRadioButton&lt;/code&gt; (    ).&lt;/p&gt;&#xA;&lt;p&gt;  -  ?      &lt;code&gt;LineEdit&lt;/code&gt;,   .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from PySide6 import QtWidgets, QtCore, QtGui&#xA;&#xA;class QTApp(QtWidgets.QWidget):&#xA;    def __init__(self):&#xA;        super(QTApp, self).__init__()&#xA;&#xA;        self.LE_sample_input_01 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_02 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_01.setPlaceholderText('')&#xA;        self.RadioButton = QtWidgets.QRadioButton('-')&#xA;        self.Button = QtWidgets.QPushButton('')&#xA;        self.Button.setStyleSheet(&amp;quot;QPushButton:pressed {background-color: #b3b3ba;}&amp;quot;)&#xA;&#xA;        layout = QtWidgets.QVBoxLayout(self)&#xA;        layout.addWidget(self.LE_sample_input_01)&#xA;        layout.addWidget(self.LE_sample_input_02)&#xA;        layout.addWidget(self.RadioButton)&#xA;        layout.addWidget(self.Button)&#xA;&#xA;        self.LE_sample_input_01.installEventFilter(self)&#xA;&#xA;    def eventFilter(self, obj, event):&#xA;        if event.type() == QtCore.QEvent.FocusOut and QtGui.QFocusEvent.reason(event) == QtCore.Qt.MouseFocusReason:&#xA;            if obj is self.LE_sample_input_01:&#xA;                try:&#xA;                    age = int(self.LE_sample_input_01.text())&#xA;                    if age &amp;lt; 18 or age &amp;gt; 80:&#xA;                        error = QtWidgets.QMessageBox()&#xA;                        error.setWindowTitle(' ')&#xA;                        error.setText('   ( 18  80 )')&#xA;                        error.setIcon(QtWidgets.QMessageBox.Warning)&#xA;                        error.addButton('',QtWidgets.QMessageBox.AcceptRole)&#xA;                        error.exec()&#xA;                        obj.setFocus()&#xA;                        obj.selectAll()&#xA;                        return True&#xA;                except: pass&#xA;        return False&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    app = QtWidgets.QApplication()&#xA;    qt_app = QTApp()&#xA;    qt_app.show()&#xA;    app.exec()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""466264"" LastEditorUserId=""217323"" LastEditDate=""2022-10-05T17:11:57.173"" LastActivityDate=""2022-10-05T17:11:57.173"" Title=""eventFilter  QLineEdit"" Tags=""&lt;python&gt;&lt;&gt;&lt;qlineedit&gt;&lt;pyside6&gt;&lt;eventfilter&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1453876"" PostTypeId=""1"" AcceptedAnswerId=""1454186"" CreationDate=""2022-10-03T20:07:28.383"" Score=""1"" ViewCount=""72"" Body=""&lt;p&gt;    ,    &amp;quot;&amp;quot;    18   80,    &lt;code&gt;QPushButton&lt;/code&gt; (  )   &lt;code&gt;QRadioButton&lt;/code&gt; (    ).&lt;/p&gt;&#xA;&lt;p&gt;  -  ?      &lt;code&gt;LineEdit&lt;/code&gt;,   .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from PySide6 import QtWidgets, QtCore, QtGui&#xA;&#xA;class QTApp(QtWidgets.QWidget):&#xA;    def __init__(self):&#xA;        super(QTApp, self).__init__()&#xA;&#xA;        self.LE_sample_input_01 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_02 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_01.setPlaceholderText('')&#xA;        self.RadioButton = QtWidgets.QRadioButton('-')&#xA;        self.Button = QtWidgets.QPushButton('')&#xA;        self.Button.setStyleSheet(&amp;quot;QPushButton:pressed {background-color: #b3b3ba;}&amp;quot;)&#xA;&#xA;        layout = QtWidgets.QVBoxLayout(self)&#xA;        layout.addWidget(self.LE_sample_input_01)&#xA;        layout.addWidget(self.LE_sample_input_02)&#xA;        layout.addWidget(self.RadioButton)&#xA;        layout.addWidget(self.Button)&#xA;&#xA;        self.LE_sample_input_01.installEventFilter(self)&#xA;&#xA;    def eventFilter(self, obj, event):&#xA;        if event.type() == QtCore.QEvent.FocusOut and QtGui.QFocusEvent.reason(event) == QtCore.Qt.MouseFocusReason:&#xA;            if obj is self.LE_sample_input_01:&#xA;                try:&#xA;                    age = int(self.LE_sample_input_01.text())&#xA;                    if age &amp;lt; 18 or age &amp;gt; 80:&#xA;                        error = QtWidgets.QMessageBox()&#xA;                        error.setWindowTitle(' ')&#xA;                        error.setText('   ( 18  80 )')&#xA;                        error.setIcon(QtWidgets.QMessageBox.Warning)&#xA;                        error.addButton('',QtWidgets.QMessageBox.AcceptRole)&#xA;                        error.exec()&#xA;                        obj.setFocus()&#xA;                        obj.selectAll()&#xA;                        return True&#xA;                except: pass&#xA;        return False&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    app = QtWidgets.QApplication()&#xA;    qt_app = QTApp()&#xA;    qt_app.show()&#xA;    app.exec()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""466264"" LastEditorUserId=""217323"" LastEditDate=""2022-10-05T17:11:57.173"" LastActivityDate=""2022-10-05T17:11:57.173"" Title=""eventFilter  QLineEdit"" Tags=""&lt;python&gt;&lt;&gt;&lt;qlineedit&gt;&lt;pyside6&gt;&lt;eventfilter&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1453876"" PostTypeId=""1"" AcceptedAnswerId=""1454186"" CreationDate=""2022-10-03T20:07:28.383"" Score=""1"" ViewCount=""72"" Body=""&lt;p&gt;    ,    &amp;quot;&amp;quot;    18   80,    &lt;code&gt;QPushButton&lt;/code&gt; (  )   &lt;code&gt;QRadioButton&lt;/code&gt; (    ).&lt;/p&gt;&#xA;&lt;p&gt;  -  ?      &lt;code&gt;LineEdit&lt;/code&gt;,   .&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from PySide6 import QtWidgets, QtCore, QtGui&#xA;&#xA;class QTApp(QtWidgets.QWidget):&#xA;    def __init__(self):&#xA;        super(QTApp, self).__init__()&#xA;&#xA;        self.LE_sample_input_01 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_02 = QtWidgets.QLineEdit()&#xA;        self.LE_sample_input_01.setPlaceholderText('')&#xA;        self.RadioButton = QtWidgets.QRadioButton('-')&#xA;        self.Button = QtWidgets.QPushButton('')&#xA;        self.Button.setStyleSheet(&amp;quot;QPushButton:pressed {background-color: #b3b3ba;}&amp;quot;)&#xA;&#xA;        layout = QtWidgets.QVBoxLayout(self)&#xA;        layout.addWidget(self.LE_sample_input_01)&#xA;        layout.addWidget(self.LE_sample_input_02)&#xA;        layout.addWidget(self.RadioButton)&#xA;        layout.addWidget(self.Button)&#xA;&#xA;        self.LE_sample_input_01.installEventFilter(self)&#xA;&#xA;    def eventFilter(self, obj, event):&#xA;        if event.type() == QtCore.QEvent.FocusOut and QtGui.QFocusEvent.reason(event) == QtCore.Qt.MouseFocusReason:&#xA;            if obj is self.LE_sample_input_01:&#xA;                try:&#xA;                    age = int(self.LE_sample_input_01.text())&#xA;                    if age &amp;lt; 18 or age &amp;gt; 80:&#xA;                        error = QtWidgets.QMessageBox()&#xA;                        error.setWindowTitle(' ')&#xA;                        error.setText('   ( 18  80 )')&#xA;                        error.setIcon(QtWidgets.QMessageBox.Warning)&#xA;                        error.addButton('',QtWidgets.QMessageBox.AcceptRole)&#xA;                        error.exec()&#xA;                        obj.setFocus()&#xA;                        obj.selectAll()&#xA;                        return True&#xA;                except: pass&#xA;        return False&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    app = QtWidgets.QApplication()&#xA;    qt_app = QTApp()&#xA;    qt_app.show()&#xA;    app.exec()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""466264"" LastEditorUserId=""217323"" LastEditDate=""2022-10-05T17:11:57.173"" LastActivityDate=""2022-10-05T17:11:57.173"" Title=""eventFilter  QLineEdit"" Tags=""&lt;python&gt;&lt;&gt;&lt;qlineedit&gt;&lt;pyside6&gt;&lt;eventfilter&gt;"" AnswerCount=""1"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1508480"" PostTypeId=""1"" CreationDate=""2023-03-27T10:42:30.337"" Score=""0"" ViewCount=""24"" Body=""&lt;p&gt;  3 :&lt;/p&gt;&#xA;&lt;p&gt;1&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;&#xA;def function_that_warns():&#xA;    warnings.warn(&amp;quot;deprecated&amp;quot;, DeprecationWarning)&#xA;&#xA;with warnings.catch_warnings():&#xA;    warnings.simplefilter(&amp;quot;ignore&amp;quot;)&#xA;    function_that_warns()  # this will not show a warning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;2&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings('ignore')&#xA;warnings.simplefilter('ignore')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;3&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings(&amp;quot;ignore&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    ,    :&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DCU8X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DCU8X.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.callbacks import EarlyStopping, ModelCheckpoint&#xA;&#xA;early_stop = EarlyStopping(monitor='val_acc', min_delta=0.001,&#xA;                           patience=10, verbose=1, mode='auto')&#xA;&#xA;chkpt = ModelCheckpoint('best_delivery_model',&#xA;                        monitor='val_loss',&#xA;                        verbose=1,&#xA;                        save_best_only=True,&#xA;                        mode='auto')&#xA;&#xA;callbacks = [early_stop, chkpt]&#xA;&#xA;hist = n_model.fit(x_train, y_train,&#xA;                 batch_size=batch_size,&#xA;                 epochs=30,&#xA;                 validation_data=(x_test, y_test),&#xA;                 callbacks=callbacks)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""460134"" LastActivityDate=""2023-03-27T10:42:30.337"" Title=""      jupyter notebook.  ?"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;&lt;jupyter-notebook&gt;&lt;warning&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1513660"" PostTypeId=""1"" CreationDate=""2023-04-16T09:27:27.640"" Score=""0"" ViewCount=""86"" Body=""&lt;pre&gt;&lt;code&gt;from transformers import AutoFeatureExtractor, AutoModelForObjectDetection&#xA;&#xA;extractor = AutoFeatureExtractor.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&#xA;model = AutoModelForObjectDetection.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\Nick\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\models\yolos\feature_extraction_yolos.py&amp;quot;, line 31&#xA;    FutureWarning,&#xA;FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.&#xA;The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pytorch , Python 64 bit&lt;/p&gt;&#xA;"" OwnerUserId=""536957"" LastActivityDate=""2023-04-16T09:27:27.640"" Title=""      Hugging Face"" Tags=""&lt;python&gt;&lt;&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1508480"" PostTypeId=""1"" CreationDate=""2023-03-27T10:42:30.337"" Score=""0"" ViewCount=""24"" Body=""&lt;p&gt;  3 :&lt;/p&gt;&#xA;&lt;p&gt;1&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;&#xA;def function_that_warns():&#xA;    warnings.warn(&amp;quot;deprecated&amp;quot;, DeprecationWarning)&#xA;&#xA;with warnings.catch_warnings():&#xA;    warnings.simplefilter(&amp;quot;ignore&amp;quot;)&#xA;    function_that_warns()  # this will not show a warning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;2&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings('ignore')&#xA;warnings.simplefilter('ignore')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;3&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings(&amp;quot;ignore&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    ,    :&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DCU8X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DCU8X.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.callbacks import EarlyStopping, ModelCheckpoint&#xA;&#xA;early_stop = EarlyStopping(monitor='val_acc', min_delta=0.001,&#xA;                           patience=10, verbose=1, mode='auto')&#xA;&#xA;chkpt = ModelCheckpoint('best_delivery_model',&#xA;                        monitor='val_loss',&#xA;                        verbose=1,&#xA;                        save_best_only=True,&#xA;                        mode='auto')&#xA;&#xA;callbacks = [early_stop, chkpt]&#xA;&#xA;hist = n_model.fit(x_train, y_train,&#xA;                 batch_size=batch_size,&#xA;                 epochs=30,&#xA;                 validation_data=(x_test, y_test),&#xA;                 callbacks=callbacks)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""460134"" LastActivityDate=""2023-03-27T10:42:30.337"" Title=""      jupyter notebook.  ?"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;&lt;jupyter-notebook&gt;&lt;warning&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1508480"" PostTypeId=""1"" CreationDate=""2023-03-27T10:42:30.337"" Score=""0"" ViewCount=""24"" Body=""&lt;p&gt;  3 :&lt;/p&gt;&#xA;&lt;p&gt;1&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;&#xA;def function_that_warns():&#xA;    warnings.warn(&amp;quot;deprecated&amp;quot;, DeprecationWarning)&#xA;&#xA;with warnings.catch_warnings():&#xA;    warnings.simplefilter(&amp;quot;ignore&amp;quot;)&#xA;    function_that_warns()  # this will not show a warning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;2&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings('ignore')&#xA;warnings.simplefilter('ignore')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;3&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings(&amp;quot;ignore&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    ,    :&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DCU8X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DCU8X.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.callbacks import EarlyStopping, ModelCheckpoint&#xA;&#xA;early_stop = EarlyStopping(monitor='val_acc', min_delta=0.001,&#xA;                           patience=10, verbose=1, mode='auto')&#xA;&#xA;chkpt = ModelCheckpoint('best_delivery_model',&#xA;                        monitor='val_loss',&#xA;                        verbose=1,&#xA;                        save_best_only=True,&#xA;                        mode='auto')&#xA;&#xA;callbacks = [early_stop, chkpt]&#xA;&#xA;hist = n_model.fit(x_train, y_train,&#xA;                 batch_size=batch_size,&#xA;                 epochs=30,&#xA;                 validation_data=(x_test, y_test),&#xA;                 callbacks=callbacks)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""460134"" LastActivityDate=""2023-03-27T10:42:30.337"" Title=""      jupyter notebook.  ?"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;&lt;jupyter-notebook&gt;&lt;warning&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1513660"" PostTypeId=""1"" CreationDate=""2023-04-16T09:27:27.640"" Score=""0"" ViewCount=""86"" Body=""&lt;pre&gt;&lt;code&gt;from transformers import AutoFeatureExtractor, AutoModelForObjectDetection&#xA;&#xA;extractor = AutoFeatureExtractor.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&#xA;model = AutoModelForObjectDetection.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\Nick\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\models\yolos\feature_extraction_yolos.py&amp;quot;, line 31&#xA;    FutureWarning,&#xA;FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.&#xA;The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pytorch , Python 64 bit&lt;/p&gt;&#xA;"" OwnerUserId=""536957"" LastActivityDate=""2023-04-16T09:27:27.640"" Title=""      Hugging Face"" Tags=""&lt;python&gt;&lt;&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1507953"" PostTypeId=""1"" AcceptedAnswerId=""1507997"" CreationDate=""2023-03-25T09:44:46.963"" Score=""0"" ViewCount=""621"" Body=""&lt;p&gt;       Silero,     .&lt;/p&gt;&#xA;&lt;p&gt;:&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import torch&#xA;import sounddevice as sd&#xA;import time&#xA;import silero&#xA;&#xA;language = 'ru'&#xA;model_id = 'ru_v3'&#xA;&#xA;sample_rate = 48000&#xA;&#xA;speaker = 'xenia'&#xA;put_accent = True&#xA;put_yo = True&#xA;&#xA;device = torch.device('cpu')&#xA;&#xA;model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;                          model='silero_tts',&#xA;                          language=language,&#xA;                          speaker=model_id)&#xA;model.to(device)&#xA;&#xA;def speak(what):&#xA;    audio = model.apply_tts(text=what+&amp;quot;..&amp;quot;,&#xA;                            speaker=speaker,&#xA;                            sample_rate=sample_rate,&#xA;                            put_accent=put_accent,&#xA;                            put_yo=put_yo)&#xA;    &#xA;    sd.play(audio, sample_rate * 1.05)&#xA;    time.sleep((len(audio) / sample_rate) +0.5)&#xA;    sd.stop&#xA;speak('')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 267&#xA;    warnings.warn(&#xA;UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour&#xA;Traceback (most recent call last):&#xA;  File &amp;quot;C:\Users\\Desktop\speak2.py&amp;quot;, line 17, in &amp;lt;module&amp;gt;&#xA;    model, _ = torch.hub.load(repo_or_dir='snakers4/silero-models_master',&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 539, in load&#xA;    repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, &amp;quot;load&amp;quot;,&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 203, in _get_cache_or_reload&#xA;    _validate_not_a_forked_repo(repo_owner, repo_name, ref)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 162, in _validate_not_a_forked_repo&#xA;    response = json.loads(_read_url(Request(url, headers=headers)))&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\hub.py&amp;quot;, line 145, in _read_url&#xA;    with urlopen(url) as r:&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 214, in urlopen&#xA;    return opener.open(url, data, timeout)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 523, in open&#xA;    response = meth(req, response)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 632, in http_response&#xA;    response = self.parent.error(&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 561, in error&#xA;    return self._call_chain(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 494, in _call_chain&#xA;    result = func(*args)&#xA;  File &amp;quot;C:\Users\\AppData\Local\Programs\Python\Python39\lib\urllib\request.py&amp;quot;, line 641, in http_error_default&#xA;    raise HTTPError(req.full_url, code, msg, hdrs, fp)&#xA;urllib.error.HTTPError: HTTP Error 404: Not Found&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""534347"" LastEditorUserId=""507516"" LastEditDate=""2023-03-25T13:56:13.763"" LastActivityDate=""2023-03-25T19:10:56.767"" Title="" python   "" Tags=""&lt;python&gt;&lt;pytorch&gt;&lt;pyttsx3&gt;&lt;tts&gt;"" AnswerCount=""1"" CommentCount=""11"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1508480"" PostTypeId=""1"" CreationDate=""2023-03-27T10:42:30.337"" Score=""0"" ViewCount=""24"" Body=""&lt;p&gt;  3 :&lt;/p&gt;&#xA;&lt;p&gt;1&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;&#xA;def function_that_warns():&#xA;    warnings.warn(&amp;quot;deprecated&amp;quot;, DeprecationWarning)&#xA;&#xA;with warnings.catch_warnings():&#xA;    warnings.simplefilter(&amp;quot;ignore&amp;quot;)&#xA;    function_that_warns()  # this will not show a warning&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;2&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings('ignore')&#xA;warnings.simplefilter('ignore')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;3&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import warnings&#xA;warnings.filterwarnings(&amp;quot;ignore&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;    ,    :&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/DCU8X.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/DCU8X.png&quot; alt=&quot;   &quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from keras.callbacks import EarlyStopping, ModelCheckpoint&#xA;&#xA;early_stop = EarlyStopping(monitor='val_acc', min_delta=0.001,&#xA;                           patience=10, verbose=1, mode='auto')&#xA;&#xA;chkpt = ModelCheckpoint('best_delivery_model',&#xA;                        monitor='val_loss',&#xA;                        verbose=1,&#xA;                        save_best_only=True,&#xA;                        mode='auto')&#xA;&#xA;callbacks = [early_stop, chkpt]&#xA;&#xA;hist = n_model.fit(x_train, y_train,&#xA;                 batch_size=batch_size,&#xA;                 epochs=30,&#xA;                 validation_data=(x_test, y_test),&#xA;                 callbacks=callbacks)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;"" OwnerUserId=""460134"" LastActivityDate=""2023-03-27T10:42:30.337"" Title=""      jupyter notebook.  ?"" Tags=""&lt;python&gt;&lt;tensorflow&gt;&lt;keras&gt;&lt;jupyter-notebook&gt;&lt;warning&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1513660"" PostTypeId=""1"" CreationDate=""2023-04-16T09:27:27.640"" Score=""0"" ViewCount=""86"" Body=""&lt;pre&gt;&lt;code&gt;from transformers import AutoFeatureExtractor, AutoModelForObjectDetection&#xA;&#xA;extractor = AutoFeatureExtractor.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&#xA;model = AutoModelForObjectDetection.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\Nick\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\models\yolos\feature_extraction_yolos.py&amp;quot;, line 31&#xA;    FutureWarning,&#xA;FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.&#xA;The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pytorch , Python 64 bit&lt;/p&gt;&#xA;"" OwnerUserId=""536957"" LastActivityDate=""2023-04-16T09:27:27.640"" Title=""      Hugging Face"" Tags=""&lt;python&gt;&lt;&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1513660"" PostTypeId=""1"" CreationDate=""2023-04-16T09:27:27.640"" Score=""0"" ViewCount=""86"" Body=""&lt;pre&gt;&lt;code&gt;from transformers import AutoFeatureExtractor, AutoModelForObjectDetection&#xA;&#xA;extractor = AutoFeatureExtractor.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&#xA;model = AutoModelForObjectDetection.from_pretrained(&amp;quot;hustvl/yolos-small&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Warning (from warnings module):&#xA;  File &amp;quot;C:\Users\Nick\AppData\Local\Programs\Python\Python37\lib\site-packages\transformers\models\yolos\feature_extraction_yolos.py&amp;quot;, line 31&#xA;    FutureWarning,&#xA;FutureWarning: The class YolosFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use YolosImageProcessor instead.&#xA;The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Pytorch , Python 64 bit&lt;/p&gt;&#xA;"" OwnerUserId=""536957"" LastActivityDate=""2023-04-16T09:27:27.640"" Title=""      Hugging Face"" Tags=""&lt;python&gt;&lt;&gt;&lt;pytorch&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1544310"" PostTypeId=""1"" CreationDate=""2023-10-05T23:06:49.710"" Score=""0"" ViewCount=""52"" Body=""&lt;p&gt;  ,         .        umap-learn.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import umap&#xA;import pandas as pd&#xA;from sklearn import preprocessing&#xA;from sklearn.manifold import TSNE&#xA;&#xA;&#xA;data = pd.read_csv('C:\\Users\\ASUS\\Desktop\\zoo.csv') #   &#xA;data.dropna(inplace=True) #  NaN &#xA;D = data.drop(['class_type', 'animal_name'], axis=1) #   &#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;T = TSNE(n_components=2, perplexity=50, random_state=123)&#xA;TSNE_features = T.fit_transform(D)&#xA;&#xA;DATA = D.copy()&#xA;DATA['x'] = TSNE_features[:, 0]&#xA;DATA['y'] = TSNE_features[:, 1]&#xA;&#xA;scaler = preprocessing.MinMaxScaler()&#xA;D = pd.DataFrame(scaler.fit_transform(D), columns=D.columns)&#xA;&#xA;n_n = (5, 25, 50)  # n_neighbors&#xA;m_d = (0.1, 0.6)  # min_dist&#xA;&#xA;um = dict()&#xA;for i in range(len(n_n)):&#xA;    for j in range(len(m_d)):&#xA;        um[(n_n[i], m_d[j])] = (umap.UMAP(n_neighbors=n_n[i], min_dist=m_d[j], random_state=123).fit_transform(DATA))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;C:\Users\ASUS\PycharmProjects\pr2\venv\lib\site-packages\umap\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.&#xA;  warn(f&amp;quot;n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;     .&#xA;     :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/f9v2g.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/f9v2g.png&quot; alt=&quot;    umap&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;   &lt;code&gt;umap.numerical_n_jobs = 1&lt;/code&gt;   .&lt;/p&gt;&#xA;"" OwnerUserId=""472084"" LastEditorUserId=""311069"" LastEditDate=""2023-10-06T06:20:59.940"" LastActivityDate=""2023-10-06T06:42:21.307"" Title=""  umap     "" Tags=""&lt;python&gt;&lt;pandas&gt;&lt;big-data&gt;&lt;sklearn&gt;"" AnswerCount=""1"" CommentCount=""1"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
/media/nima/SSD/stackexchange/extracted/ru.stackoverflow.com,"  <row Id=""1553147"" PostTypeId=""1"" CreationDate=""2023-11-25T14:31:54.180"" Score=""1"" ViewCount=""20"" Body=""&lt;p&gt;     c++ pytorch &lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pytorch.org/cppdocs/installing.html&lt;/a&gt;&#xA; cuda toolkit    12.1 ,   libtorch  cuda 12. 1&#xA;cmake :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;cmake_minimum_required(VERSION 3.18 FATAL_ERROR)&#xA;project(example-app)&#xA;&#xA;find_package(Torch REQUIRED)&#xA;set(CMAKE_CXX_FLAGS &amp;quot;${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}&amp;quot;)&#xA;&#xA;add_executable(example-app example-app.cpp)&#xA;target_link_libraries(example-app &amp;quot;${TORCH_LIBRARIES}&amp;quot;)&#xA;set_property(TARGET example-app PROPERTY CXX_STANDARD 17)&#xA;&#xA;# The following code block is suggested to be used on Windows.&#xA;# According to https://github.com/pytorch/pytorch/issues/25457,&#xA;# the DLLs need to be copied to avoid memory errors.&#xA;if (MSVC)&#xA;  file(GLOB TORCH_DLLS &amp;quot;${TORCH_INSTALL_PREFIX}/lib/*.dll&amp;quot;)&#xA;  add_custom_command(TARGET example-app&#xA;                     POST_BUILD&#xA;                     COMMAND ${CMAKE_COMMAND} -E copy_if_different&#xA;                     ${TORCH_DLLS}&#xA;                     $&amp;lt;TARGET_FILE_DIR:example-app&amp;gt;)&#xA;endif (MSVC)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; build      cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA; :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;PS C:\example-app&amp;gt; cd build&#xA;PS C:\example-app\build&amp;gt; cmake -DCMAKE_PREFIX_PATH=C:\libtorch ..&#xA;-- Building for: Visual Studio 17 2022&#xA;-- Selecting Windows SDK version 10.0.22000.0 to target Windows 10.0.22621.&#xA;-- The C compiler identification is MSVC 19.37.32825.0&#xA;-- The CXX compiler identification is MSVC 19.37.32825.0&#xA;-- Detecting C compiler ABI info&#xA;-- Detecting C compiler ABI info - done&#xA;-- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting C compile features&#xA;-- Detecting C compile features - done&#xA;-- Detecting CXX compiler ABI info&#xA;-- Detecting CXX compiler ABI info - done&#xA;-- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.37.32822/bin/Hostx64/x64/cl.exe - skipped&#xA;-- Detecting CXX compile features&#xA;-- Detecting CXX compile features - done&#xA;-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1 (found version &amp;quot;12.1&amp;quot;)&#xA;-- The CUDA compiler identification is NVIDIA 12.1.66&#xA;-- Detecting CUDA compiler ABI info&#xA;-- Detecting CUDA compiler ABI info - done&#xA;-- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe - skipped&#xA;-- Detecting CUDA compile features&#xA;-- Detecting CUDA compile features - done&#xA;-- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/include (found version &amp;quot;12.1.66&amp;quot;) &#xA;-- Caffe2: CUDA detected: 12.1&#xA;-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1/bin/nvcc.exe&#xA;-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1&#xA;-- Caffe2: Header version is: 12.1&#xA;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;  Failed to compute shorthash for libnvrtc.so&#xA;Call Stack (most recent call first):&#xA;  C:/libtorch/share/cmake/Caffe2/Caffe2Config.cmake:87 (include)&#xA;  C:/libtorch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)&#xA;  CMakeLists.txt:4 (find_package)&#xA;&#xA;&#xA;-- USE_CUDNN is set to 0. Compiling without cuDNN support&#xA;-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support&#xA;-- Autodetected CUDA architecture(s):  7.5&#xA;-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75&#xA;-- Found Torch: C:/libtorch/lib/torch.lib&#xA;-- Configuring done (18.4s)&#xA;-- Generating done (0.1s)&#xA;-- Build files have been written to: C:/example-app/build&#xA;PS C:\example-app\build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  ,  -      ,     :&lt;/p&gt;&#xA;&lt;p&gt;Python CMake Warning at C:/libtorch/share/cmake/Caffe2/public/cuda.cmake:185 (message):&#xA;Failed to compute shorthash for libnvrtc.so&lt;/p&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@4b5a67132e81:/example-app/build# cmake --build . --config Release&#xA;Scanning dependencies of target example-app&#xA;[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o&#xA;[100%] Linking CXX executable example-app&#xA;[100%] Built target example-app&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;  :&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; MSBuild 17.7.2+d6990bcfa  .NET Framework&#xA;&#xA;  1&amp;gt;Checking Build System&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;  example-app.cpp&#xA;       C:/example-app/build/Release/example-app.lib   C:/example-app/build/Release/example-app.exp&#xA;  example-app.vcxproj -&amp;gt; C:\example-app\build\Release\example-app.exe&#xA;  Building Custom Rule C:/example-app/CMakeLists.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;"" OwnerUserId=""567924"" LastEditorUserId=""567924"" LastEditDate=""2023-11-25T14:37:55.693"" LastActivityDate=""2023-11-25T14:37:55.693"" Title=""Failed to compute shorthash for libnvrtc.so"" Tags=""&lt;python&gt;&lt;c++&gt;&lt;cmake&gt;&lt;pytorch&gt;&lt;cuda&gt;"" AnswerCount=""0"" CommentCount=""0"" ContentLicense=""CC BY-SA 4.0"" />
"
